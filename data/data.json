{"codes_hierarchy": {"children": [{"children": [{"children": [{"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}], "name": "1st person", "code": "point of view@1st person", "textinfo": "The scene point of view is through the eyes of the user."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}], "name": "3rd person", "code": "point of view@3rd person", "textinfo": "The scene point of view is in front of or on the side side of the user."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper288-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173862", "caption": "Figure 1. The experimental setup (a) and a close-up view of the grid interface (b) in the study.", "name": "CHI18_paper288-Figure1-1.png"}], "name": "overshoulder 3/4", "code": "point of view@overshoulder 3/4", "textinfo": "The scene point of view is from the back of the user, typically over the user's shoulder."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}], "name": "top", "code": "point of view@top", "textinfo": "The point of view of the scene is from the top."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}], "name": "UI only", "code": "point of view@UI only", "textinfo": "The illustration is only composed of the UI, there is no 3D scene. The point of view is always TOP, rendering the UI in the camera plane. We can admit representations of fingers and hands."}], "name": "point of view", "code": "point of view", "textinfo": "What is the point of view of the figure? Where is the camera?"}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}], "name": "one frame", "code": "number of frames@one frame", "textinfo": "The figure is structured with only one frame."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}], "name": "multi frames", "code": "number of frames@multi frames", "textinfo": "The figure is structured with multiple frames."}], "name": "number of frames", "code": "number of frames", "textinfo": "Number of frames composing the illustrations. From 1 to N."}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper347-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242589", "caption": "Figure 3: FingerArc and FingerChord: (a) holding an action key with a special hand posture for a predesignated delay time reveals the shortcut interface; (b) selecting the primary command using the index finger with others tucked in (FingerArc) or the middle finger (FingerChord); (c-e) selecting other commands using the angle of the thumb (FingerArc) or pressing different key areas (FingerChord); (f) releasing the key maintaining a hand posture triggers the command (e.g. primary command); (g) revealing all the fingers while holding the key cancels the operation.", "name": "UIST18_paper347-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper927-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242588", "caption": "Figure 7. Attention Guidance: left or right-sided normal force on face guides users to search toward left or right respectively.", "name": "UIST18_paper927-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper354-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3173928", "caption": "Figure 18. Setup for visual analytics application.", "name": "CHI18_paper354-Figure18-1.png"}, {"is_image": true, "path": "CHI18_paper5-Figure9-1.png", "doi": "https://doi.org/10.1145/10.1145/3173574.3173579", "caption": "Figure 9. Paper Torch by Nendo (http://www.nendo.jp/en/works/papertorch/).", "name": "CHI18_paper5-Figure9-1.png"}], "name": "inset (PiP)", "code": "sub-framing@inset (PiP)", "textinfo": "A frame is included in another frame's space with a different point of view."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper65-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242665", "caption": "Figure 2. MetaArms design approach: a closely situated anthropomorphic arms system driven by leg and feet motion, with haptic feedback loop.", "name": "UIST18_paper65-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 10. (a) Changing the axis dimensions, and (b) remote control from a distance to set the focus onto a specific visualization view.", "name": "CHI18_paper19-Figure10-1.png"}], "name": "magnification lens", "code": "sub-framing@magnification lens", "textinfo": "A frame (whatever its nature) is included in another frame's space. This inclusion does not belong to the scene, and does represent something in the scene. It has been made to help the reader understand the figure."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}], "name": "UI embedded", "code": "sub-framing@UI embedded", "textinfo": "The UI is embedded in the 3D scene, possibly using perspective."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper90-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173664", "caption": "Figure 6. A phase of loosely-coupled collaboration during the groupbased expert walkthrough. Domain experts navigated egocentrically to select their individual points of view.", "name": "CHI18_paper90-Figure6-1.png"}], "name": "UI overlay", "code": "sub-framing@UI overlay", "textinfo": "The UI has been added on top of the scene (mostly used to illustrate the user view from a AR system)."}, {"children": [{"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}], "name": "juxtaposition", "code": "sub-framing@juxtaposition", "textinfo": "The frame is composed of multiple juxtaposed sub-frames. (vertically/horizontally)"}], "name": "sub-framing", "code": "sub-framing", "textinfo": "Frames that are included in the frame space and do not belong to the represented scene itself."}], "name": "layout", "code": "layout", "textinfo": "How is organized the figure?"}], "name": "composition", "code": "composition", "textinfo": "Key categories that describe the main structure of a figure."}, {"children": [{"children": [{"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "simplistic", "code": "realism@simplistic", "textinfo": "Very simplistic drawings are used. "}, {"children": [{"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 11: (a) TapZoom. (b1) Initial screen. (b2) User doubletaps on the left side of the screen. (b3) User holds contact for continuously zooming out.", "name": "CHI18_paper634-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 10: (a) PanPress. (b1) Initial screen. User performs a first pan. (b2) User holds contact for continuously panning. (b3) User adjusts the panning direction with subtle finger motion.", "name": "CHI18_paper634-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 9. Basic workflow: 1. Sketch a design on fabric, 2. adhere Circuitry Stickers, 3. draw the circuit and electrical connections, 4. capture a picture of the sketch, 5. insert the fabric into the embroidery machine for stitching, and 6. replace the stickers with hardware components.", "name": "CHI18_paper82-Figure9-1.png"}, {"is_image": true, "path": "Ubicomp18_paper194-Figure1-1.png", "doi": "https://doi.org/10.1145/3287072", "caption": "Fig. 1. Obstacle detection using smartphone.", "name": "Ubicomp18_paper194-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 9. I/O Braid\u2019s capability to sense touch and rotation input along the length of a headphone cord allows less precise input when on the go, for example, when jogging. The integrated visual feedback can be used to communicate phone connection or music status. This functionality could help signal social cues such as interruptibility, to onlookers. The photos show how flowing light can be used for directional feedback.", "name": "UIST18_paper485-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper419-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173993", "caption": "Figure 1. Consistent, user-defined gesture sets for controlling reading flow via RSVP on three device types: phone, watch and glasses.", "name": "CHI18_paper419-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper647-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174221", "caption": "Figure 2. This figure illustrates the experimental setup. (1) HTC Vive optical tracker (at 2.5m) and tracking space with 4\u00d7 4m2. (2) Virtual keyboard, stimulus and text input field. (3) Participant wearing HTC Vive and tracked hand-held controllers. (4) PC for experiment control and filling out questionnaires.", "name": "CHI18_paper647-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 3. Rotating the I/O Braid: Relative capacitive signal strengths are shown in graph at bottom. As each pair of columns approaches a finger, its signal strength increases.", "name": "UIST18_paper485-Figure3-1.png"}], "name": "realistic", "code": "realism@realistic", "textinfo": "Drawings are close to the reality. "}], "name": "realism", "code": "realism", "textinfo": "How is the drawing realism?"}], "name": "drawing", "code": "drawing", "textinfo": "The element is a drawn object (representation of a person, a smartphone, a computer, a room, etc.)."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 2. Scenariot localization principle.", "name": "CHI18_paper219-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 4. Abstracting attributes by demonstration. An arrow\u2019s \u201cangle\u201d attribute is abstracted as a variable using two constraint markers.", "name": "CHI18_paper199-Figure4-1.png"}], "name": "drawing", "code": "UI@drawing", "textinfo": "The UI has been drawn (by hand or not)."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}], "name": "rendering", "code": "UI@rendering", "textinfo": "The UI has been generated or captured."}], "name": "UI", "code": "UI", "textinfo": "The element is a UI"}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}], "name": "photo", "code": "type@photo", "textinfo": "The element is a photography."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 2: The cross-section of two resistive-coated yarns, which form a resistor (a) that can measure applied force (b).", "name": "UIST18_paper745-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper628-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.317420", "caption": "Figure 6. Highlighting interactions: Type 1 (left) & Type 2 (right)", "name": "CHI18_paper628-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 1: In-house Macro Recorder.", "name": "CHI18_paper638-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper123-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173697", "caption": "Figure 8: Collections with layouts: (a) Repeat Grid. (b) Partition Stack. (c) Partition Stacks Nested in a Repeat Grid", "name": "CHI18_paper123-Figure8-1.png"}], "name": "data visualization", "code": "type@data visualization", "textinfo": "The element represents data through a diagram, a graph, a chart... "}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 2. The different manipulations that can be applied to a search result", "name": "CHI18_paper251-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper610-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174184", "caption": "Figure 1. Example use case of the proposed model: Different levels of detail are shown depending on perceivable screen resolution based on the device\u2019s position and orientation in the field of view.", "name": "CHI18_paper610-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}], "name": "clipart/icon", "code": "type@clipart/icon", "textinfo": "The figure contains some cliparts or icons."}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}], "name": "text", "code": "type@text", "textinfo": "The figure contains some text to describe some elements."}], "name": "type", "code": "type", "textinfo": "The different types of elements that compose a figure."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}], "name": "transparency", "code": "color@transparency", "textinfo": "Is transparency used?"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}], "name": "colors", "code": "hue@colors", "textinfo": "The figure uses multiple colors."}, {"children": [{"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 2: Users are confined into tiles. The rotation maneuver allows apps to move their user to the adjacent tile. This way users do not feel confined.", "name": "CHI18_paper241-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 4: The focus maneuver temporarily provides the badminton app with control over most of the physical space. The other apps go into a defocus state.", "name": "CHI18_paper241-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper374-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173948", "caption": "Figure 2: (a) Anticlastic (saddle-shape) curvature (v > 0), (b) synclastic (dome-shape) curvature (v < 0) [201] (\u00a9 2014 image reproduced with permission from Elsevier).", "name": "CHI18_paper374-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 1. Various representations of the \u2018single finger swipe\u2019 gesture in different academic papers [27,36,10,14,35,5,13,2].", "name": "CHI18_paper547-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper161-Figure9-1.png", "doi": "https://doi.org/10.1145/3287039", "caption": "Fig. 9. Examples of actual grips (a, c) and the corresponding simulated grips (b, d).", "name": "Ubicomp18_paper161-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper20-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173594", "caption": "Figure 2. (A) A low vision user using SteeringWheel with Surface Dial. (B) The Surface Dial and its gestures.", "name": "CHI18_paper20-Figure2-1.png"}], "name": "grayscale", "code": "hue@grayscale", "textinfo": "The figure uses grayscale"}, {"children": [{"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 14: Making the pendulum reach its two degrees of freedom requires two user interactions: (a) The first hit makes the prop swing back and forth and (b) the second hit allows it to orbit.", "name": "CHI18_paper89-Figure14-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 4. Three different body postures in which different factors affect interaction such as a) Sitting at a table resting arms; b) Standing; c) Lying down with the back to the floor. (Arrows represent gravity and the circles are restrictions)", "name": "CHI18_paper202-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper737-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242615", "caption": "Figure 1. Example of Magneto-Haptics. (A) Three cylindrical magnets provide a magnetic force to the cube magnet. (B) This feedback creates a sensation as if one is moving an object on a gradient during active touch.", "name": "UIST18_paper737-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}, {"is_image": true, "path": "UIST18_paper877-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242604", "caption": "Figure 9: (a) After experimenting with various 3D printed shapes and sizes, (b) the \u201cflattened teardrop\u201d design performed best, because its orientation can be felt any time.", "name": "UIST18_paper877-Figure9-1.png"}], "name": "monochrome", "code": "hue@monochrome", "textinfo": "The figure is only based on 2 colors : a light and dark color (mostly black and white)."}], "name": "hue", "code": "hue", "textinfo": "What is the hue of the figure?"}], "name": "color", "code": "color", "textinfo": "The color used in the figure."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper737-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242615", "caption": "Figure 1. Example of Magneto-Haptics. (A) Three cylindrical magnets provide a magnetic force to the cube magnet. (B) This feedback creates a sensation as if one is moving an object on a gradient during active touch.", "name": "UIST18_paper737-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}], "name": "dashed", "code": "line style@dashed", "textinfo": "Dashed lines are used."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 14: Making the pendulum reach its two degrees of freedom requires two user interactions: (a) The first hit makes the prop swing back and forth and (b) the second hit allows it to orbit.", "name": "CHI18_paper89-Figure14-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 2: Users are confined into tiles. The rotation maneuver allows apps to move their user to the adjacent tile. This way users do not feel confined.", "name": "CHI18_paper241-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}], "name": "width", "code": "line style@width", "textinfo": "Lines with different width are used."}], "name": "line style", "code": "line style", "textinfo": "What line styles are used for the different objects?"}], "name": "visual characteristics", "code": "visual characteristics", "textinfo": "Key visual characteristics that compose an illustration."}, {"children": [{"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 11: (a) TapZoom. (b1) Initial screen. (b2) User doubletaps on the left side of the screen. (b3) User holds contact for continuously zooming out.", "name": "CHI18_paper634-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}], "name": "color area", "code": "region@color area", "textinfo": "Emphasize a region of the figure by applying a specific colour in the pointed area."}], "name": "region", "code": "region", "textinfo": "Emphasize a region of the figure."}, {"children": [{"children": [{"is_image": true, "full_res_path": "fake_path", "doi": "fake_doi", "caption": "fake_caption", "name": "fake_name"}], "name": "arrows", "code": "element@arrows", "textinfo": "Arrows are used to focus on an element, mostly by pointing to an object."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}], "name": "color highlight", "code": "element@color highlight", "textinfo": "A specific colour is used to highlight a specific object."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 4: The focus maneuver temporarily provides the badminton app with control over most of the physical space. The other apps go into a defocus state.", "name": "CHI18_paper241-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 9. (a) Once the HMD user approaches the aggressive shield installed at the door, (b) the system pauses the game with a grayed out viewport, and (c) lead the user to the safe zone indicated by a green halo. The game resumes with a button on the controller.", "name": "UIST18_paper499-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper334-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173908", "caption": "Figure 5: Top-ten chosen gestures in each of the three conditions: standing, sitting and projection", "name": "CHI18_paper334-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 2: In-house Smart Assistant.", "name": "CHI18_paper638-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 9. Drawing interface to mark custom spatial entities. (a) Holding drawing button at bottom left corner of display, (b) activates drawing mode, dimming screen, allowing user to sketch line to specify path or (c\u2013d) sketch (nearly) closed outline and scribble inside it to specify area. (e) Tapping \u201c+\u201d button creates SpaceToken for the marked area. (f) An area SpaceToken is added above the \u201c+\u201d button.", "name": "CHI18_paper248-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 9. Natural grasp & Magic grasp.", "name": "UIST18_paper5-Figure9-1.png"}], "name": "exact contour line", "code": "enclosing@exact contour line", "textinfo": "The exact contours of a specific object are highlighted using a specific line style."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 4. Three different body postures in which different factors affect interaction such as a) Sitting at a table resting arms; b) Standing; c) Lying down with the back to the floor. (Arrows represent gravity and the circles are restrictions)", "name": "CHI18_paper202-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 14. Monitoring the IoT assets (a, b) and navigating the user towards the assets by visualizing the direction on the screen(c, d).", "name": "CHI18_paper219-Figure14-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 10. Safe zone is defined as a personal region located at the center of the maximum inscribed circle found in the available free space", "name": "UIST18_paper499-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}], "name": "circle/rectangle", "code": "enclosing@circle/rectangle", "textinfo": "A circle/rectangle shape has been drawn around the focused object."}], "name": "enclosing", "code": "enclosing", "textinfo": "The focus element is enclosed."}], "name": "element", "code": "element", "textinfo": "Emphasize an element of the figure."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper499-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 9. (a) Once the HMD user approaches the aggressive shield installed at the door, (b) the system pauses the game with a grayed out viewport, and (c) lead the user to the safe zone indicated by a green halo. The game resumes with a button on the controller.", "name": "UIST18_paper499-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper581-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242599", "caption": "Figure 10. Study 3 was conducted on a subway", "name": "UIST18_paper581-Figure10-1.png"}], "name": "grayed out/blurred", "code": "background@grayed out/blurred", "textinfo": "A blur effect or a specific colour with transparency is applied on the background."}, {"children": [{"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper90-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173664", "caption": "Figure 6. A phase of loosely-coupled collaboration during the groupbased expert walkthrough. Domain experts navigated egocentrically to select their individual points of view.", "name": "CHI18_paper90-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 3. (a) User\u2019s hand moving from left to right on the shape display. (b) Virtual hand displacement scaled up on the shape display to improve resolution.", "name": "CHI18_paper150-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper354-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3173928", "caption": "Figure 18. Setup for visual analytics application.", "name": "CHI18_paper354-Figure18-1.png"}, {"is_image": true, "path": "UIST18_paper595-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242635", "caption": "Figure 5: Gesture set for the glasses (top) and watch (bottom).", "name": "UIST18_paper595-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 6. The SpaceFace application and its outside view (a), inside view (b) interaction and visualization concept (c) and physical interaction scenario (d).", "name": "CHI18_paper54-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper90-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173664", "caption": "Figure 7. A phase of tightly-coupled collaboration during the groupbased expert walkthrough. The domain experts selected a similar point of view and discussed the next analysis steps. Deictic gestures were used frequently during discussions.", "name": "CHI18_paper90-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper161-Figure2-1.png", "doi": "https://doi.org/10.1145/3287039", "caption": "Fig. 2. Objects used in Study 1. In the basic group, we selected six everyday objects in accordance with the Schlesinger taxonomy [37]. In the size group (control weight), there are four levels in size for each shape and all objects have the same weight of 100g. In the weight group (control size), objects sharing the same shape have the same size while there are four levels in weight. The objects in the latter two groups were 3D printed. Weights were controlled by adding iron sand inside.", "name": "Ubicomp18_paper161-Figure2-1.png"}], "name": "removed", "code": "background@removed", "textinfo": "The highlighted objects are detoured and the background is removed."}], "name": "background", "code": "background", "textinfo": "Strategy used to emphasize the element in the foreground from the background"}], "name": "emphasize", "code": "emphasize", "textinfo": "Visual techniques used to visually emphasize, highlight specific information in the illustration."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper76-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173650", "caption": "Figure 3. While telling researchers a story about her photos, P12 zoomed into the right photo in this collage to emphasize the butterfly on her head.", "name": "CHI18_paper76-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper289-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173863", "caption": "Figure 1. Dyads performed the first (A) and second (B) tasks in the faceto-face conditions. In virtual reality conditions, avatars appeared across the table from each other (C), but were actually positioned on opposite sides of the motion capture stage (D). In the embodVR condition, participants were able to see both avatars (E). In the no_embodVR condition, participants were unable to see their partner and could only see their hands in the second task, to assist with furniture manipulation (F).", "name": "CHI18_paper289-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 3. (a) The first version of the probe is non interactive. Children used it to built tactile bracelets and generate novel scenarios for documenting and reflecting on field-trips (here, in a museum). (b) The second version is a functional bracelet recording and playing sound.", "name": "CHI18_paper236-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 5. Left: P8 (WO2) points toward overview device, other members shifted their attention to it. In NO groups pointing rarely led to shared attention (right): P29 (INT1) points toward her device; other members\u2019 focus stays on own devices.", "name": "CHI18_paper300-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper443-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174017 ", "caption": "Figure 2. Participant Playing the Game in Lab Setting with the Experimenter", "name": "CHI18_paper443-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper551-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174125", "caption": "Figure 3: Children interacting with the interactive surface.", "name": "CHI18_paper551-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper359-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173933 ", "caption": "Figure 2. The VR view of the 360\u00b0 live-video feed, as it was presented to the viewer through the smartphone-VR setup.", "name": "CHI18_paper359-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 4. Shared attention on the overview device (SSV; left) often led to active discussion (AD; right) as the device gave the group a common focus and starting point for a diccussion.", "name": "CHI18_paper300-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper397-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173971", "caption": "Figure 5. Y. has drawn a new bird-card and is trying it on the Birdhouse. She is a little disappointed that nothing happens when she waives the card in front of the sensors as she uses to do with the other ones.", "name": "CHI18_paper397-Figure5-1.png"}], "name": "blur", "code": "anonymization@blur", "textinfo": "Parts of the figure are blurred to not be recognized."}, {"children": [{"is_image": true, "full_res_path": "fake_path", "doi": "fake_doi", "caption": "fake_caption", "name": "fake_name"}], "name": "filling", "code": "anonymization@filling", "textinfo": "Parts of the figure are filled with colour to not be recognized."}], "name": "anonymization", "code": "anonymization", "textinfo": "Visual techniques used to anonymize specific elements in the figure (e.g. users)."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 4. Three different body postures in which different factors affect interaction such as a) Sitting at a table resting arms; b) Standing; c) Lying down with the back to the floor. (Arrows represent gravity and the circles are restrictions)", "name": "CHI18_paper202-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}], "name": "color grouping", "code": "grouping and linking@color grouping", "textinfo": "Linking/Grouping of elements is made using colours."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 2. Scenariot localization principle.", "name": "CHI18_paper219-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 2: The cross-section of two resistive-coated yarns, which form a resistor (a) that can measure applied force (b).", "name": "UIST18_paper745-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper385-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173959", "caption": "Figure 1. The BIGFile interface as the user navigates to \u201cDog\u201d in a file retrieval task. (a) and (c) show the adaptive part with two shortcuts, (b) and (d) the static part. In Step 1, the shortcuts do not help and the user selects \u201cAnimals\u201d in the static part, leading to Step 2 where the user directly selects \u201cDog\u201d in the first shortcut, saving one step.", "name": "CHI18_paper385-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 3: Example of text selection using FORCESELECT HIGH-", "name": "CHI18_paper477-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper354-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3173928", "caption": "Figure 18. Setup for visual analytics application.", "name": "CHI18_paper354-Figure18-1.png"}], "name": "identifiers grouping", "code": "grouping and linking@identifiers grouping", "textinfo": "Linking/Grouping of elements is made using identifiers (numbers or letters). Line can be used between the identifier and the element."}, {"children": [{"is_image": true, "full_res_path": "fake_path", "doi": "fake_doi", "caption": "fake_caption", "name": "fake_name"}], "name": "grouping lines", "code": "grouping and linking@grouping lines", "textinfo": "Lines or Arrows used to link/group multiple elements inside the figure or extra information outside the figure."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper177-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173751", "caption": "Figure 2. Our implementation uses a 3D printed PLA dish with an acrylic base plate. 2.4 mm diameter steel electrodes were embedded at 11 mm intervals and driven by a switching circuit.", "name": "CHI18_paper177-Figure2-1.png"}], "name": "text annotation", "code": "grouping and linking@text annotation", "textinfo": "Text attached to an element of a figure (e.g. label, definition) using lines or arrows."}], "name": "grouping and linking", "code": "grouping and linking", "textinfo": "Visual techniques used to link/group elements between each others or with extra information outside the figure (e.g. definitions)."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}], "name": "stroboscopic", "code": "effects@stroboscopic", "textinfo": "A stroboscopic effect is used to show the different steps of the motion."}, {"children": [{"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}, {"is_image": true, "path": "CHI18_paper385-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173959", "caption": "Figure 1. The BIGFile interface as the user navigates to \u201cDog\u201d in a file retrieval task. (a) and (c) show the adaptive part with two shortcuts, (b) and (d) the static part. In Step 1, the shortcuts do not help and the user selects \u201cAnimals\u201d in the static part, leading to Step 2 where the user directly selects \u201cDog\u201d in the first shortcut, saving one step.", "name": "CHI18_paper385-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 1: In-house Macro Recorder.", "name": "CHI18_paper638-Figure1-1.png"}], "name": "waves", "code": "effects@waves", "textinfo": "Waves are used to show that an element is hitting another element."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 8. The experiment was conducted with PW using a controller (PWC) and ray-casting (RC). Using PWC, the participant (a) first searched for the target (white), (b) grabbed the window (blue), (c) moved the window while adjusting its apparent size, and (d) released it onto the target when the apparent sizes matched. Using RC, the participant (e) first searched for the target (white), (f) dragged the window (blue) to the target, (g) placed it in the target, and (h) scaled it so that the absolute sizes matched.", "name": "CHI18_paper218-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper530-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174104", "caption": "Figure 1. Inpher makes it possible to define physical properties like bounciness of virtual objects through mimicking their physical behavior. (a) The user is equipped with virtual reality goggles and controllers for 3D input. (b) Users can grasp virtual objects and describe a physical motion to define the object\u2019s physical behavior (top). The free-flight physical motion of the object resembles the user\u2019s input curve (bottom).", "name": "CHI18_paper530-Figure1-1.png"}], "name": "motion blur", "code": "effects@motion blur", "textinfo": "A directional blur effect is used to illustrate motion. The position of the blur can also indicate the path/direction of the movement."}], "name": "effects", "code": "effects", "textinfo": "Dynamic style effects"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 2. OctoPocus with traditional video prototyping. The designers create a rough stop-motion movie with only four stages of the interface, resulting in a poor representation of the dynamic interaction.", "name": "UIST18_paper675-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}], "name": "trajectories", "code": "lines and arrows@trajectories", "textinfo": "Shows the motion path of an element in the figure from the beginning to the end."}, {"children": [{"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}], "name": "direction", "code": "lines and arrows@direction", "textinfo": "Shows the direction of the motion."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper628-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.317420", "caption": "Figure 6. Highlighting interactions: Type 1 (left) & Type 2 (right)", "name": "CHI18_paper628-Figure6-1.png"}], "name": "transfer", "code": "lines and arrows@transfer", "textinfo": "Shows transition between two states of a model/system."}, {"children": [{"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}], "name": "projection", "code": "lines and arrows@projection", "textinfo": "Shows the corresponding projection of an object."}], "name": "lines and arrows", "code": "lines and arrows", "textinfo": "Geometric shapes are used to help understanding the dynamic illustrated."}, {"children": [{"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper27-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173601", "caption": "Figure 2. Editing operations and matching gestures. (a) Caret movement using horizontal directional gestures. (b) Text selection using caret movement and touching the screen with a second thumb. (c) After selection, users cycle through the clipboard operations using vertical directional gestures and keeping the other thumb on the screen.", "name": "CHI18_paper27-Figure2-1.png"}], "name": "Contact shapes", "code": "dynamic@Contact shapes", "textinfo": "Geometric shapes, such as circles, used to represent the region of contact between two elements."}], "name": "dynamic", "code": "dynamic", "textinfo": "Visual techniques used to describe dynamic content on a static support."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 11: (a) TapZoom. (b1) Initial screen. (b2) User doubletaps on the left side of the screen. (b3) User holds contact for continuously zooming out.", "name": "CHI18_paper634-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 1: In-house Macro Recorder.", "name": "CHI18_paper638-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 9. Basic workflow: 1. Sketch a design on fabric, 2. adhere Circuitry Stickers, 3. draw the circuit and electrical connections, 4. capture a picture of the sketch, 5. insert the fabric into the embroidery machine for stitching, and 6. replace the stickers with hardware components.", "name": "CHI18_paper82-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 2: In-house Smart Assistant.", "name": "CHI18_paper638-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper441-Figure21-1.png", "doi": "https://doi.org/10.1145/3173574.3174015", "caption": "Figure 21. Designers must consider model orientation in the printer. Structure 1 is more fragile than 2: (1) horizontally", "name": "CHI18_paper441-Figure21-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 7. Sensor Sticker process: 1. The user cuts the sticker, 2. adheres it to fabric, and draws connecting traces with trace color pen, 3. captures the design and removes the sticker, 4. the system stitches the sensor.", "name": "CHI18_paper82-Figure7-1.png"}], "name": "numbers", "code": "identifiers@numbers", "textinfo": "Numbers are used (often in a corner) to order the sequence of frames or the different parts of a figure."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}], "name": "letters", "code": "identifiers@letters", "textinfo": "Letters are used (often in a corner) to order the sequence of frames or the different parts of a figure."}, {"children": [{"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper497-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174071", "caption": "Figure 6. PolarTrack\u2019s tracking quality was evaluated with 1-finger, 1- hand, and 2-hands occlusion.", "name": "CHI18_paper497-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}], "name": "title", "code": "identifiers@title", "textinfo": "A different title is used to annotate the sub frames."}], "name": "identifiers", "code": "identifiers", "textinfo": "Strategies used to identify, order, different parts of the same figure."}], "name": "structure", "code": "structure", "textinfo": "Visual techniques used to structure an illustration"}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 4. Abstracting attributes by demonstration. An arrow\u2019s \u201cangle\u201d attribute is abstracted as a variable using two constraint markers.", "name": "CHI18_paper199-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 5. Experimental tasks for the three timing methods: Libet clock (left), Visual Alphabet (middle) and Audio Alphabet (right).", "name": "CHI18_paper541-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 9. Experimental tasks for the two timing methods: Libet Clock (left) and Haptic Clock (right).", "name": "CHI18_paper541-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper603-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174177", "caption": "Figure 1. (a) Lassoing icons. Icons whose entire area is inside of the loop are selected (highlighted in blue). The start and end points are automatically closed. The work presented here focuses on the straight stroking segments. (b-f) Steering through obstacles with various conditions.", "name": "CHI18_paper603-Figure1-1.png"}], "name": "Arrows", "code": "measure@Arrows", "textinfo": "Arrows are used to indicate angles, physical dimensions."}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 5. Experimental tasks for the three timing methods: Libet clock (left), Visual Alphabet (middle) and Audio Alphabet (right).", "name": "CHI18_paper541-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 9. Experimental tasks for the two timing methods: Libet Clock (left) and Haptic Clock (right).", "name": "CHI18_paper541-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper603-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174177", "caption": "Figure 1. (a) Lassoing icons. Icons whose entire area is inside of the loop are selected (highlighted in blue). The start and end points are automatically closed. The work presented here focuses on the straight stroking segments. (b-f) Steering through obstacles with various conditions.", "name": "CHI18_paper603-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 8. Stretchable input sensors compatible with our workflow: (a) a touch sensor, (b) a proximity sensor, (c) a slider, (d) a strain sensor, (e) a capacitive pressure sensor", "name": "CHI18_paper188-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper589-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174163", "caption": "Figure 5: Definition used for the pitch and roll angles.", "name": "CHI18_paper589-Figure5-1.png"}], "name": "Text indicator", "code": "measure@Text indicator", "textinfo": "Text is used to directly show physical dimensions."}], "name": "measure", "code": "measure", "textinfo": "Visual techniques used to illustrate physical units."}], "name": "Visual techniques", "code": "Visual techniques", "textinfo": "Visual techniques identified across multiple figures."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}], "name": "design space", "code": "purpose@design space", "textinfo": "The figure illustrates a set of functionalities, possibilities, commands or use cases available in a system."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}], "name": "interaction sequence", "code": "purpose@interaction sequence", "textinfo": "The figure illustrates an interaction sequence (or storyboard), i.e. a sequence of actions that must be accomplished in a specific order to launch a system command."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}], "name": "interactive system", "code": "purpose@interactive system", "textinfo": "The figure illustrates an interactive system but is focused on the setup of this system and its sensing capabilities rather than on the interaction between the user and the system."}], "name": "purpose", "code": "purpose", "textinfo": "What is the purpose of the figure?"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}], "name": "moving", "code": "time@moving", "textinfo": "Time is evolving across the figure, i.e. we can see a transition, progression, within the user's actions and/or the system's responses."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}], "name": "still", "code": "time@still", "textinfo": "We are observing the user and/or the system at a specific moment in time, without any information on the next/previous user's actions or system's responses."}], "name": "time", "code": "time", "textinfo": "How time is represented in the figure?"}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper170-Figure5-1.png", "doi": "https://doi.org/10.1145/3287048", "caption": "Fig. 5. FarmChat speech-based interaction", "name": "Ubicomp18_paper170-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 4. The Canvas sketching interface: The final composition (left) and the interface (right) show the \u201cuser overlay\u201d. Both sides have a list of sketches and animation controls at the bottom. The in/out buttons make the selected sketches appear/disappear. The sliders control the stroke-start, now at 0%, and the stroke-end, now at 100%.", "name": "UIST18_paper675-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper439-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174013", "caption": "Figure 6. Experiment 2 setup. A user held the device in one hand, and typed on the invisible keyboard with her preferred posture. The spatially-adapted and the unadapted keyboards shared the same appearance.", "name": "CHI18_paper439-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper339-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173913", "caption": "Figure 5. Seven out of our 10 participants interacting with the probe. P2, PII and PIX use the slider. P3 and P5 are about the change the shape by clicking on the central button. P6 and P8 use the rotary knob.", "name": "CHI18_paper339-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper45-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173619", "caption": "Figure 1. One participant is reading with RSVP while walking during the study. In the study, we investigated the effect of different text positions and presentation types on binocular see-through smart glasses.", "name": "CHI18_paper45-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper622-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174196", "caption": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "name": "CHI18_paper622-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper224-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173798", "caption": "Figure 4. Posting a digital sticky note to the whiteboard is a two-step process. First, the user touches the NoteCanvas at the location where the sticky note should appear, which enables the post button on the NoteCreator. Second, the user taps the post button on the NoteCreator, which will post the sticky note to the whiteboard and remove it from the NoteCreator (indicated with a semi-transparent sticky note).", "name": "CHI18_paper224-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 9: Application highlights: (a) switching from pencil to highlighter with Side and Heel postures (document annotation); (b) choosing pen colour from radial menu with Side-RingOut-PinkyOut (document annotation); (c) object creation menu with Side-PinkyOut (vector drawing); (d) using SidePinkyIn to use handwriting recognition for creating a text object (vector drawing); (e) gesture command mode using Float-Index (vector drawing).", "name": "UIST18_paper825-Figure9-1.png"}], "name": "production", "code": "activity@production", "textinfo": "Digital content production: programming, text entry."}, {"children": [{"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper53-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242626", "caption": "Figure 6: As a partner, MobiLimb can express behaviors and embody virtual agents. a) Cat with a tail, that reacts to users\u2019 actions. b) Hostile scorpion. c) Curious device. d) Assistive guide showing how to scroll on a page.", "name": "UIST18_paper53-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}], "name": "entertainment", "code": "activity@entertainment", "textinfo": "Video, music, video games..."}, {"children": [{"is_image": true, "path": "CHI18_paper362-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 7. Users maintained an awareness of their activity levels and formed micro-plans such as grabbing water while waiting for the printer, or walking 1000 steps in the next hour.", "name": "CHI18_paper362-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 3. Alternative BioFidget designs. (a) Basic design. (b) BioFidget with an additional clip for PPG sensing stabilization. (c) Fan-shaped wing to react to respiration. (d) BioFidget with a handheld display for rich visual biofeedback.", "name": "CHI18_paper613-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper867-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242636", "caption": "Figure 2. The direct interaction technique enables the radiologists to interact with the environment by performing the selected gestures at the working zone.", "name": "UIST18_paper867-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper362-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 3. Users were nine times more likely to check their tracker during vigorous physical activity than when sedentary. Such engagements increased in frequency near goal completion and fueled users\u2019 motivation to meet their goals", "name": "CHI18_paper362-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 1. ActiveErgo is the first active approach to improving ergonomics by providing automatic and personalized computer workspace adjustment. Our prototype uses a Microsoft Kinect sensor for skeletal tracking and uses robotic arms and motorized desk to provide automatic workspace adjustment.", "name": "CHI18_paper558-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper362-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 4. Trackers were checked strategically towards gaining insights on behaviors \u2013 such as right before starting, and after finishing walking a dog, to know how many steps were gained.", "name": "CHI18_paper362-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper209-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173783", "caption": "Figure 2. Learner is \u2018wearing\u2019 an expert\u2019s experience, performing an ultrasound diagnosis with the player application", "name": "CHI18_paper209-Figure2-1.png"}], "name": "medical", "code": "activity@medical", "textinfo": "Medical activity."}, {"children": [{"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 7. A snapshot of the VR simulated driving game", "name": "CHI18_paper401-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper181-Figure1-1.png", "doi": "https://doi.org/10.1145/3287059", "caption": "Fig. 1. FarSight system overview.", "name": "Ubicomp18_paper181-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 3. View from inside vehicle with participant on left, Driving Wizard on right, and Interaction Wizard in backseat, top. Participant\u2019s view with hand in foreground, left. Bird\u2019s eye view of virtual vehicle in virtual world, right.", "name": "CHI18_paper165-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure11-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 11. GridDrones interactive animation of the flight of a butterfly at the LEGO\u00ae World Expo 2018.", "name": "UIST18_paper87-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 1. VR-OOM allows participants to experience the physical sensations of the real-world with the controlled virtual environments and events. Photo by Arjan Reef.", "name": "CHI18_paper165-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper246-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.", "caption": "Figure 3: Experiment setup. Left: participant\u2019s view, the yellow circle shows participants gaze. Right: nal set up with the tablet PC mounted on the right side of the participant.", "name": "CHI18_paper246-Figure3-1.png"}], "name": "driving", "code": "activity@driving", "textinfo": "Driving."}, {"children": [{"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper127-Figure16-1.png", "doi": "https://doi.org/10.1145/3242587.3242619", "caption": "Figure 16. Working with a physical prototype that consists of four foamcore layers (a). The geometry of the model is continuously captured and can be rendered in Blender with the house model of part 1 (b).", "name": "UIST18_paper127-Figure16-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper275-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242592", "caption": "Figure 4. The Jumping sculpture (material: marble; rendered body parts: all). (a) First and final video frames. (b) Novel-view rendering. (c, d) The motion sculpture is inserted back into the original scene and to a synthetic scene, respectively.", "name": "UIST18_paper275-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper289-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173863", "caption": "Figure 1. Dyads performed the first (A) and second (B) tasks in the faceto-face conditions. In virtual reality conditions, avatars appeared across the table from each other (C), but were actually positioned on opposite sides of the motion capture stage (D). In the embodVR condition, participants were able to see both avatars (E). In the no_embodVR condition, participants were unable to see their partner and could only see their hands in the second task, to assist with furniture manipulation (F).", "name": "CHI18_paper289-Figure1-1.png"}], "name": "2D/3D creation", "code": "activity@2D/3D creation", "textinfo": "2D drawing, 3D modelling..."}, {"children": [{"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper177-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173751", "caption": "Figure 2. Our implementation uses a 3D printed PLA dish with an acrylic base plate. 2.4 mm diameter steel electrodes were embedded at 11 mm intervals and driven by a switching circuit.", "name": "CHI18_paper177-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure8-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 8. Prop extension. Parallel extension: a) Extension flattened state. b) Extension pops up. c) Holding in precision grasp. Tilt extension: d) Extension flattened state. e) Extension pops up. f) Holding in pen-like tripod grasp.", "name": "UIST18_paper5-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper151-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173725", "caption": "Figure 4. The Light Writer", "name": "CHI18_paper151-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 3. Example lateral UI elements for a passively mobile shape display: (a) slider, (b) joystick, and (c) knob", "name": "CHI18_paper291-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}], "name": "fabrication", "code": "activity@fabrication", "textinfo": "Physical fabrication."}, {"children": [{"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 3. A snapshot of the virtual Tai Chi training studio", "name": "CHI18_paper401-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper281-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173855", "caption": "Figure 8: VC+ReMa \u2013 Group 1. Clara (TS) uses spatial hand gesture to describe the movement Lina (MS) should execute (annotated for clarity).", "name": "CHI18_paper281-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 6: The switch maneuver allows two apps to switch tiles.", "name": "CHI18_paper241-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 5. Left: P8 (WO2) points toward overview device, other members shifted their attention to it. In NO groups pointing rarely led to shared attention (right): P29 (INT1) points toward her device; other members\u2019 focus stays on own devices.", "name": "CHI18_paper300-Figure5-1.png"}], "name": "communication", "code": "activity@communication", "textinfo": "Communication or exchange between users."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "data manipulation", "code": "activity@data manipulation", "textinfo": "Maps, charts, numbers..."}], "name": "activity", "code": "activity", "textinfo": "What are the users doing with the system?"}], "name": "interactive Scenario", "code": "interactive Scenario", "textinfo": "Key elements of an interactive scenario."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}], "name": "Solo-user", "code": "number@Solo-user", "textinfo": "Only one user is interacting with the system."}, {"children": [{"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 2: Users are confined into tiles. The rotation maneuver allows apps to move their user to the adjacent tile. This way users do not feel confined.", "name": "CHI18_paper241-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}], "name": "Multi-users", "code": "number@Multi-users", "textinfo": "Multiple users are interacting with the system."}], "name": "number", "code": "number", "textinfo": "The number of users involved in the interaction."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}], "name": "hand", "code": "Specific part@hand", "textinfo": "A focus is especially made on user's hands. Nothing else if visible."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "fingers", "code": "Specific part@fingers", "textinfo": "A focus is especially made on user's fingers. Nothing else is visible."}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 7. Adding a cape to a toy figure. a). User doodles to create a spline patch with the AR controller. b). Patch rendered in AR. c). User creates the cape in AR. d). Robot prints directly on the lion model. e). Printed result.", "name": "CHI18_paper579-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 6. The SpaceFace application and its outside view (a), inside view (b) interaction and visualization concept (c) and physical interaction scenario (d).", "name": "CHI18_paper54-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper198-Figure3-1.png", "doi": "https://doi.org/10.1145/3287076", "caption": "Fig. 3. The final design of the HeadGesture set for the nine commands. The movement of head is indicated by the arrows. \"2\u00d7\" represents the repeating of the action for twice. \"1s\" is an illustration for a dwell.", "name": "Ubicomp18_paper198-Figure3-1.png"}], "name": "head", "code": "Specific part@head", "textinfo": "A focus is especially made on the user's head. Nothing else is visible."}, {"children": [{"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper334-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173908", "caption": "Figure 5: Top-ten chosen gestures in each of the three conditions: standing, sitting and projection", "name": "CHI18_paper334-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper65-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242665", "caption": "Figure 2. MetaArms design approach: a closely situated anthropomorphic arms system driven by leg and feet motion, with haptic feedback loop.", "name": "UIST18_paper65-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 8. IB task procedure of Study 2 (*not done in baseline outcome blocks, ** not done in baseline action blocks).", "name": "CHI18_paper541-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper336-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173910 ", "caption": "Figure 5: (a) This shoe sole is flat by default. (b) The user transforms it into a treaded sole it by pulling a string, e.g., when it starts snowing. (c) Note that the sole is functional and robust enough to walk on.", "name": "CHI18_paper336-Figure5-1.png"}], "name": "foot", "code": "Specific part@foot", "textinfo": "A focus is especially made on the user's feet. Nothing else is visible."}], "name": "Specific part", "code": "Specific part", "textinfo": "A focus is made on a specific part of the body."}, {"children": [{"is_image": true, "path": "UIST18_paper499-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 10. Safe zone is defined as a personal region located at the center of the maximum inscribed circle found in the available free space", "name": "UIST18_paper499-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 5. (a) Connecting two edge shields with a proximity gesture forms (b)(c) a wall shield in the VR zone", "name": "UIST18_paper499-Figure5-1.png"}], "name": "lower body", "code": "body part@lower body", "textinfo": "Most of the lower body (legs and feet) is visible."}, {"children": [{"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper170-Figure5-1.png", "doi": "https://doi.org/10.1145/3287048", "caption": "Fig. 5. FarmChat speech-based interaction", "name": "Ubicomp18_paper170-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}], "name": "upper body", "code": "body part@upper body", "textinfo": "Most of the upper body (torso and arms) is visible"}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 14: Making the pendulum reach its two degrees of freedom requires two user interactions: (a) The first hit makes the prop swing back and forth and (b) the second hit allows it to orbit.", "name": "CHI18_paper89-Figure14-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}], "name": "full body", "code": "body part@full body", "textinfo": "Most of the body is visible."}], "name": "body part", "code": "body part", "textinfo": "Which parts of the Human body are represented?"}], "name": "Users", "code": "Users", "textinfo": "How are users represented in the figure?"}, {"children": [{"children": [{"children": [{"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}], "name": "Pointing", "code": "discrete@Pointing", "textinfo": "User is pointing at a target."}, {"children": [{"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper151-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173725", "caption": "Figure 4. The Light Writer", "name": "CHI18_paper151-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 3. Example lateral UI elements for a passively mobile shape display: (a) slider, (b) joystick, and (c) knob", "name": "CHI18_paper291-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}], "name": "Key press", "code": "discrete@Key press", "textinfo": "User is pressing a button."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}], "name": "Symbolic gesture", "code": "discrete@Symbolic gesture", "textinfo": "User is performing a symbolic gesture."}], "name": "discrete", "code": "discrete", "textinfo": "User's actions are really quick and brief over the time."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper737-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242615", "caption": "Figure 1. Example of Magneto-Haptics. (A) Three cylindrical magnets provide a magnetic force to the cube magnet. (B) This feedback creates a sensation as if one is moving an object on a gradient during active touch.", "name": "UIST18_paper737-Figure1-1.png"}], "name": "translating", "code": "continuous@translating", "textinfo": "User is translating an object."}, {"children": [{"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper21-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173595", "caption": "Figure 2. The six rotation directions.", "name": "CHI18_paper21-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper321-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242662", "caption": "Figure 12. Study apparatus for hinge (left) and slide (right).", "name": "UIST18_paper321-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper321-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242662", "caption": "Figure 1. Indutivo recognizes the tap of a conductive objects on a smartwatch, such as (a) a dime, or (b) finger. It can sense (c) the rotation of a bottle cap instrumented using copper tape, (d) hinge of a metal credit card, and (e) slide of the handle of a table knife.", "name": "UIST18_paper321-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 4. Abstracting attributes by demonstration. An arrow\u2019s \u201cangle\u201d attribute is abstracted as a variable using two constraint markers.", "name": "CHI18_paper199-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure13-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 13. Reactile lets a user to abstract attributes as variables through demonstration with blue constraint markers. When the system detects the demonstration, it updates the left panel to show a list of variables and current states.", "name": "CHI18_paper199-Figure13-1.png"}], "name": "rotating", "code": "continuous@rotating", "textinfo": "User is rotating an object."}, {"children": [{"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper76-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173650", "caption": "Figure 3. While telling researchers a story about her photos, P12 zoomed into the right photo in this collage to emphasize the butterfly on her head.", "name": "CHI18_paper76-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 3. Example lateral UI elements for a passively mobile shape display: (a) slider, (b) joystick, and (c) knob", "name": "CHI18_paper291-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper245-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173819", "caption": "Figure 12. Sample interactions demonstrating applications of multirays", "name": "CHI18_paper245-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 5. Modifying the generated mechanism by (a) moving the point and (b) changing the scale", "name": "CHI18_paper411-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 12. Workspace scaling tool for drawing a car. The user uses a large workspace to draw the shape of the car in a small, comfortable, scale (left). She then defines a small workspace centered on a headlight (right). This \u201czoom in\u201d effect allows her to capture the details of the headlight\u2019s shape, define a new canvas on it, and draw highlights.", "name": "CHI18_paper185-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper142-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173716", "caption": "Figure 1. Our framework enables users to both pan & zoom the context view and to create independent focus views, either DragMags or lenses.", "name": "CHI18_paper142-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper159-Figure6-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 6. The at-home setup with a TV set facilitates the edutainment value of the task.", "name": "CSCW18_paper159-Figure6-1.png"}], "name": "scaling", "code": "continuous@scaling", "textinfo": "User is scaling an object."}, {"children": [{"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper339-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173913", "caption": "Figure 5. Seven out of our 10 participants interacting with the probe. P2, PII and PIX use the slider. P3 and P5 are about the change the shape by clicking on the central button. P6 and P8 use the rotary knob.", "name": "CHI18_paper339-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper185-Figure4-1.png", "doi": "https://doi.org/10.1145/3274454", "caption": "Fig. 4. Emma scrolling to the next clue while Olivia moves the big picture to the new clue location. I-Spy images courtesy of Maria Neradova.", "name": "CSCW18_paper185-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper245-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173819", "caption": "Figure 12. Sample interactions demonstrating applications of multirays", "name": "CHI18_paper245-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper123-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173697", "caption": "Figure 8: Collections with layouts: (a) Repeat Grid. (b) Partition Stack. (c) Partition Stacks Nested in a Repeat Grid", "name": "CHI18_paper123-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper867-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242636", "caption": "Figure 5. Shows the representation of the hands in virtual reality. Also the described pinch gesture can be seen which enables the user to perform indirect windowing and scrolling.", "name": "UIST18_paper867-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper249-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173823", "caption": "Figure 4: Menu techniques: (a) M3 Gesture Menu, (b) Multi-Stroke Marking Menu, and (c) Linear Menu. Screenshots were taken when selecting terminal targets on the second-level menus with visual feedback.", "name": "CHI18_paper249-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper73-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173647", "caption": "Figure 4. (top) In TILT, SS points at a location where he needs LS to move his ROI, (bottom) In TOUCH, SS uses \u201cflexible ownership\u201d feature to drive LS\u2019s ROI to desired location.", "name": "CHI18_paper73-Figure4-1.png"}], "name": "scrolling", "code": "continuous@scrolling", "textinfo": "User is scrolling a view."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper569-Figure27-1.png", "doi": "https://doi.org/10.1145/3173574.3174143", "caption": "Figure 27. Lampshade", "name": "CHI18_paper569-Figure27-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure8-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 8. Prop extension. Parallel extension: a) Extension flattened state. b) Extension pops up. c) Holding in precision grasp. Tilt extension: d) Extension flattened state. e) Extension pops up. f) Holding in pen-like tripod grasp.", "name": "UIST18_paper5-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 2: The cross-section of two resistive-coated yarns, which form a resistor (a) that can measure applied force (b).", "name": "UIST18_paper745-Figure2-1.png"}], "name": "deforming", "code": "continuous@deforming", "textinfo": "User is deforming [a part of] an object/the system."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 4. The Canvas sketching interface: The final composition (left) and the interface (right) show the \u201cuser overlay\u201d. Both sides have a list of sketches and animation controls at the bottom. The in/out buttons make the selected sketches appear/disappear. The sliders control the stroke-start, now at 0%, and the stroke-end, now at 100%.", "name": "UIST18_paper675-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}], "name": "writing/drawing", "code": "continuous@writing/drawing", "textinfo": "User is writing or drawing."}], "name": "continuous", "code": "continuous", "textinfo": "The user is continuously interacting with the system."}], "name": "action", "code": "action", "textinfo": "How is the user action?"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}], "name": "touch interaction", "code": "Interaction type@touch interaction", "textinfo": "Interaction relying on a touch sensitive surface."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}], "name": "mid-air interaction", "code": "Interaction type@mid-air interaction", "textinfo": "Interaction is performed in mid-air. The system may involve controllers, tracking."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}], "name": "distal interaction", "code": "Interaction type@distal interaction", "textinfo": "Directly interacting with objects from a distance (through intermediate devices or not)."}, {"children": [{"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper192-Figure4-1.png", "doi": "https://doi.org/10.1145/3274461", "caption": "Figure 4: Interrupting. The child is about to tap the screen; dad reaches out to stop the action and pulls his hand away.", "name": "CSCW18_paper192-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper765-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242647", "caption": "Figure 1. We explore skin irritations, such as itching, by means of electrical stimulation for use as a feedback modality. The sensations generated with this method range from gentle and soothing ones all the way to stinging and irritating ones. Compared to vibrotactile feedback, itching grabs attention more and increases the urge to react.", "name": "UIST18_paper765-Figure1-1.png"}], "name": "on body interaction", "code": "Interaction type@on body interaction", "textinfo": "The interactive system is on the Human body."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper622-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174196", "caption": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "name": "CHI18_paper622-Figure4-1.png"}], "name": "computer interaction ", "code": "Interaction type@computer interaction ", "textinfo": "Classic computer interaction (using keyboard/touchpad/mouse)."}, {"children": [{"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper450-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174024", "caption": "Figure 10. Proposed applications.", "name": "CHI18_paper450-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}], "name": "tangible interaction", "code": "Interaction type@tangible interaction", "textinfo": "Interaction with the system is made though tangible objects."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper521-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242601", "caption": "Figure 1. Left: Users select a destination to teleport to using their controller with a raycast. Right; a portal (blue circle) that shows a preview of the location to be teleported to appears either to the left, right or center depending on the position of the user in the tracking space. Users must step into the portal to activate teleportation, which unobtrusively reorients and repositions them away from the tracking space boundary in order to increase available walking space.", "name": "UIST18_paper521-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 4: The focus maneuver temporarily provides the badminton app with control over most of the physical space. The other apps go into a defocus state.", "name": "CHI18_paper241-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 9. (a) Once the HMD user approaches the aggressive shield installed at the door, (b) the system pauses the game with a grayed out viewport, and (c) lead the user to the safe zone indicated by a green halo. The game resumes with a button on the controller.", "name": "UIST18_paper499-Figure9-1.png"}], "name": "controllers interaction", "code": "Interaction type@controllers interaction", "textinfo": "Interaction with the system is made with controllers (joysticks, tv remotes)."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 4. The Canvas sketching interface: The final composition (left) and the interface (right) show the \u201cuser overlay\u201d. Both sides have a list of sketches and animation controls at the bottom. The in/out buttons make the selected sketches appear/disappear. The sliders control the stroke-start, now at 0%, and the stroke-end, now at 100%.", "name": "UIST18_paper675-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 9. Basic workflow: 1. Sketch a design on fabric, 2. adhere Circuitry Stickers, 3. draw the circuit and electrical connections, 4. capture a picture of the sketch, 5. insert the fabric into the embroidery machine for stitching, and 6. replace the stickers with hardware components.", "name": "CHI18_paper82-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 9: Application highlights: (a) switching from pencil to highlighter with Side and Heel postures (document annotation); (b) choosing pen colour from radial menu with Side-RingOut-PinkyOut (document annotation); (c) object creation menu with Side-PinkyOut (vector drawing); (d) using SidePinkyIn to use handwriting recognition for creating a text object (vector drawing); (e) gesture command mode using Float-Index (vector drawing).", "name": "UIST18_paper825-Figure9-1.png"}], "name": "pen interaction", "code": "Interaction type@pen interaction", "textinfo": "Interaction with the system is made with a pen."}, {"children": [{"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper288-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173862", "caption": "Figure 1. The experimental setup (a) and a close-up view of the grid interface (b) in the study.", "name": "CHI18_paper288-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 10. Compact fractal radial menus with deep structure in GazeBrowser. a) Example application controlling humidity and temperature sensors. b) HoloLens screenshot of menu used with device refinement. c) A close-up showing selection of very small radial menu items using Pinpointing. The selected item is the small yellow dot marked by the crosshair.", "name": "CHI18_paper81-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 4. Shared attention on the overview device (SSV; left) often led to active discussion (AD; right) as the device gave the group a common focus and starting point for a diccussion.", "name": "CHI18_paper300-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper349-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173923", "caption": "Figure 3. Experiment setup.", "name": "CHI18_paper349-Figure3-1.png"}], "name": "gaze interaction", "code": "Interaction type@gaze interaction", "textinfo": "Interaction with the system is made with the eyes."}], "name": "Interaction type", "code": "Interaction type", "textinfo": "What is the type of interaction involved?"}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}], "name": "outside in", "code": "tracking based@outside in", "textinfo": "User is tracked by the system with external sensors."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper288-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173862", "caption": "Figure 1. The experimental setup (a) and a close-up view of the grid interface (b) in the study.", "name": "CHI18_paper288-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper340-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173914", "caption": "Figure 3: Breathing actions and corresponding effects in the FPS game.", "name": "CHI18_paper340-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper610-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174184", "caption": "Figure 1. Example use case of the proposed model: Different levels of detail are shown depending on perceivable screen resolution based on the device\u2019s position and orientation in the field of view.", "name": "CHI18_paper610-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper362-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 3. Users were nine times more likely to check their tracker during vigorous physical activity than when sedentary. Such engagements increased in frequency near goal completion and fueled users\u2019 motivation to meet their goals", "name": "CHI18_paper362-Figure3-1.png"}], "name": "inside out", "code": "tracking based@inside out", "textinfo": "User is tracked by the system with sensors on/inside him."}], "name": "tracking based", "code": "tracking based", "textinfo": "User is tracked by the system."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}], "name": "mobile devices", "code": "device based@mobile devices", "textinfo": "The user is interacting with the system by using a mobile device: smartphones, tablets, smartwatches, etc."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 1. A visually impaired student exploring the multisensory map. The system augments a tactile map with projection and audio output.", "name": "CHI18_paper629-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 9. On rare locations we observed helping and blocking behavior to gain benefits for the team or over the competitor.", "name": "CHI18_paper539-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}], "name": "large surfaces", "code": "device based@large surfaces", "textinfo": "The user is interacting with the system though large screens: screen on a wall, touch table, tv, etc."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper347-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242589", "caption": "Figure 3: FingerArc and FingerChord: (a) holding an action key with a special hand posture for a predesignated delay time reveals the shortcut interface; (b) selecting the primary command using the index finger with others tucked in (FingerArc) or the middle finger (FingerChord); (c-e) selecting other commands using the angle of the thumb (FingerArc) or pressing different key areas (FingerChord); (f) releasing the key maintaining a hand posture triggers the command (e.g. primary command); (g) revealing all the fingers while holding the key cancels the operation.", "name": "UIST18_paper347-Figure3-1.png"}], "name": "desktop devices", "code": "device based@desktop devices", "textinfo": "The user is interacting with the system through desktop input devices: mouse, keyboad, touchpad on a desk, etc. "}, {"children": [{"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper622-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174196", "caption": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "name": "CHI18_paper622-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper20-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173594", "caption": "Figure 2. (A) A low vision user using SteeringWheel with Surface Dial. (B) The Surface Dial and its gestures.", "name": "CHI18_paper20-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper353-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173927", "caption": "Figure 4. Example of a designer using ProtoAR\u2019s interactive capture tools to for physical-digital prototyping of a mobile AR furniture placement app.", "name": "CHI18_paper353-Figure4-1.png"}, {"is_image": true, "path": "Ubicomp18_paper176-Figure3-1.png", "doi": "https://doi.org/10.1145/3287054", "caption": "Fig. 3. (a) H4.F pointing at H4.M\u2019s shower data, (b) vice versa.", "name": "Ubicomp18_paper176-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 6: A table (shown on screen). Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys, and (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the position of the cell and its content are read out aloud.", "name": "CHI18_paper11-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 7: A user is searching a table (shown on screen) for the word \u2018Jill\u2019. Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys. (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the number of occurrences of the search query in the respective column or row are read aloud. When the query is found, the position and content of the cell are read out aloud.", "name": "CHI18_paper11-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper21-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173595", "caption": "Figure 1. We propose to extend keyboard shortcuts with arm and wrist rotations gestures, performed while pressing down a key. The figure shows left/right wrist rolls. The user\u2019s rotation angles can be used, for example, for continuous control, such as changing the volume.", "name": "CHI18_paper21-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper281-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173855", "caption": "Figure 1. Remote Manipulator (ReMa) has two parts: it detects manipulations on an object (Left-yellow) using a set of sensors (Left-red), and then reproduces these with a proxy object (Right-yellow) using a Baxter robot arm (Right-red). ReMa allows shows the Manipulator Site collaborator (Right) the object with the same orientation as at the Tracking Site (Left). Collaborators can also use video chat (blue).", "name": "CHI18_paper281-Figure1-1.png"}], "name": "laptop devices", "code": "device based@laptop devices", "textinfo": "The user is interacting with the system through laptop input devices: integrated keyboad, integrated touchpad, etc."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper340-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173914", "caption": "Figure 3: Breathing actions and corresponding effects in the FPS game.", "name": "CHI18_paper340-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper521-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242601", "caption": "Figure 1. Left: Users select a destination to teleport to using their controller with a raycast. Right; a portal (blue circle) that shows a preview of the location to be teleported to appears either to the left, right or center depending on the position of the user in the tracking space. Users must step into the portal to activate teleportation, which unobtrusively reorients and repositions them away from the tracking space boundary in order to increase available walking space.", "name": "UIST18_paper521-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}], "name": "controllers", "code": "device based@controllers", "textinfo": "The user is interacting with the system using a controller (tv remote, game controller, joystick...)"}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper450-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174024", "caption": "Figure 10. Proposed applications.", "name": "CHI18_paper450-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}], "name": "tangible objects", "code": "device based@tangible objects", "textinfo": "The user is interacting with tangible objects"}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 2. Scenariot localization principle.", "name": "CHI18_paper219-Figure2-1.png"}], "name": "AR", "code": "device based@AR", "textinfo": "The user is interacting in an AR context"}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper340-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173914", "caption": "Figure 3: Breathing actions and corresponding effects in the FPS game.", "name": "CHI18_paper340-Figure3-1.png"}], "name": "VR", "code": "device based@VR", "textinfo": "The user is interacting in a VR context"}], "name": "device based", "code": "device based", "textinfo": "The user is interacting with the system through a manipulable device."}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper170-Figure5-1.png", "doi": "https://doi.org/10.1145/3287048", "caption": "Fig. 5. FarmChat speech-based interaction", "name": "Ubicomp18_paper170-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 3. Intermodulation II, 11/10/2016", "name": "CHI18_paper160-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 3. (a) The first version of the probe is non interactive. Children used it to built tactile bracelets and generate novel scenarios for documenting and reflecting on field-trips (here, in a museum). (b) The second version is a functional bracelet recording and playing sound.", "name": "CHI18_paper236-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 2. The second version of the probe is worn on the wrist. It consists of a strap covered with velcro on which an audio recorder/player.", "name": "CHI18_paper236-Figure2-1.png"}], "name": "speech input", "code": "sound based@speech input", "textinfo": "The user is interacting with the system by speaking through a microphone."}], "name": "sound based", "code": "sound based", "textinfo": "The system is listening to the sounds emitted by the user."}], "name": "input channel", "code": "input channel", "textinfo": "How is the user actually interacting with the system?"}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper450-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174024", "caption": "Figure 10. Proposed applications.", "name": "CHI18_paper450-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}], "name": "haptic", "code": "output modality@haptic", "textinfo": "There is an haptic response from the system."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}], "name": "large displays", "code": "visual@large displays", "textinfo": "Visual feedback through large displays (tv, wall screen, touch table..)"}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper127-Figure16-1.png", "doi": "https://doi.org/10.1145/3242587.3242619", "caption": "Figure 16. Working with a physical prototype that consists of four foamcore layers (a). The geometry of the model is continuously captured and can be rendered in Blender with the house model of part 1 (b).", "name": "UIST18_paper127-Figure16-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}], "name": "computer displays", "code": "visual@computer displays", "textinfo": "Visual feedback through computer screens (laptop, desktop, car embedded display...)"}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "mobile displays", "code": "visual@mobile displays", "textinfo": "Visual feedback through mobile displays (smartphones, tablets, smartwatches...)"}, {"children": [{"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 1. A visually impaired student exploring the multisensory map. The system augments a tactile map with projection and audio output.", "name": "CHI18_paper629-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 3. A snapshot of the virtual Tai Chi training studio", "name": "CHI18_paper401-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper173-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173747", "caption": "Figure 3. Illustration of the experimental setup: participants stood 2 meters away from a rear-projector screen. The input device was connected via USB to the computer in order to guarantee stable data transfer for smooth control.", "name": "CHI18_paper173-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure13-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 13. Reactile lets a user to abstract attributes as variables through demonstration with blue constraint markers. When the system detects the demonstration, it updates the left panel to show a list of variables and current states.", "name": "CHI18_paper199-Figure13-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 4. Generating a linkage mechanism through (a) placing the AR marker board on the test board and pressing the record button, (b) moving the AR marker board, and (c) pressing the record button again. Then, the system generates the linkage mechanism.", "name": "CHI18_paper411-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 4. A snapshot of the VR shooting game", "name": "CHI18_paper401-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 7. Construction Mode: building a map using Wikki Stix and magnets combined with projection and audio feedback.", "name": "CHI18_paper629-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 5. Modifying the generated mechanism by (a) moving the point and (b) changing the scale", "name": "CHI18_paper411-Figure5-1.png"}], "name": "projected displays", "code": "visual@projected displays", "textinfo": "Visual feedback through a projected image on a surface."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}], "name": "head mounted displays", "code": "visual@head mounted displays", "textinfo": "AR, VR displays."}, {"children": [{"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 3. Alternative BioFidget designs. (a) Basic design. (b) BioFidget with an additional clip for PPG sensing stabilization. (c) Fan-shaped wing to react to respiration. (d) BioFidget with a handheld display for rich visual biofeedback.", "name": "CHI18_paper613-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper569-Figure27-1.png", "doi": "https://doi.org/10.1145/3173574.3174143", "caption": "Figure 27. Lampshade", "name": "CHI18_paper569-Figure27-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper151-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173725", "caption": "Figure 4. The Light Writer", "name": "CHI18_paper151-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper5-Figure7-1.png", "doi": "https://doi.org/10.1145/10.1145/3173574.3173579", "caption": "Figure 7. Computationally corresponding action with electroluminescent light intensity.", "name": "CHI18_paper5-Figure7-1.png"}], "name": "lights", "code": "visual@lights", "textinfo": "Lamps, leds, any bulbs."}], "name": "visual", "code": "visual", "textinfo": "Response from the system involves a screen, a display."}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CSCW18_paper192-Figure8-1.png", "doi": "https://doi.org/10.1145/3274461", "caption": "Figure 8: American parents sat back, away from the child, when playing the instructional game.", "name": "CSCW18_paper192-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 1. A visually impaired student exploring the multisensory map. The system augments a tactile map with projection and audio output.", "name": "CHI18_paper629-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 5. Experimental tasks for the three timing methods: Libet clock (left), Visual Alphabet (middle) and Audio Alphabet (right).", "name": "CHI18_paper541-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 5. The second method to access different block types: audio-cue typed blocks, when a typed block in the toolbox and the blocks in the workspace that accept it play the same distinct audio cues.", "name": "CHI18_paper69-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 7. I/O Braid USB-C headphones with embedded gesture recognition and audio feedback for music control, enabled by Nanoboard. The user starts playback by pinching, then rolls the I/O Braid to increase the volume.", "name": "UIST18_paper485-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 6. Student using our prototype during exploration mode: tactile map combined with projection and audio output.", "name": "CHI18_paper629-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 8. IB task procedure of Study 2 (*not done in baseline outcome blocks, ** not done in baseline action blocks).", "name": "CHI18_paper541-Figure8-1.png"}], "name": "sound based", "code": "audible@sound based", "textinfo": "The system emits specific sounds."}, {"children": [{"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 3. (a) The first version of the probe is non interactive. Children used it to built tactile bracelets and generate novel scenarios for documenting and reflecting on field-trips (here, in a museum). (b) The second version is a functional bracelet recording and playing sound.", "name": "CHI18_paper236-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 2. Two methods to move blocks: (a) audio-guided drag and drop, which speaks aloud the location of the block as it is dragged across the screen (gray box indicates audio output of program) and (b) location-first select, select, drop, where a location is selected via gray \u201cconnection blocks\u201d, then the toolbox of blocks that can be placed there appears.", "name": "CHI18_paper69-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 4. The first method to access different block types: embedded typed blocks, accessed from a menu embedded within each block (e.g. \"Repeat 2/3 times\")", "name": "CHI18_paper69-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 3. Two methods to indicate the spatial structure of the code: (a) a spatial representation with nested statements placed vertically above inner blocks of enclosing statements, and (b) an audio representation with nesting communicated aurally with spearcons (shortened audio representations of words).", "name": "CHI18_paper69-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 7: A user is searching a table (shown on screen) for the word \u2018Jill\u2019. Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys. (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the number of occurrences of the search query in the respective column or row are read aloud. When the query is found, the position and content of the cell are read out aloud.", "name": "CHI18_paper11-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 2. The second version of the probe is worn on the wrist. It consists of a strap covered with velcro on which an audio recorder/player.", "name": "CHI18_paper236-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper473-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242620", "caption": "Figure 3. Display rack in a smart showroom. (a) An RFIMatch module is mounted behind each item. Several RFIMatch fingerstalls are provided with voice guidance in different languages. (b) A visitor wearing the German fingerstall hears the embedded audio message in German by touching the icon. (c) Two users wearing different fingerstalls greet each other in their own languages.", "name": "UIST18_paper473-Figure3-1.png"}], "name": "speech output", "code": "audible@speech output", "textinfo": "The system speaks to the user in his language."}], "name": "audible", "code": "audible", "textinfo": "The response from the system is audible."}], "name": "output modality", "code": "output modality", "textinfo": "System response"}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 6. Student using our prototype during exploration mode: tactile map combined with projection and audio output.", "name": "CHI18_paper629-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper176-Figure3-1.png", "doi": "https://doi.org/10.1145/3287054", "caption": "Fig. 3. (a) H4.F pointing at H4.M\u2019s shower data, (b) vice versa.", "name": "Ubicomp18_paper176-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 9. Virtual content such as a terrain map can be modified and physically explored real-time using physical proxies such as a wand (left). Two displays are used to physically render two different virtual houses (right).", "name": "CHI18_paper291-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 1: (left) AirBnb (https://www.airbnb.com/s/places), with the initial selection chosen by SPRITEs highlighted when the user presses the topmost key in the rightmost keyboard key. (middle) When the user presses \u2018\\\u2019 key, SPRITEs reads out \u201cmenu\u201d and double pressing \u2018\\\u2019 activates the menu on the numeric row. (right) Pressing the \u20181\u2019 key on the numeric row reads out the first element in the menu.", "name": "CHI18_paper11-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper95-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173669", "caption": "Figure 4. Top of screen: raw data from our ten time-offlight sensors (red dots), with estimated touch point shown in green. Bottom of screen: resulting touch paths. On arm: current path is projected for debugging.", "name": "CHI18_paper95-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper426-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174000", "caption": "Figure 4. JND study setup", "name": "CHI18_paper426-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 6. Setup: the user puts on the HoloLens and draws with a motiontracked stylus, on a tablet (left), or mid-air (right) using a mouse affixed to the back of the tablet.", "name": "CHI18_paper185-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 8. The Letter Plates Montessori exercise provides feedback as words are formed and tactile letter-shapes are traced. Paper overlays on the Mat can provide additional context to the exercise.", "name": "CHI18_paper515-Figure8-1.png"}], "name": "Desktop", "code": "Situation@Desktop", "textinfo": "On a classic desk/table (mostly involving desktop computers, keyboard, mouse...)"}, {"children": [{"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 5. Left: P8 (WO2) points toward overview device, other members shifted their attention to it. In NO groups pointing rarely led to shared attention (right): P29 (INT1) points toward her device; other members\u2019 focus stays on own devices.", "name": "CHI18_paper300-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 3. View from inside vehicle with participant on left, Driving Wizard on right, and Interaction Wizard in backseat, top. Participant\u2019s view with hand in foreground, left. Bird\u2019s eye view of virtual vehicle in virtual world, right.", "name": "CHI18_paper165-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper581-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242599", "caption": "Figure 10. Study 3 was conducted on a subway", "name": "UIST18_paper581-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 1. VR-OOM allows participants to experience the physical sensations of the real-world with the controlled virtual environments and events. Photo by Arjan Reef.", "name": "CHI18_paper165-Figure1-1.png"}], "name": "Public/private transport", "code": "Situation@Public/private transport", "textinfo": "In a car, bus, plane, on a bike..."}, {"children": [{"is_image": true, "path": "CHI18_paper362-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 1. Fitbit Flex (left), Fitbit Charge HR (right)", "name": "CHI18_paper362-Figure1-1.png"}], "name": "Outdoors", "code": "Situation@Outdoors", "textinfo": "Park, streets."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 6: The user configures the intensity of the desired light bulb using a cup as a stand-in for a dial (passive prop). Using EMS force feedback, our system augments the tangible with constraints and detents.", "name": "CHI18_paper446-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper511-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242648", "caption": "Figure 12: (a) Our first control condition implements a teleport functionality and displays chaperone bounds to keep users from leaving the tracking volume. (b) Our second control condition, scaled motion, changes the mapping of physical motion (solid line) to virtual motion (dashed line).", "name": "UIST18_paper511-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper213-Figure20-1.png", "doi": "https://doi.org/10.1145/3242587.3242609", "caption": "Figure 20. Smartwatches could track additional health information, such as coughing (A), and recommend actions (B).", "name": "UIST18_paper213-Figure20-1.png"}, {"is_image": true, "path": "CHI18_paper73-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173647", "caption": "Figure 4. (top) In TILT, SS points at a location where he needs LS to move his ROI, (bottom) In TOUCH, SS uses \u201cflexible ownership\u201d feature to drive LS\u2019s ROI to desired location.", "name": "CHI18_paper73-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper142-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173716", "caption": "Figure 1. Our framework enables users to both pan & zoom the context view and to create independent focus views, either DragMags or lenses.", "name": "CHI18_paper142-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 5. Example of video taken for all three body postures", "name": "CHI18_paper202-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper159-Figure6-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 6. The at-home setup with a TV set facilitates the edutainment value of the task.", "name": "CSCW18_paper159-Figure6-1.png"}], "name": "Private indoors", "code": "Situation@Private indoors", "textinfo": "At the office, at work, at home..."}, {"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper61-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173635", "caption": "Figure 1. Left: Original Wire Costume (Oskar Schlemmer, Draht-Figur 1922). Photo \u00a9 Staatsgalerie Stuttgart. Right: Our version of the costume.", "name": "CHI18_paper61-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure11-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 11. GridDrones interactive animation of the flight of a butterfly at the LEGO\u00ae World Expo 2018.", "name": "UIST18_paper87-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 18: Participant balancing the marble (image from the study, with consent of the participant).", "name": "CHI18_paper446-Figure18-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 12. SynchronizAR supports spontaneous collaboration, i.e., a new user (b) join an existing AR collaboration (a) instantly (c).", "name": "UIST18_paper19-Figure12-1.png"}], "name": "Public indoors", "code": "Situation@Public indoors", "textinfo": "Malls, stations, museums, at gym. Might involve a crowded place."}], "name": "Situation", "code": "Situation", "textinfo": "Where does the interaction take place?"}], "name": "Interactive system", "code": "Interactive system", "textinfo": "Key elements describing an interactive system."}], "name": "Taxonomy", "code": "Taxonomy", "textinfo": "These different categories present the essential elements to describe and represent interactive scenarios."}, "what_codes_hierarchies": [{"children": [{"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}], "name": "design space", "code": "purpose@design space", "textinfo": "The figure illustrates a set of functionalities, possibilities, commands or use cases available in a system."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}], "name": "interaction sequence", "code": "purpose@interaction sequence", "textinfo": "The figure illustrates an interaction sequence (or storyboard), i.e. a sequence of actions that must be accomplished in a specific order to launch a system command."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}], "name": "interactive system", "code": "purpose@interactive system", "textinfo": "The figure illustrates an interactive system but is focused on the setup of this system and its sensing capabilities rather than on the interaction between the user and the system."}], "name": "purpose", "code": "purpose", "textinfo": "What is the purpose of the figure?"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}], "name": "moving", "code": "time@moving", "textinfo": "Time is evolving across the figure, i.e. we can see a transition, progression, within the user's actions and/or the system's responses."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}], "name": "still", "code": "time@still", "textinfo": "We are observing the user and/or the system at a specific moment in time, without any information on the next/previous user's actions or system's responses."}], "name": "time", "code": "time", "textinfo": "How time is represented in the figure?"}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper170-Figure5-1.png", "doi": "https://doi.org/10.1145/3287048", "caption": "Fig. 5. FarmChat speech-based interaction", "name": "Ubicomp18_paper170-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 4. The Canvas sketching interface: The final composition (left) and the interface (right) show the \u201cuser overlay\u201d. Both sides have a list of sketches and animation controls at the bottom. The in/out buttons make the selected sketches appear/disappear. The sliders control the stroke-start, now at 0%, and the stroke-end, now at 100%.", "name": "UIST18_paper675-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper439-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174013", "caption": "Figure 6. Experiment 2 setup. A user held the device in one hand, and typed on the invisible keyboard with her preferred posture. The spatially-adapted and the unadapted keyboards shared the same appearance.", "name": "CHI18_paper439-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper339-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173913", "caption": "Figure 5. Seven out of our 10 participants interacting with the probe. P2, PII and PIX use the slider. P3 and P5 are about the change the shape by clicking on the central button. P6 and P8 use the rotary knob.", "name": "CHI18_paper339-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper45-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173619", "caption": "Figure 1. One participant is reading with RSVP while walking during the study. In the study, we investigated the effect of different text positions and presentation types on binocular see-through smart glasses.", "name": "CHI18_paper45-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper622-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174196", "caption": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "name": "CHI18_paper622-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper224-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173798", "caption": "Figure 4. Posting a digital sticky note to the whiteboard is a two-step process. First, the user touches the NoteCanvas at the location where the sticky note should appear, which enables the post button on the NoteCreator. Second, the user taps the post button on the NoteCreator, which will post the sticky note to the whiteboard and remove it from the NoteCreator (indicated with a semi-transparent sticky note).", "name": "CHI18_paper224-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 9: Application highlights: (a) switching from pencil to highlighter with Side and Heel postures (document annotation); (b) choosing pen colour from radial menu with Side-RingOut-PinkyOut (document annotation); (c) object creation menu with Side-PinkyOut (vector drawing); (d) using SidePinkyIn to use handwriting recognition for creating a text object (vector drawing); (e) gesture command mode using Float-Index (vector drawing).", "name": "UIST18_paper825-Figure9-1.png"}], "name": "production", "code": "activity@production", "textinfo": "Digital content production: programming, text entry."}, {"children": [{"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper53-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242626", "caption": "Figure 6: As a partner, MobiLimb can express behaviors and embody virtual agents. a) Cat with a tail, that reacts to users\u2019 actions. b) Hostile scorpion. c) Curious device. d) Assistive guide showing how to scroll on a page.", "name": "UIST18_paper53-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}], "name": "entertainment", "code": "activity@entertainment", "textinfo": "Video, music, video games..."}, {"children": [{"is_image": true, "path": "CHI18_paper362-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 7. Users maintained an awareness of their activity levels and formed micro-plans such as grabbing water while waiting for the printer, or walking 1000 steps in the next hour.", "name": "CHI18_paper362-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 3. Alternative BioFidget designs. (a) Basic design. (b) BioFidget with an additional clip for PPG sensing stabilization. (c) Fan-shaped wing to react to respiration. (d) BioFidget with a handheld display for rich visual biofeedback.", "name": "CHI18_paper613-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper867-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242636", "caption": "Figure 2. The direct interaction technique enables the radiologists to interact with the environment by performing the selected gestures at the working zone.", "name": "UIST18_paper867-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper362-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 3. Users were nine times more likely to check their tracker during vigorous physical activity than when sedentary. Such engagements increased in frequency near goal completion and fueled users\u2019 motivation to meet their goals", "name": "CHI18_paper362-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 1. ActiveErgo is the first active approach to improving ergonomics by providing automatic and personalized computer workspace adjustment. Our prototype uses a Microsoft Kinect sensor for skeletal tracking and uses robotic arms and motorized desk to provide automatic workspace adjustment.", "name": "CHI18_paper558-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper362-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 4. Trackers were checked strategically towards gaining insights on behaviors \u2013 such as right before starting, and after finishing walking a dog, to know how many steps were gained.", "name": "CHI18_paper362-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper209-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173783", "caption": "Figure 2. Learner is \u2018wearing\u2019 an expert\u2019s experience, performing an ultrasound diagnosis with the player application", "name": "CHI18_paper209-Figure2-1.png"}], "name": "medical", "code": "activity@medical", "textinfo": "Medical activity."}, {"children": [{"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 7. A snapshot of the VR simulated driving game", "name": "CHI18_paper401-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper181-Figure1-1.png", "doi": "https://doi.org/10.1145/3287059", "caption": "Fig. 1. FarSight system overview.", "name": "Ubicomp18_paper181-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 3. View from inside vehicle with participant on left, Driving Wizard on right, and Interaction Wizard in backseat, top. Participant\u2019s view with hand in foreground, left. Bird\u2019s eye view of virtual vehicle in virtual world, right.", "name": "CHI18_paper165-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure11-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 11. GridDrones interactive animation of the flight of a butterfly at the LEGO\u00ae World Expo 2018.", "name": "UIST18_paper87-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 1. VR-OOM allows participants to experience the physical sensations of the real-world with the controlled virtual environments and events. Photo by Arjan Reef.", "name": "CHI18_paper165-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper246-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.", "caption": "Figure 3: Experiment setup. Left: participant\u2019s view, the yellow circle shows participants gaze. Right: nal set up with the tablet PC mounted on the right side of the participant.", "name": "CHI18_paper246-Figure3-1.png"}], "name": "driving", "code": "activity@driving", "textinfo": "Driving."}, {"children": [{"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper127-Figure16-1.png", "doi": "https://doi.org/10.1145/3242587.3242619", "caption": "Figure 16. Working with a physical prototype that consists of four foamcore layers (a). The geometry of the model is continuously captured and can be rendered in Blender with the house model of part 1 (b).", "name": "UIST18_paper127-Figure16-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper275-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242592", "caption": "Figure 4. The Jumping sculpture (material: marble; rendered body parts: all). (a) First and final video frames. (b) Novel-view rendering. (c, d) The motion sculpture is inserted back into the original scene and to a synthetic scene, respectively.", "name": "UIST18_paper275-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper289-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173863", "caption": "Figure 1. Dyads performed the first (A) and second (B) tasks in the faceto-face conditions. In virtual reality conditions, avatars appeared across the table from each other (C), but were actually positioned on opposite sides of the motion capture stage (D). In the embodVR condition, participants were able to see both avatars (E). In the no_embodVR condition, participants were unable to see their partner and could only see their hands in the second task, to assist with furniture manipulation (F).", "name": "CHI18_paper289-Figure1-1.png"}], "name": "2D/3D creation", "code": "activity@2D/3D creation", "textinfo": "2D drawing, 3D modelling..."}, {"children": [{"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper177-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173751", "caption": "Figure 2. Our implementation uses a 3D printed PLA dish with an acrylic base plate. 2.4 mm diameter steel electrodes were embedded at 11 mm intervals and driven by a switching circuit.", "name": "CHI18_paper177-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure8-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 8. Prop extension. Parallel extension: a) Extension flattened state. b) Extension pops up. c) Holding in precision grasp. Tilt extension: d) Extension flattened state. e) Extension pops up. f) Holding in pen-like tripod grasp.", "name": "UIST18_paper5-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper151-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173725", "caption": "Figure 4. The Light Writer", "name": "CHI18_paper151-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 3. Example lateral UI elements for a passively mobile shape display: (a) slider, (b) joystick, and (c) knob", "name": "CHI18_paper291-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}], "name": "fabrication", "code": "activity@fabrication", "textinfo": "Physical fabrication."}, {"children": [{"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 3. A snapshot of the virtual Tai Chi training studio", "name": "CHI18_paper401-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper281-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173855", "caption": "Figure 8: VC+ReMa \u2013 Group 1. Clara (TS) uses spatial hand gesture to describe the movement Lina (MS) should execute (annotated for clarity).", "name": "CHI18_paper281-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 6: The switch maneuver allows two apps to switch tiles.", "name": "CHI18_paper241-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 5. Left: P8 (WO2) points toward overview device, other members shifted their attention to it. In NO groups pointing rarely led to shared attention (right): P29 (INT1) points toward her device; other members\u2019 focus stays on own devices.", "name": "CHI18_paper300-Figure5-1.png"}], "name": "communication", "code": "activity@communication", "textinfo": "Communication or exchange between users."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "data manipulation", "code": "activity@data manipulation", "textinfo": "Maps, charts, numbers..."}], "name": "activity", "code": "activity", "textinfo": "What are the users doing with the system?"}], "name": "interactive Scenario", "code": "interactive Scenario", "textinfo": "Key elements of an interactive scenario."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}], "name": "Solo-user", "code": "number@Solo-user", "textinfo": "Only one user is interacting with the system."}, {"children": [{"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 2: Users are confined into tiles. The rotation maneuver allows apps to move their user to the adjacent tile. This way users do not feel confined.", "name": "CHI18_paper241-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}], "name": "Multi-users", "code": "number@Multi-users", "textinfo": "Multiple users are interacting with the system."}], "name": "number", "code": "number", "textinfo": "The number of users involved in the interaction."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}], "name": "hand", "code": "Specific part@hand", "textinfo": "A focus is especially made on user's hands. Nothing else if visible."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "fingers", "code": "Specific part@fingers", "textinfo": "A focus is especially made on user's fingers. Nothing else is visible."}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 7. Adding a cape to a toy figure. a). User doodles to create a spline patch with the AR controller. b). Patch rendered in AR. c). User creates the cape in AR. d). Robot prints directly on the lion model. e). Printed result.", "name": "CHI18_paper579-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 6. The SpaceFace application and its outside view (a), inside view (b) interaction and visualization concept (c) and physical interaction scenario (d).", "name": "CHI18_paper54-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper198-Figure3-1.png", "doi": "https://doi.org/10.1145/3287076", "caption": "Fig. 3. The final design of the HeadGesture set for the nine commands. The movement of head is indicated by the arrows. \"2\u00d7\" represents the repeating of the action for twice. \"1s\" is an illustration for a dwell.", "name": "Ubicomp18_paper198-Figure3-1.png"}], "name": "head", "code": "Specific part@head", "textinfo": "A focus is especially made on the user's head. Nothing else is visible."}, {"children": [{"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper334-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173908", "caption": "Figure 5: Top-ten chosen gestures in each of the three conditions: standing, sitting and projection", "name": "CHI18_paper334-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper65-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242665", "caption": "Figure 2. MetaArms design approach: a closely situated anthropomorphic arms system driven by leg and feet motion, with haptic feedback loop.", "name": "UIST18_paper65-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 8. IB task procedure of Study 2 (*not done in baseline outcome blocks, ** not done in baseline action blocks).", "name": "CHI18_paper541-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper336-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173910 ", "caption": "Figure 5: (a) This shoe sole is flat by default. (b) The user transforms it into a treaded sole it by pulling a string, e.g., when it starts snowing. (c) Note that the sole is functional and robust enough to walk on.", "name": "CHI18_paper336-Figure5-1.png"}], "name": "foot", "code": "Specific part@foot", "textinfo": "A focus is especially made on the user's feet. Nothing else is visible."}], "name": "Specific part", "code": "Specific part", "textinfo": "A focus is made on a specific part of the body."}, {"children": [{"is_image": true, "path": "UIST18_paper499-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 10. Safe zone is defined as a personal region located at the center of the maximum inscribed circle found in the available free space", "name": "UIST18_paper499-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 5. (a) Connecting two edge shields with a proximity gesture forms (b)(c) a wall shield in the VR zone", "name": "UIST18_paper499-Figure5-1.png"}], "name": "lower body", "code": "body part@lower body", "textinfo": "Most of the lower body (legs and feet) is visible."}, {"children": [{"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper170-Figure5-1.png", "doi": "https://doi.org/10.1145/3287048", "caption": "Fig. 5. FarmChat speech-based interaction", "name": "Ubicomp18_paper170-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}], "name": "upper body", "code": "body part@upper body", "textinfo": "Most of the upper body (torso and arms) is visible"}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 14: Making the pendulum reach its two degrees of freedom requires two user interactions: (a) The first hit makes the prop swing back and forth and (b) the second hit allows it to orbit.", "name": "CHI18_paper89-Figure14-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}], "name": "full body", "code": "body part@full body", "textinfo": "Most of the body is visible."}], "name": "body part", "code": "body part", "textinfo": "Which parts of the Human body are represented?"}], "name": "Users", "code": "Users", "textinfo": "How are users represented in the figure?"}, {"children": [{"children": [{"children": [{"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}], "name": "Pointing", "code": "discrete@Pointing", "textinfo": "User is pointing at a target."}, {"children": [{"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper151-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173725", "caption": "Figure 4. The Light Writer", "name": "CHI18_paper151-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 3. Example lateral UI elements for a passively mobile shape display: (a) slider, (b) joystick, and (c) knob", "name": "CHI18_paper291-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}], "name": "Key press", "code": "discrete@Key press", "textinfo": "User is pressing a button."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}], "name": "Symbolic gesture", "code": "discrete@Symbolic gesture", "textinfo": "User is performing a symbolic gesture."}], "name": "discrete", "code": "discrete", "textinfo": "User's actions are really quick and brief over the time."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper737-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242615", "caption": "Figure 1. Example of Magneto-Haptics. (A) Three cylindrical magnets provide a magnetic force to the cube magnet. (B) This feedback creates a sensation as if one is moving an object on a gradient during active touch.", "name": "UIST18_paper737-Figure1-1.png"}], "name": "translating", "code": "continuous@translating", "textinfo": "User is translating an object."}, {"children": [{"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper21-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173595", "caption": "Figure 2. The six rotation directions.", "name": "CHI18_paper21-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper321-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242662", "caption": "Figure 12. Study apparatus for hinge (left) and slide (right).", "name": "UIST18_paper321-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper321-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242662", "caption": "Figure 1. Indutivo recognizes the tap of a conductive objects on a smartwatch, such as (a) a dime, or (b) finger. It can sense (c) the rotation of a bottle cap instrumented using copper tape, (d) hinge of a metal credit card, and (e) slide of the handle of a table knife.", "name": "UIST18_paper321-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 4. Abstracting attributes by demonstration. An arrow\u2019s \u201cangle\u201d attribute is abstracted as a variable using two constraint markers.", "name": "CHI18_paper199-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure13-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 13. Reactile lets a user to abstract attributes as variables through demonstration with blue constraint markers. When the system detects the demonstration, it updates the left panel to show a list of variables and current states.", "name": "CHI18_paper199-Figure13-1.png"}], "name": "rotating", "code": "continuous@rotating", "textinfo": "User is rotating an object."}, {"children": [{"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper76-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173650", "caption": "Figure 3. While telling researchers a story about her photos, P12 zoomed into the right photo in this collage to emphasize the butterfly on her head.", "name": "CHI18_paper76-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 3. Example lateral UI elements for a passively mobile shape display: (a) slider, (b) joystick, and (c) knob", "name": "CHI18_paper291-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper245-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173819", "caption": "Figure 12. Sample interactions demonstrating applications of multirays", "name": "CHI18_paper245-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 5. Modifying the generated mechanism by (a) moving the point and (b) changing the scale", "name": "CHI18_paper411-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 12. Workspace scaling tool for drawing a car. The user uses a large workspace to draw the shape of the car in a small, comfortable, scale (left). She then defines a small workspace centered on a headlight (right). This \u201czoom in\u201d effect allows her to capture the details of the headlight\u2019s shape, define a new canvas on it, and draw highlights.", "name": "CHI18_paper185-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper142-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173716", "caption": "Figure 1. Our framework enables users to both pan & zoom the context view and to create independent focus views, either DragMags or lenses.", "name": "CHI18_paper142-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper159-Figure6-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 6. The at-home setup with a TV set facilitates the edutainment value of the task.", "name": "CSCW18_paper159-Figure6-1.png"}], "name": "scaling", "code": "continuous@scaling", "textinfo": "User is scaling an object."}, {"children": [{"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper339-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173913", "caption": "Figure 5. Seven out of our 10 participants interacting with the probe. P2, PII and PIX use the slider. P3 and P5 are about the change the shape by clicking on the central button. P6 and P8 use the rotary knob.", "name": "CHI18_paper339-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper185-Figure4-1.png", "doi": "https://doi.org/10.1145/3274454", "caption": "Fig. 4. Emma scrolling to the next clue while Olivia moves the big picture to the new clue location. I-Spy images courtesy of Maria Neradova.", "name": "CSCW18_paper185-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper245-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173819", "caption": "Figure 12. Sample interactions demonstrating applications of multirays", "name": "CHI18_paper245-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper123-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173697", "caption": "Figure 8: Collections with layouts: (a) Repeat Grid. (b) Partition Stack. (c) Partition Stacks Nested in a Repeat Grid", "name": "CHI18_paper123-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper867-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242636", "caption": "Figure 5. Shows the representation of the hands in virtual reality. Also the described pinch gesture can be seen which enables the user to perform indirect windowing and scrolling.", "name": "UIST18_paper867-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper249-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173823", "caption": "Figure 4: Menu techniques: (a) M3 Gesture Menu, (b) Multi-Stroke Marking Menu, and (c) Linear Menu. Screenshots were taken when selecting terminal targets on the second-level menus with visual feedback.", "name": "CHI18_paper249-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper73-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173647", "caption": "Figure 4. (top) In TILT, SS points at a location where he needs LS to move his ROI, (bottom) In TOUCH, SS uses \u201cflexible ownership\u201d feature to drive LS\u2019s ROI to desired location.", "name": "CHI18_paper73-Figure4-1.png"}], "name": "scrolling", "code": "continuous@scrolling", "textinfo": "User is scrolling a view."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper569-Figure27-1.png", "doi": "https://doi.org/10.1145/3173574.3174143", "caption": "Figure 27. Lampshade", "name": "CHI18_paper569-Figure27-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure8-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 8. Prop extension. Parallel extension: a) Extension flattened state. b) Extension pops up. c) Holding in precision grasp. Tilt extension: d) Extension flattened state. e) Extension pops up. f) Holding in pen-like tripod grasp.", "name": "UIST18_paper5-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 2: The cross-section of two resistive-coated yarns, which form a resistor (a) that can measure applied force (b).", "name": "UIST18_paper745-Figure2-1.png"}], "name": "deforming", "code": "continuous@deforming", "textinfo": "User is deforming [a part of] an object/the system."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 4. The Canvas sketching interface: The final composition (left) and the interface (right) show the \u201cuser overlay\u201d. Both sides have a list of sketches and animation controls at the bottom. The in/out buttons make the selected sketches appear/disappear. The sliders control the stroke-start, now at 0%, and the stroke-end, now at 100%.", "name": "UIST18_paper675-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}], "name": "writing/drawing", "code": "continuous@writing/drawing", "textinfo": "User is writing or drawing."}], "name": "continuous", "code": "continuous", "textinfo": "The user is continuously interacting with the system."}], "name": "action", "code": "action", "textinfo": "How is the user action?"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}], "name": "touch interaction", "code": "Interaction type@touch interaction", "textinfo": "Interaction relying on a touch sensitive surface."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}], "name": "mid-air interaction", "code": "Interaction type@mid-air interaction", "textinfo": "Interaction is performed in mid-air. The system may involve controllers, tracking."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}], "name": "distal interaction", "code": "Interaction type@distal interaction", "textinfo": "Directly interacting with objects from a distance (through intermediate devices or not)."}, {"children": [{"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper192-Figure4-1.png", "doi": "https://doi.org/10.1145/3274461", "caption": "Figure 4: Interrupting. The child is about to tap the screen; dad reaches out to stop the action and pulls his hand away.", "name": "CSCW18_paper192-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper765-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242647", "caption": "Figure 1. We explore skin irritations, such as itching, by means of electrical stimulation for use as a feedback modality. The sensations generated with this method range from gentle and soothing ones all the way to stinging and irritating ones. Compared to vibrotactile feedback, itching grabs attention more and increases the urge to react.", "name": "UIST18_paper765-Figure1-1.png"}], "name": "on body interaction", "code": "Interaction type@on body interaction", "textinfo": "The interactive system is on the Human body."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper622-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174196", "caption": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "name": "CHI18_paper622-Figure4-1.png"}], "name": "computer interaction ", "code": "Interaction type@computer interaction ", "textinfo": "Classic computer interaction (using keyboard/touchpad/mouse)."}, {"children": [{"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper450-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174024", "caption": "Figure 10. Proposed applications.", "name": "CHI18_paper450-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}], "name": "tangible interaction", "code": "Interaction type@tangible interaction", "textinfo": "Interaction with the system is made though tangible objects."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper521-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242601", "caption": "Figure 1. Left: Users select a destination to teleport to using their controller with a raycast. Right; a portal (blue circle) that shows a preview of the location to be teleported to appears either to the left, right or center depending on the position of the user in the tracking space. Users must step into the portal to activate teleportation, which unobtrusively reorients and repositions them away from the tracking space boundary in order to increase available walking space.", "name": "UIST18_paper521-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 4: The focus maneuver temporarily provides the badminton app with control over most of the physical space. The other apps go into a defocus state.", "name": "CHI18_paper241-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 9. (a) Once the HMD user approaches the aggressive shield installed at the door, (b) the system pauses the game with a grayed out viewport, and (c) lead the user to the safe zone indicated by a green halo. The game resumes with a button on the controller.", "name": "UIST18_paper499-Figure9-1.png"}], "name": "controllers interaction", "code": "Interaction type@controllers interaction", "textinfo": "Interaction with the system is made with controllers (joysticks, tv remotes)."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "name": "CHI18_paper185-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 4. The Canvas sketching interface: The final composition (left) and the interface (right) show the \u201cuser overlay\u201d. Both sides have a list of sketches and animation controls at the bottom. The in/out buttons make the selected sketches appear/disappear. The sliders control the stroke-start, now at 0%, and the stroke-end, now at 100%.", "name": "UIST18_paper675-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 9. Basic workflow: 1. Sketch a design on fabric, 2. adhere Circuitry Stickers, 3. draw the circuit and electrical connections, 4. capture a picture of the sketch, 5. insert the fabric into the embroidery machine for stitching, and 6. replace the stickers with hardware components.", "name": "CHI18_paper82-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 9: Application highlights: (a) switching from pencil to highlighter with Side and Heel postures (document annotation); (b) choosing pen colour from radial menu with Side-RingOut-PinkyOut (document annotation); (c) object creation menu with Side-PinkyOut (vector drawing); (d) using SidePinkyIn to use handwriting recognition for creating a text object (vector drawing); (e) gesture command mode using Float-Index (vector drawing).", "name": "UIST18_paper825-Figure9-1.png"}], "name": "pen interaction", "code": "Interaction type@pen interaction", "textinfo": "Interaction with the system is made with a pen."}, {"children": [{"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper288-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173862", "caption": "Figure 1. The experimental setup (a) and a close-up view of the grid interface (b) in the study.", "name": "CHI18_paper288-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 10. Compact fractal radial menus with deep structure in GazeBrowser. a) Example application controlling humidity and temperature sensors. b) HoloLens screenshot of menu used with device refinement. c) A close-up showing selection of very small radial menu items using Pinpointing. The selected item is the small yellow dot marked by the crosshair.", "name": "CHI18_paper81-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 4. Shared attention on the overview device (SSV; left) often led to active discussion (AD; right) as the device gave the group a common focus and starting point for a diccussion.", "name": "CHI18_paper300-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper349-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173923", "caption": "Figure 3. Experiment setup.", "name": "CHI18_paper349-Figure3-1.png"}], "name": "gaze interaction", "code": "Interaction type@gaze interaction", "textinfo": "Interaction with the system is made with the eyes."}], "name": "Interaction type", "code": "Interaction type", "textinfo": "What is the type of interaction involved?"}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}], "name": "outside in", "code": "tracking based@outside in", "textinfo": "User is tracked by the system with external sensors."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper288-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173862", "caption": "Figure 1. The experimental setup (a) and a close-up view of the grid interface (b) in the study.", "name": "CHI18_paper288-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper340-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173914", "caption": "Figure 3: Breathing actions and corresponding effects in the FPS game.", "name": "CHI18_paper340-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper610-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174184", "caption": "Figure 1. Example use case of the proposed model: Different levels of detail are shown depending on perceivable screen resolution based on the device\u2019s position and orientation in the field of view.", "name": "CHI18_paper610-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper362-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 3. Users were nine times more likely to check their tracker during vigorous physical activity than when sedentary. Such engagements increased in frequency near goal completion and fueled users\u2019 motivation to meet their goals", "name": "CHI18_paper362-Figure3-1.png"}], "name": "inside out", "code": "tracking based@inside out", "textinfo": "User is tracked by the system with sensors on/inside him."}], "name": "tracking based", "code": "tracking based", "textinfo": "User is tracked by the system."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}], "name": "mobile devices", "code": "device based@mobile devices", "textinfo": "The user is interacting with the system by using a mobile device: smartphones, tablets, smartwatches, etc."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 1. A visually impaired student exploring the multisensory map. The system augments a tactile map with projection and audio output.", "name": "CHI18_paper629-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 9. On rare locations we observed helping and blocking behavior to gain benefits for the team or over the competitor.", "name": "CHI18_paper539-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}], "name": "large surfaces", "code": "device based@large surfaces", "textinfo": "The user is interacting with the system though large screens: screen on a wall, touch table, tv, etc."}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper347-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242589", "caption": "Figure 3: FingerArc and FingerChord: (a) holding an action key with a special hand posture for a predesignated delay time reveals the shortcut interface; (b) selecting the primary command using the index finger with others tucked in (FingerArc) or the middle finger (FingerChord); (c-e) selecting other commands using the angle of the thumb (FingerArc) or pressing different key areas (FingerChord); (f) releasing the key maintaining a hand posture triggers the command (e.g. primary command); (g) revealing all the fingers while holding the key cancels the operation.", "name": "UIST18_paper347-Figure3-1.png"}], "name": "desktop devices", "code": "device based@desktop devices", "textinfo": "The user is interacting with the system through desktop input devices: mouse, keyboad, touchpad on a desk, etc. "}, {"children": [{"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper622-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174196", "caption": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "name": "CHI18_paper622-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper20-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173594", "caption": "Figure 2. (A) A low vision user using SteeringWheel with Surface Dial. (B) The Surface Dial and its gestures.", "name": "CHI18_paper20-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper353-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173927", "caption": "Figure 4. Example of a designer using ProtoAR\u2019s interactive capture tools to for physical-digital prototyping of a mobile AR furniture placement app.", "name": "CHI18_paper353-Figure4-1.png"}, {"is_image": true, "path": "Ubicomp18_paper176-Figure3-1.png", "doi": "https://doi.org/10.1145/3287054", "caption": "Fig. 3. (a) H4.F pointing at H4.M\u2019s shower data, (b) vice versa.", "name": "Ubicomp18_paper176-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 6: A table (shown on screen). Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys, and (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the position of the cell and its content are read out aloud.", "name": "CHI18_paper11-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 7: A user is searching a table (shown on screen) for the word \u2018Jill\u2019. Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys. (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the number of occurrences of the search query in the respective column or row are read aloud. When the query is found, the position and content of the cell are read out aloud.", "name": "CHI18_paper11-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper21-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173595", "caption": "Figure 1. We propose to extend keyboard shortcuts with arm and wrist rotations gestures, performed while pressing down a key. The figure shows left/right wrist rolls. The user\u2019s rotation angles can be used, for example, for continuous control, such as changing the volume.", "name": "CHI18_paper21-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper281-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173855", "caption": "Figure 1. Remote Manipulator (ReMa) has two parts: it detects manipulations on an object (Left-yellow) using a set of sensors (Left-red), and then reproduces these with a proxy object (Right-yellow) using a Baxter robot arm (Right-red). ReMa allows shows the Manipulator Site collaborator (Right) the object with the same orientation as at the Tracking Site (Left). Collaborators can also use video chat (blue).", "name": "CHI18_paper281-Figure1-1.png"}], "name": "laptop devices", "code": "device based@laptop devices", "textinfo": "The user is interacting with the system through laptop input devices: integrated keyboad, integrated touchpad, etc."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper340-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173914", "caption": "Figure 3: Breathing actions and corresponding effects in the FPS game.", "name": "CHI18_paper340-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper521-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242601", "caption": "Figure 1. Left: Users select a destination to teleport to using their controller with a raycast. Right; a portal (blue circle) that shows a preview of the location to be teleported to appears either to the left, right or center depending on the position of the user in the tracking space. Users must step into the portal to activate teleportation, which unobtrusively reorients and repositions them away from the tracking space boundary in order to increase available walking space.", "name": "UIST18_paper521-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}], "name": "controllers", "code": "device based@controllers", "textinfo": "The user is interacting with the system using a controller (tv remote, game controller, joystick...)"}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper450-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174024", "caption": "Figure 10. Proposed applications.", "name": "CHI18_paper450-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}], "name": "tangible objects", "code": "device based@tangible objects", "textinfo": "The user is interacting with tangible objects"}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 2. Scenariot localization principle.", "name": "CHI18_paper219-Figure2-1.png"}], "name": "AR", "code": "device based@AR", "textinfo": "The user is interacting in an AR context"}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper340-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173914", "caption": "Figure 3: Breathing actions and corresponding effects in the FPS game.", "name": "CHI18_paper340-Figure3-1.png"}], "name": "VR", "code": "device based@VR", "textinfo": "The user is interacting in a VR context"}], "name": "device based", "code": "device based", "textinfo": "The user is interacting with the system through a manipulable device."}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper170-Figure5-1.png", "doi": "https://doi.org/10.1145/3287048", "caption": "Fig. 5. FarmChat speech-based interaction", "name": "Ubicomp18_paper170-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 3. Intermodulation II, 11/10/2016", "name": "CHI18_paper160-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 3. (a) The first version of the probe is non interactive. Children used it to built tactile bracelets and generate novel scenarios for documenting and reflecting on field-trips (here, in a museum). (b) The second version is a functional bracelet recording and playing sound.", "name": "CHI18_paper236-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 2. The second version of the probe is worn on the wrist. It consists of a strap covered with velcro on which an audio recorder/player.", "name": "CHI18_paper236-Figure2-1.png"}], "name": "speech input", "code": "sound based@speech input", "textinfo": "The user is interacting with the system by speaking through a microphone."}], "name": "sound based", "code": "sound based", "textinfo": "The system is listening to the sounds emitted by the user."}], "name": "input channel", "code": "input channel", "textinfo": "How is the user actually interacting with the system?"}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper450-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174024", "caption": "Figure 10. Proposed applications.", "name": "CHI18_paper450-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}], "name": "haptic", "code": "output modality@haptic", "textinfo": "There is an haptic response from the system."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}], "name": "large displays", "code": "visual@large displays", "textinfo": "Visual feedback through large displays (tv, wall screen, touch table..)"}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "name": "UIST18_paper335-Figure3-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper127-Figure16-1.png", "doi": "https://doi.org/10.1145/3242587.3242619", "caption": "Figure 16. Working with a physical prototype that consists of four foamcore layers (a). The geometry of the model is continuously captured and can be rendered in Blender with the house model of part 1 (b).", "name": "UIST18_paper127-Figure16-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}], "name": "computer displays", "code": "visual@computer displays", "textinfo": "Visual feedback through computer screens (laptop, desktop, car embedded display...)"}, {"children": [{"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "mobile displays", "code": "visual@mobile displays", "textinfo": "Visual feedback through mobile displays (smartphones, tablets, smartwatches...)"}, {"children": [{"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 1. A visually impaired student exploring the multisensory map. The system augments a tactile map with projection and audio output.", "name": "CHI18_paper629-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 3. A snapshot of the virtual Tai Chi training studio", "name": "CHI18_paper401-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper173-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173747", "caption": "Figure 3. Illustration of the experimental setup: participants stood 2 meters away from a rear-projector screen. The input device was connected via USB to the computer in order to guarantee stable data transfer for smooth control.", "name": "CHI18_paper173-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure13-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 13. Reactile lets a user to abstract attributes as variables through demonstration with blue constraint markers. When the system detects the demonstration, it updates the left panel to show a list of variables and current states.", "name": "CHI18_paper199-Figure13-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 4. Generating a linkage mechanism through (a) placing the AR marker board on the test board and pressing the record button, (b) moving the AR marker board, and (c) pressing the record button again. Then, the system generates the linkage mechanism.", "name": "CHI18_paper411-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper401-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173975", "caption": "Figure 4. A snapshot of the VR shooting game", "name": "CHI18_paper401-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 7. Construction Mode: building a map using Wikki Stix and magnets combined with projection and audio feedback.", "name": "CHI18_paper629-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 5. Modifying the generated mechanism by (a) moving the point and (b) changing the scale", "name": "CHI18_paper411-Figure5-1.png"}], "name": "projected displays", "code": "visual@projected displays", "textinfo": "Visual feedback through a projected image on a surface."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}], "name": "head mounted displays", "code": "visual@head mounted displays", "textinfo": "AR, VR displays."}, {"children": [{"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper160-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173734", "caption": "Figure 2. Intermodulation I, 12/18/2015", "name": "CHI18_paper160-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 3. Alternative BioFidget designs. (a) Basic design. (b) BioFidget with an additional clip for PPG sensing stabilization. (c) Fan-shaped wing to react to respiration. (d) BioFidget with a handheld display for rich visual biofeedback.", "name": "CHI18_paper613-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper569-Figure27-1.png", "doi": "https://doi.org/10.1145/3173574.3174143", "caption": "Figure 27. Lampshade", "name": "CHI18_paper569-Figure27-1.png"}, {"is_image": true, "path": "CHI18_paper98-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173672", "caption": "Figure 4: 3 children using the interactive blood vessel.", "name": "CHI18_paper98-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper151-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173725", "caption": "Figure 4. The Light Writer", "name": "CHI18_paper151-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper5-Figure7-1.png", "doi": "https://doi.org/10.1145/10.1145/3173574.3173579", "caption": "Figure 7. Computationally corresponding action with electroluminescent light intensity.", "name": "CHI18_paper5-Figure7-1.png"}], "name": "lights", "code": "visual@lights", "textinfo": "Lamps, leds, any bulbs."}], "name": "visual", "code": "visual", "textinfo": "Response from the system involves a screen, a display."}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CSCW18_paper192-Figure8-1.png", "doi": "https://doi.org/10.1145/3274461", "caption": "Figure 8: American parents sat back, away from the child, when playing the instructional game.", "name": "CSCW18_paper192-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 1. A visually impaired student exploring the multisensory map. The system augments a tactile map with projection and audio output.", "name": "CHI18_paper629-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 5. Experimental tasks for the three timing methods: Libet clock (left), Visual Alphabet (middle) and Audio Alphabet (right).", "name": "CHI18_paper541-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 5. The second method to access different block types: audio-cue typed blocks, when a typed block in the toolbox and the blocks in the workspace that accept it play the same distinct audio cues.", "name": "CHI18_paper69-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 7. I/O Braid USB-C headphones with embedded gesture recognition and audio feedback for music control, enabled by Nanoboard. The user starts playback by pinching, then rolls the I/O Braid to increase the volume.", "name": "UIST18_paper485-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 6. Student using our prototype during exploration mode: tactile map combined with projection and audio output.", "name": "CHI18_paper629-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 8. IB task procedure of Study 2 (*not done in baseline outcome blocks, ** not done in baseline action blocks).", "name": "CHI18_paper541-Figure8-1.png"}], "name": "sound based", "code": "audible@sound based", "textinfo": "The system emits specific sounds."}, {"children": [{"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 3. (a) The first version of the probe is non interactive. Children used it to built tactile bracelets and generate novel scenarios for documenting and reflecting on field-trips (here, in a museum). (b) The second version is a functional bracelet recording and playing sound.", "name": "CHI18_paper236-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 2. Two methods to move blocks: (a) audio-guided drag and drop, which speaks aloud the location of the block as it is dragged across the screen (gray box indicates audio output of program) and (b) location-first select, select, drop, where a location is selected via gray \u201cconnection blocks\u201d, then the toolbox of blocks that can be placed there appears.", "name": "CHI18_paper69-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 4. The first method to access different block types: embedded typed blocks, accessed from a menu embedded within each block (e.g. \"Repeat 2/3 times\")", "name": "CHI18_paper69-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper69-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173643", "caption": "Figure 3. Two methods to indicate the spatial structure of the code: (a) a spatial representation with nested statements placed vertically above inner blocks of enclosing statements, and (b) an audio representation with nesting communicated aurally with spearcons (shortened audio representations of words).", "name": "CHI18_paper69-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 7: A user is searching a table (shown on screen) for the word \u2018Jill\u2019. Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys. (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the number of occurrences of the search query in the respective column or row are read aloud. When the query is found, the position and content of the cell are read out aloud.", "name": "CHI18_paper11-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 2. The second version of the probe is worn on the wrist. It consists of a strap covered with velcro on which an audio recorder/player.", "name": "CHI18_paper236-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper473-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242620", "caption": "Figure 3. Display rack in a smart showroom. (a) An RFIMatch module is mounted behind each item. Several RFIMatch fingerstalls are provided with voice guidance in different languages. (b) A visitor wearing the German fingerstall hears the embedded audio message in German by touching the icon. (c) Two users wearing different fingerstalls greet each other in their own languages.", "name": "UIST18_paper473-Figure3-1.png"}], "name": "speech output", "code": "audible@speech output", "textinfo": "The system speaks to the user in his language."}], "name": "audible", "code": "audible", "textinfo": "The response from the system is audible."}], "name": "output modality", "code": "output modality", "textinfo": "System response"}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper629-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174203", "caption": "Figure 6. Student using our prototype during exploration mode: tactile map combined with projection and audio output.", "name": "CHI18_paper629-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper176-Figure3-1.png", "doi": "https://doi.org/10.1145/3287054", "caption": "Fig. 3. (a) H4.F pointing at H4.M\u2019s shower data, (b) vice versa.", "name": "Ubicomp18_paper176-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 9. Virtual content such as a terrain map can be modified and physically explored real-time using physical proxies such as a wand (left). Two displays are used to physically render two different virtual houses (right).", "name": "CHI18_paper291-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 1: (left) AirBnb (https://www.airbnb.com/s/places), with the initial selection chosen by SPRITEs highlighted when the user presses the topmost key in the rightmost keyboard key. (middle) When the user presses \u2018\\\u2019 key, SPRITEs reads out \u201cmenu\u201d and double pressing \u2018\\\u2019 activates the menu on the numeric row. (right) Pressing the \u20181\u2019 key on the numeric row reads out the first element in the menu.", "name": "CHI18_paper11-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper95-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173669", "caption": "Figure 4. Top of screen: raw data from our ten time-offlight sensors (red dots), with estimated touch point shown in green. Bottom of screen: resulting touch paths. On arm: current path is projected for debugging.", "name": "CHI18_paper95-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper426-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174000", "caption": "Figure 4. JND study setup", "name": "CHI18_paper426-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 6. Setup: the user puts on the HoloLens and draws with a motiontracked stylus, on a tablet (left), or mid-air (right) using a mouse affixed to the back of the tablet.", "name": "CHI18_paper185-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 8. The Letter Plates Montessori exercise provides feedback as words are formed and tactile letter-shapes are traced. Paper overlays on the Mat can provide additional context to the exercise.", "name": "CHI18_paper515-Figure8-1.png"}], "name": "Desktop", "code": "Situation@Desktop", "textinfo": "On a classic desk/table (mostly involving desktop computers, keyboard, mouse...)"}, {"children": [{"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 5. Left: P8 (WO2) points toward overview device, other members shifted their attention to it. In NO groups pointing rarely led to shared attention (right): P29 (INT1) points toward her device; other members\u2019 focus stays on own devices.", "name": "CHI18_paper300-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 3. View from inside vehicle with participant on left, Driving Wizard on right, and Interaction Wizard in backseat, top. Participant\u2019s view with hand in foreground, left. Bird\u2019s eye view of virtual vehicle in virtual world, right.", "name": "CHI18_paper165-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper581-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242599", "caption": "Figure 10. Study 3 was conducted on a subway", "name": "UIST18_paper581-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper165-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173739", "caption": "Figure 1. VR-OOM allows participants to experience the physical sensations of the real-world with the controlled virtual environments and events. Photo by Arjan Reef.", "name": "CHI18_paper165-Figure1-1.png"}], "name": "Public/private transport", "code": "Situation@Public/private transport", "textinfo": "In a car, bus, plane, on a bike..."}, {"children": [{"is_image": true, "path": "CHI18_paper362-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173936", "caption": "Figure 1. Fitbit Flex (left), Fitbit Charge HR (right)", "name": "CHI18_paper362-Figure1-1.png"}], "name": "Outdoors", "code": "Situation@Outdoors", "textinfo": "Park, streets."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 6: The user configures the intensity of the desired light bulb using a cup as a stand-in for a dial (passive prop). Using EMS force feedback, our system augments the tangible with constraints and detents.", "name": "CHI18_paper446-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper511-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242648", "caption": "Figure 12: (a) Our first control condition implements a teleport functionality and displays chaperone bounds to keep users from leaving the tracking volume. (b) Our second control condition, scaled motion, changes the mapping of physical motion (solid line) to virtual motion (dashed line).", "name": "UIST18_paper511-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper213-Figure20-1.png", "doi": "https://doi.org/10.1145/3242587.3242609", "caption": "Figure 20. Smartwatches could track additional health information, such as coughing (A), and recommend actions (B).", "name": "UIST18_paper213-Figure20-1.png"}, {"is_image": true, "path": "CHI18_paper73-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173647", "caption": "Figure 4. (top) In TILT, SS points at a location where he needs LS to move his ROI, (bottom) In TOUCH, SS uses \u201cflexible ownership\u201d feature to drive LS\u2019s ROI to desired location.", "name": "CHI18_paper73-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper142-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173716", "caption": "Figure 1. Our framework enables users to both pan & zoom the context view and to create independent focus views, either DragMags or lenses.", "name": "CHI18_paper142-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 5. Example of video taken for all three body postures", "name": "CHI18_paper202-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper159-Figure6-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 6. The at-home setup with a TV set facilitates the edutainment value of the task.", "name": "CSCW18_paper159-Figure6-1.png"}], "name": "Private indoors", "code": "Situation@Private indoors", "textinfo": "At the office, at work, at home..."}, {"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper61-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173635", "caption": "Figure 1. Left: Original Wire Costume (Oskar Schlemmer, Draht-Figur 1922). Photo \u00a9 Staatsgalerie Stuttgart. Right: Our version of the costume.", "name": "CHI18_paper61-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure11-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 11. GridDrones interactive animation of the flight of a butterfly at the LEGO\u00ae World Expo 2018.", "name": "UIST18_paper87-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 18: Participant balancing the marble (image from the study, with consent of the participant).", "name": "CHI18_paper446-Figure18-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 12. SynchronizAR supports spontaneous collaboration, i.e., a new user (b) join an existing AR collaboration (a) instantly (c).", "name": "UIST18_paper19-Figure12-1.png"}], "name": "Public indoors", "code": "Situation@Public indoors", "textinfo": "Malls, stations, museums, at gym. Might involve a crowded place."}], "name": "Situation", "code": "Situation", "textinfo": "Where does the interaction take place?"}], "name": "Interactive system", "code": "Interactive system", "textinfo": "Key elements describing an interactive system."}], "how_codes_hierarchies": [{"children": [{"children": [{"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}], "name": "1st person", "code": "point of view@1st person", "textinfo": "The scene point of view is through the eyes of the user."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}], "name": "3rd person", "code": "point of view@3rd person", "textinfo": "The scene point of view is in front of or on the side side of the user."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper539-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174113", "caption": "Figure 1. One player while interacting with the Pac-Many game.", "name": "CHI18_paper539-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper513-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174087", "caption": "Figure 9: Two different ways for playing the Concentration Game", "name": "CHI18_paper513-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "Ubicomp18_paper184-Figure4-1.png", "doi": "https://doi.org/10.1145/3287062", "caption": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "name": "Ubicomp18_paper184-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper288-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173862", "caption": "Figure 1. The experimental setup (a) and a close-up view of the grid interface (b) in the study.", "name": "CHI18_paper288-Figure1-1.png"}], "name": "overshoulder 3/4", "code": "point of view@overshoulder 3/4", "textinfo": "The scene point of view is from the back of the user, typically over the user's shoulder."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}], "name": "top", "code": "point of view@top", "textinfo": "The point of view of the scene is from the top."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "name": "UIST18_paper5-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}], "name": "UI only", "code": "point of view@UI only", "textinfo": "The illustration is only composed of the UI, there is no 3D scene. The point of view is always TOP, rendering the UI in the camera plane. We can admit representations of fingers and hands."}], "name": "point of view", "code": "point of view", "textinfo": "What is the point of view of the figure? Where is the camera?"}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "name": "UIST18_paper745-Figure10-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}], "name": "one frame", "code": "number of frames@one frame", "textinfo": "The figure is structured with only one frame."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}], "name": "multi frames", "code": "number of frames@multi frames", "textinfo": "The figure is structured with multiple frames."}], "name": "number of frames", "code": "number of frames", "textinfo": "Number of frames composing the illustrations. From 1 to N."}, {"children": [{"children": [{"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper174-Figure1-1.png", "doi": "https://doi.org/10.1145/3287052", "caption": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "name": "Ubicomp18_paper174-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "name": "CHI18_paper86-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper347-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242589", "caption": "Figure 3: FingerArc and FingerChord: (a) holding an action key with a special hand posture for a predesignated delay time reveals the shortcut interface; (b) selecting the primary command using the index finger with others tucked in (FingerArc) or the middle finger (FingerChord); (c-e) selecting other commands using the angle of the thumb (FingerArc) or pressing different key areas (FingerChord); (f) releasing the key maintaining a hand posture triggers the command (e.g. primary command); (g) revealing all the fingers while holding the key cancels the operation.", "name": "UIST18_paper347-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper927-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242588", "caption": "Figure 7. Attention Guidance: left or right-sided normal force on face guides users to search toward left or right respectively.", "name": "UIST18_paper927-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper354-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3173928", "caption": "Figure 18. Setup for visual analytics application.", "name": "CHI18_paper354-Figure18-1.png"}, {"is_image": true, "path": "CHI18_paper5-Figure9-1.png", "doi": "https://doi.org/10.1145/10.1145/3173574.3173579", "caption": "Figure 9. Paper Torch by Nendo (http://www.nendo.jp/en/works/papertorch/).", "name": "CHI18_paper5-Figure9-1.png"}], "name": "inset (PiP)", "code": "sub-framing@inset (PiP)", "textinfo": "A frame is included in another frame's space with a different point of view."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper65-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242665", "caption": "Figure 2. MetaArms design approach: a closely situated anthropomorphic arms system driven by leg and feet motion, with haptic feedback loop.", "name": "UIST18_paper65-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 10. (a) Changing the axis dimensions, and (b) remote control from a distance to set the focus onto a specific visualization view.", "name": "CHI18_paper19-Figure10-1.png"}], "name": "magnification lens", "code": "sub-framing@magnification lens", "textinfo": "A frame (whatever its nature) is included in another frame's space. This inclusion does not belong to the scene, and does represent something in the scene. It has been made to help the reader understand the figure."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 12. Message editing with PageFlip.", "name": "CHI18_paper529-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}], "name": "UI embedded", "code": "sub-framing@UI embedded", "textinfo": "The UI is embedded in the 3D scene, possibly using perspective."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper515-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174089", "caption": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "name": "CHI18_paper515-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper46-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173620", "caption": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "name": "CHI18_paper46-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper90-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173664", "caption": "Figure 6. A phase of loosely-coupled collaboration during the groupbased expert walkthrough. Domain experts navigated egocentrically to select their individual points of view.", "name": "CHI18_paper90-Figure6-1.png"}], "name": "UI overlay", "code": "sub-framing@UI overlay", "textinfo": "The UI has been added on top of the scene (mostly used to illustrate the user view from a AR system)."}, {"children": [{"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper320-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173894", "caption": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "name": "CHI18_paper320-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper557-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242614", "caption": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "name": "UIST18_paper557-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}], "name": "juxtaposition", "code": "sub-framing@juxtaposition", "textinfo": "The frame is composed of multiple juxtaposed sub-frames. (vertically/horizontally)"}], "name": "sub-framing", "code": "sub-framing", "textinfo": "Frames that are included in the frame space and do not belong to the represented scene itself."}], "name": "layout", "code": "layout", "textinfo": "How is organized the figure?"}], "name": "composition", "code": "composition", "textinfo": "Key categories that describe the main structure of a figure."}, {"children": [{"children": [{"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}], "name": "simplistic", "code": "realism@simplistic", "textinfo": "Very simplistic drawings are used. "}, {"children": [{"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 11: (a) TapZoom. (b1) Initial screen. (b2) User doubletaps on the left side of the screen. (b3) User holds contact for continuously zooming out.", "name": "CHI18_paper634-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 10: (a) PanPress. (b1) Initial screen. User performs a first pan. (b2) User holds contact for continuously panning. (b3) User adjusts the panning direction with subtle finger motion.", "name": "CHI18_paper634-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 9. Basic workflow: 1. Sketch a design on fabric, 2. adhere Circuitry Stickers, 3. draw the circuit and electrical connections, 4. capture a picture of the sketch, 5. insert the fabric into the embroidery machine for stitching, and 6. replace the stickers with hardware components.", "name": "CHI18_paper82-Figure9-1.png"}, {"is_image": true, "path": "Ubicomp18_paper194-Figure1-1.png", "doi": "https://doi.org/10.1145/3287072", "caption": "Fig. 1. Obstacle detection using smartphone.", "name": "Ubicomp18_paper194-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 9. I/O Braid\u2019s capability to sense touch and rotation input along the length of a headphone cord allows less precise input when on the go, for example, when jogging. The integrated visual feedback can be used to communicate phone connection or music status. This functionality could help signal social cues such as interruptibility, to onlookers. The photos show how flowing light can be used for directional feedback.", "name": "UIST18_paper485-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper419-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173993", "caption": "Figure 1. Consistent, user-defined gesture sets for controlling reading flow via RSVP on three device types: phone, watch and glasses.", "name": "CHI18_paper419-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper647-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174221", "caption": "Figure 2. This figure illustrates the experimental setup. (1) HTC Vive optical tracker (at 2.5m) and tracking space with 4\u00d7 4m2. (2) Virtual keyboard, stimulus and text input field. (3) Participant wearing HTC Vive and tracked hand-held controllers. (4) PC for experiment control and filling out questionnaires.", "name": "CHI18_paper647-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 3. Rotating the I/O Braid: Relative capacitive signal strengths are shown in graph at bottom. As each pair of columns approaches a finger, its signal strength increases.", "name": "UIST18_paper485-Figure3-1.png"}], "name": "realistic", "code": "realism@realistic", "textinfo": "Drawings are close to the reality. "}], "name": "realism", "code": "realism", "textinfo": "How is the drawing realism?"}], "name": "drawing", "code": "drawing", "textinfo": "The element is a drawn object (representation of a person, a smartphone, a computer, a room, etc.)."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 2. Scenariot localization principle.", "name": "CHI18_paper219-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 4. Abstracting attributes by demonstration. An arrow\u2019s \u201cangle\u201d attribute is abstracted as a variable using two constraint markers.", "name": "CHI18_paper199-Figure4-1.png"}], "name": "drawing", "code": "UI@drawing", "textinfo": "The UI has been drawn (by hand or not)."}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}], "name": "rendering", "code": "UI@rendering", "textinfo": "The UI has been generated or captured."}], "name": "UI", "code": "UI", "textinfo": "The element is a UI"}, {"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper198-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173772", "caption": "Figure 9. Interactive Campus Map: Model III", "name": "CHI18_paper198-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper87-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242658", "caption": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "name": "UIST18_paper87-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}], "name": "photo", "code": "type@photo", "textinfo": "The element is a photography."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 2: The cross-section of two resistive-coated yarns, which form a resistor (a) that can measure applied force (b).", "name": "UIST18_paper745-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper628-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.317420", "caption": "Figure 6. Highlighting interactions: Type 1 (left) & Type 2 (right)", "name": "CHI18_paper628-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 1: In-house Macro Recorder.", "name": "CHI18_paper638-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper123-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173697", "caption": "Figure 8: Collections with layouts: (a) Repeat Grid. (b) Partition Stack. (c) Partition Stacks Nested in a Repeat Grid", "name": "CHI18_paper123-Figure8-1.png"}], "name": "data visualization", "code": "type@data visualization", "textinfo": "The element represents data through a diagram, a graph, a chart... "}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 2. The different manipulations that can be applied to a search result", "name": "CHI18_paper251-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper610-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174184", "caption": "Figure 1. Example use case of the proposed model: Different levels of detail are shown depending on perceivable screen resolution based on the device\u2019s position and orientation in the field of view.", "name": "CHI18_paper610-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}], "name": "clipart/icon", "code": "type@clipart/icon", "textinfo": "The figure contains some cliparts or icons."}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper360-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173934", "caption": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "name": "CHI18_paper360-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper251-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173825", "caption": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "name": "CHI18_paper251-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}], "name": "text", "code": "type@text", "textinfo": "The figure contains some text to describe some elements."}], "name": "type", "code": "type", "textinfo": "The different types of elements that compose a figure."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}], "name": "transparency", "code": "color@transparency", "textinfo": "Is transparency used?"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper184-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173758", "caption": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "name": "CHI18_paper184-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "name": "CHI18_paper558-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}], "name": "colors", "code": "hue@colors", "textinfo": "The figure uses multiple colors."}, {"children": [{"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 2: Users are confined into tiles. The rotation maneuver allows apps to move their user to the adjacent tile. This way users do not feel confined.", "name": "CHI18_paper241-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper11-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173585", "caption": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "name": "CHI18_paper11-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 4: The focus maneuver temporarily provides the badminton app with control over most of the physical space. The other apps go into a defocus state.", "name": "CHI18_paper241-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper374-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173948", "caption": "Figure 2: (a) Anticlastic (saddle-shape) curvature (v > 0), (b) synclastic (dome-shape) curvature (v < 0) [201] (\u00a9 2014 image reproduced with permission from Elsevier).", "name": "CHI18_paper374-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 1. Various representations of the \u2018single finger swipe\u2019 gesture in different academic papers [27,36,10,14,35,5,13,2].", "name": "CHI18_paper547-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper161-Figure9-1.png", "doi": "https://doi.org/10.1145/3287039", "caption": "Fig. 9. Examples of actual grips (a, c) and the corresponding simulated grips (b, d).", "name": "Ubicomp18_paper161-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper20-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173594", "caption": "Figure 2. (A) A low vision user using SteeringWheel with Surface Dial. (B) The Surface Dial and its gestures.", "name": "CHI18_paper20-Figure2-1.png"}], "name": "grayscale", "code": "hue@grayscale", "textinfo": "The figure uses grayscale"}, {"children": [{"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CSCW18_paper151-Figure10-1.png", "doi": "https://doi.org/10.1145/3274420", "caption": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "name": "CSCW18_paper151-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 14: Making the pendulum reach its two degrees of freedom requires two user interactions: (a) The first hit makes the prop swing back and forth and (b) the second hit allows it to orbit.", "name": "CHI18_paper89-Figure14-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 4. Three different body postures in which different factors affect interaction such as a) Sitting at a table resting arms; b) Standing; c) Lying down with the back to the floor. (Arrows represent gravity and the circles are restrictions)", "name": "CHI18_paper202-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper737-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242615", "caption": "Figure 1. Example of Magneto-Haptics. (A) Three cylindrical magnets provide a magnetic force to the cube magnet. (B) This feedback creates a sensation as if one is moving an object on a gradient during active touch.", "name": "UIST18_paper737-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}, {"is_image": true, "path": "UIST18_paper877-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242604", "caption": "Figure 9: (a) After experimenting with various 3D printed shapes and sizes, (b) the \u201cflattened teardrop\u201d design performed best, because its orientation can be felt any time.", "name": "UIST18_paper877-Figure9-1.png"}], "name": "monochrome", "code": "hue@monochrome", "textinfo": "The figure is only based on 2 colors : a light and dark color (mostly black and white)."}], "name": "hue", "code": "hue", "textinfo": "What is the hue of the figure?"}], "name": "color", "code": "color", "textinfo": "The color used in the figure."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper737-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242615", "caption": "Figure 1. Example of Magneto-Haptics. (A) Three cylindrical magnets provide a magnetic force to the cube magnet. (B) This feedback creates a sensation as if one is moving an object on a gradient during active touch.", "name": "UIST18_paper737-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}], "name": "dashed", "code": "line style@dashed", "textinfo": "Dashed lines are used."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 14: Making the pendulum reach its two degrees of freedom requires two user interactions: (a) The first hit makes the prop swing back and forth and (b) the second hit allows it to orbit.", "name": "CHI18_paper89-Figure14-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 2: Users are confined into tiles. The rotation maneuver allows apps to move their user to the adjacent tile. This way users do not feel confined.", "name": "CHI18_paper241-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}], "name": "width", "code": "line style@width", "textinfo": "Lines with different width are used."}], "name": "line style", "code": "line style", "textinfo": "What line styles are used for the different objects?"}], "name": "visual characteristics", "code": "visual characteristics", "textinfo": "Key visual characteristics that compose an illustration."}, {"children": [{"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 11: (a) TapZoom. (b1) Initial screen. (b2) User doubletaps on the left side of the screen. (b3) User holds contact for continuously zooming out.", "name": "CHI18_paper634-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "name": "CHI18_paper241-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper697-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242629", "caption": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "name": "UIST18_paper697-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}], "name": "color area", "code": "region@color area", "textinfo": "Emphasize a region of the figure by applying a specific colour in the pointed area."}], "name": "region", "code": "region", "textinfo": "Emphasize a region of the figure."}, {"children": [{"children": [{"is_image": true, "full_res_path": "fake_path", "doi": "fake_doi", "caption": "fake_caption", "name": "fake_name"}], "name": "arrows", "code": "element@arrows", "textinfo": "Arrows are used to focus on an element, mostly by pointing to an object."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}], "name": "color highlight", "code": "element@color highlight", "textinfo": "A specific colour is used to highlight a specific object."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper363-Figure2-1.png", "doi": "", "caption": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "name": "CHI18_paper363-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper241-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173815", "caption": "Figure 4: The focus maneuver temporarily provides the badminton app with control over most of the physical space. The other apps go into a defocus state.", "name": "CHI18_paper241-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 9. (a) Once the HMD user approaches the aggressive shield installed at the door, (b) the system pauses the game with a grayed out viewport, and (c) lead the user to the safe zone indicated by a green halo. The game resumes with a button on the controller.", "name": "UIST18_paper499-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 5: Interactions with data table (shown with cursor annotation).", "name": "CHI18_paper638-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper334-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173908", "caption": "Figure 5: Top-ten chosen gestures in each of the three conditions: standing, sitting and projection", "name": "CHI18_paper334-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 2: In-house Smart Assistant.", "name": "CHI18_paper638-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 9. Drawing interface to mark custom spatial entities. (a) Holding drawing button at bottom left corner of display, (b) activates drawing mode, dimming screen, allowing user to sketch line to specify path or (c\u2013d) sketch (nearly) closed outline and scribble inside it to specify area. (e) Tapping \u201c+\u201d button creates SpaceToken for the marked area. (f) An area SpaceToken is added above the \u201c+\u201d button.", "name": "CHI18_paper248-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper5-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242628", "caption": "Figure 9. Natural grasp & Magic grasp.", "name": "UIST18_paper5-Figure9-1.png"}], "name": "exact contour line", "code": "enclosing@exact contour line", "textinfo": "The exact contours of a specific object are highlighted using a specific line style."}, {"children": [{"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure1-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "name": "UIST18_paper825-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 4. Three different body postures in which different factors affect interaction such as a) Sitting at a table resting arms; b) Standing; c) Lying down with the back to the floor. (Arrows represent gravity and the circles are restrictions)", "name": "CHI18_paper202-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper613-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174187", "caption": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "name": "CHI18_paper613-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure14-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 14. Monitoring the IoT assets (a, b) and navigating the user towards the assets by visualizing the direction on the screen(c, d).", "name": "CHI18_paper219-Figure14-1.png"}, {"is_image": true, "path": "UIST18_paper499-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 10. Safe zone is defined as a personal region located at the center of the maximum inscribed circle found in the available free space", "name": "UIST18_paper499-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}], "name": "circle/rectangle", "code": "enclosing@circle/rectangle", "textinfo": "A circle/rectangle shape has been drawn around the focused object."}], "name": "enclosing", "code": "enclosing", "textinfo": "The focus element is enclosed."}], "name": "element", "code": "element", "textinfo": "Emphasize an element of the figure."}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper499-Figure9-1.png", "doi": "https://doi.org/10.1145/3242587.3242630", "caption": "Figure 9. (a) Once the HMD user approaches the aggressive shield installed at the door, (b) the system pauses the game with a grayed out viewport, and (c) lead the user to the safe zone indicated by a green halo. The game resumes with a button on the controller.", "name": "UIST18_paper499-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper581-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242599", "caption": "Figure 10. Study 3 was conducted on a subway", "name": "UIST18_paper581-Figure10-1.png"}], "name": "grayed out/blurred", "code": "background@grayed out/blurred", "textinfo": "A blur effect or a specific colour with transparency is applied on the background."}, {"children": [{"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "name": "CHI18_paper54-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper90-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173664", "caption": "Figure 6. A phase of loosely-coupled collaboration during the groupbased expert walkthrough. Domain experts navigated egocentrically to select their individual points of view.", "name": "CHI18_paper90-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 3. (a) User\u2019s hand moving from left to right on the shape display. (b) Virtual hand displacement scaled up on the shape display to improve resolution.", "name": "CHI18_paper150-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper354-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3173928", "caption": "Figure 18. Setup for visual analytics application.", "name": "CHI18_paper354-Figure18-1.png"}, {"is_image": true, "path": "UIST18_paper595-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242635", "caption": "Figure 5: Gesture set for the glasses (top) and watch (bottom).", "name": "UIST18_paper595-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper54-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173628", "caption": "Figure 6. The SpaceFace application and its outside view (a), inside view (b) interaction and visualization concept (c) and physical interaction scenario (d).", "name": "CHI18_paper54-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper90-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173664", "caption": "Figure 7. A phase of tightly-coupled collaboration during the groupbased expert walkthrough. The domain experts selected a similar point of view and discussed the next analysis steps. Deictic gestures were used frequently during discussions.", "name": "CHI18_paper90-Figure7-1.png"}, {"is_image": true, "path": "Ubicomp18_paper161-Figure2-1.png", "doi": "https://doi.org/10.1145/3287039", "caption": "Fig. 2. Objects used in Study 1. In the basic group, we selected six everyday objects in accordance with the Schlesinger taxonomy [37]. In the size group (control weight), there are four levels in size for each shape and all objects have the same weight of 100g. In the weight group (control size), objects sharing the same shape have the same size while there are four levels in weight. The objects in the latter two groups were 3D printed. Weights were controlled by adding iron sand inside.", "name": "Ubicomp18_paper161-Figure2-1.png"}], "name": "removed", "code": "background@removed", "textinfo": "The highlighted objects are detoured and the background is removed."}], "name": "background", "code": "background", "textinfo": "Strategy used to emphasize the element in the foreground from the background"}], "name": "emphasize", "code": "emphasize", "textinfo": "Visual techniques used to visually emphasize, highlight specific information in the illustration."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper76-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173650", "caption": "Figure 3. While telling researchers a story about her photos, P12 zoomed into the right photo in this collage to emphasize the butterfly on her head.", "name": "CHI18_paper76-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper289-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173863", "caption": "Figure 1. Dyads performed the first (A) and second (B) tasks in the faceto-face conditions. In virtual reality conditions, avatars appeared across the table from each other (C), but were actually positioned on opposite sides of the motion capture stage (D). In the embodVR condition, participants were able to see both avatars (E). In the no_embodVR condition, participants were unable to see their partner and could only see their hands in the second task, to assist with furniture manipulation (F).", "name": "CHI18_paper289-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper236-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173810", "caption": "Figure 3. (a) The first version of the probe is non interactive. Children used it to built tactile bracelets and generate novel scenarios for documenting and reflecting on field-trips (here, in a museum). (b) The second version is a functional bracelet recording and playing sound.", "name": "CHI18_paper236-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 5. Left: P8 (WO2) points toward overview device, other members shifted their attention to it. In NO groups pointing rarely led to shared attention (right): P29 (INT1) points toward her device; other members\u2019 focus stays on own devices.", "name": "CHI18_paper300-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper443-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174017 ", "caption": "Figure 2. Participant Playing the Game in Lab Setting with the Experimenter", "name": "CHI18_paper443-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper551-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174125", "caption": "Figure 3: Children interacting with the interactive surface.", "name": "CHI18_paper551-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper359-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173933 ", "caption": "Figure 2. The VR view of the 360\u00b0 live-video feed, as it was presented to the viewer through the smartphone-VR setup.", "name": "CHI18_paper359-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper300-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173874", "caption": "Figure 4. Shared attention on the overview device (SSV; left) often led to active discussion (AD; right) as the device gave the group a common focus and starting point for a diccussion.", "name": "CHI18_paper300-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper397-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173971", "caption": "Figure 5. Y. has drawn a new bird-card and is trying it on the Birdhouse. She is a little disappointed that nothing happens when she waives the card in front of the sensors as she uses to do with the other ones.", "name": "CHI18_paper397-Figure5-1.png"}], "name": "blur", "code": "anonymization@blur", "textinfo": "Parts of the figure are blurred to not be recognized."}, {"children": [{"is_image": true, "full_res_path": "fake_path", "doi": "fake_doi", "caption": "fake_caption", "name": "fake_name"}], "name": "filling", "code": "anonymization@filling", "textinfo": "Parts of the figure are filled with colour to not be recognized."}], "name": "anonymization", "code": "anonymization", "textinfo": "Visual techniques used to anonymize specific elements in the figure (e.g. users)."}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 4. Three different body postures in which different factors affect interaction such as a) Sitting at a table resting arms; b) Standing; c) Lying down with the back to the floor. (Arrows represent gravity and the circles are restrictions)", "name": "CHI18_paper202-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}], "name": "color grouping", "code": "grouping and linking@color grouping", "textinfo": "Linking/Grouping of elements is made using colours."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper219-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173793", "caption": "Figure 2. Scenariot localization principle.", "name": "CHI18_paper219-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "UIST18_paper745-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242664", "caption": "Figure 2: The cross-section of two resistive-coated yarns, which form a resistor (a) that can measure applied force (b).", "name": "UIST18_paper745-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper385-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173959", "caption": "Figure 1. The BIGFile interface as the user navigates to \u201cDog\u201d in a file retrieval task. (a) and (c) show the adaptive part with two shortcuts, (b) and (d) the static part. In Step 1, the shortcuts do not help and the user selects \u201cAnimals\u201d in the static part, leading to Step 2 where the user directly selects \u201cDog\u201d in the first shortcut, saving one step.", "name": "CHI18_paper385-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper477-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 3: Example of text selection using FORCESELECT HIGH-", "name": "CHI18_paper477-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper354-Figure18-1.png", "doi": "https://doi.org/10.1145/3173574.3173928", "caption": "Figure 18. Setup for visual analytics application.", "name": "CHI18_paper354-Figure18-1.png"}], "name": "identifiers grouping", "code": "grouping and linking@identifiers grouping", "textinfo": "Linking/Grouping of elements is made using identifiers (numbers or letters). Line can be used between the identifier and the element."}, {"children": [{"is_image": true, "full_res_path": "fake_path", "doi": "fake_doi", "caption": "fake_caption", "name": "fake_name"}], "name": "grouping lines", "code": "grouping and linking@grouping lines", "textinfo": "Lines or Arrows used to link/group multiple elements inside the figure or extra information outside the figure."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 4. The experimental setup.", "name": "CHI18_paper132-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper429-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174003", "caption": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "name": "CHI18_paper429-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "name": "CHI18_paper218-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "name": "CHI18_paper188-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper177-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173751", "caption": "Figure 2. Our implementation uses a 3D printed PLA dish with an acrylic base plate. 2.4 mm diameter steel electrodes were embedded at 11 mm intervals and driven by a switching circuit.", "name": "CHI18_paper177-Figure2-1.png"}], "name": "text annotation", "code": "grouping and linking@text annotation", "textinfo": "Text attached to an element of a figure (e.g. label, definition) using lines or arrows."}], "name": "grouping and linking", "code": "grouping and linking", "textinfo": "Visual techniques used to link/group elements between each others or with extra information outside the figure (e.g. definitions)."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper839-Figure3-1.png", "doi": "https://doi.org/10.1145/3242587.3242627", "caption": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "name": "UIST18_paper839-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}], "name": "stroboscopic", "code": "effects@stroboscopic", "textinfo": "A stroboscopic effect is used to show the different steps of the motion."}, {"children": [{"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CSCW18_paper128-Figure2-1.png", "doi": "https://doi.org/10.1145/3274397", "caption": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "name": "CSCW18_paper128-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper368-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173942", "caption": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "name": "CHI18_paper368-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper365-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242645", "caption": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "name": "UIST18_paper365-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}, {"is_image": true, "path": "CHI18_paper385-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173959", "caption": "Figure 1. The BIGFile interface as the user navigates to \u201cDog\u201d in a file retrieval task. (a) and (c) show the adaptive part with two shortcuts, (b) and (d) the static part. In Step 1, the shortcuts do not help and the user selects \u201cAnimals\u201d in the static part, leading to Step 2 where the user directly selects \u201cDog\u201d in the first shortcut, saving one step.", "name": "CHI18_paper385-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 1: In-house Macro Recorder.", "name": "CHI18_paper638-Figure1-1.png"}], "name": "waves", "code": "effects@waves", "textinfo": "Waves are used to show that an element is hitting another element."}, {"children": [{"is_image": true, "path": "CHI18_paper218-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 8. The experiment was conducted with PW using a controller (PWC) and ray-casting (RC). Using PWC, the participant (a) first searched for the target (white), (b) grabbed the window (blue), (c) moved the window while adjusting its apparent size, and (d) released it onto the target when the apparent sizes matched. Using RC, the participant (e) first searched for the target (white), (f) dragged the window (blue) to the target, (g) placed it in the target, and (h) scaled it so that the absolute sizes matched.", "name": "CHI18_paper218-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper530-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174104", "caption": "Figure 1. Inpher makes it possible to define physical properties like bounciness of virtual objects through mimicking their physical behavior. (a) The user is equipped with virtual reality goggles and controllers for 3D input. (b) Users can grasp virtual objects and describe a physical motion to define the object\u2019s physical behavior (top). The free-flight physical motion of the object resembles the user\u2019s input curve (bottom).", "name": "CHI18_paper530-Figure1-1.png"}], "name": "motion blur", "code": "effects@motion blur", "textinfo": "A directional blur effect is used to illustrate motion. The position of the blur can also indicate the path/direction of the movement."}], "name": "effects", "code": "effects", "textinfo": "Dynamic style effects"}, {"children": [{"children": [{"is_image": true, "path": "CHI18_paper107-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173681", "caption": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "name": "CHI18_paper107-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper234-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173808", "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "name": "CHI18_paper234-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper675-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242613", "caption": "Figure 2. OctoPocus with traditional video prototyping. The designers create a rough stop-motion movie with only four stages of the interface, resulting in a poor representation of the dynamic interaction.", "name": "UIST18_paper675-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper185-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3173759", "caption": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "name": "CHI18_paper185-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "name": "CHI18_paper89-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper89-Figure15-1.png", "doi": "https://doi.org/10.1145/3173574.3173663", "caption": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "name": "CHI18_paper89-Figure15-1.png"}], "name": "trajectories", "code": "lines and arrows@trajectories", "textinfo": "Shows the motion path of an element in the figure from the beginning to the end."}, {"children": [{"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper335-Figure7-1.png", "doi": "https://doi.org/10.1145/3242587.3242651", "caption": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "name": "UIST18_paper335-Figure7-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper150-Figure23-1.png", "doi": "https://doi.org/10.1145/3173574.3173724", "caption": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "name": "CHI18_paper150-Figure23-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper446-Figure10-1.png", "doi": "https://doi.org/10.1145/3173574.3174020", "caption": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "name": "CHI18_paper446-Figure10-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper81-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173655", "caption": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "name": "CHI18_paper81-Figure1-1.png"}], "name": "direction", "code": "lines and arrows@direction", "textinfo": "Shows the direction of the motion."}, {"children": [{"is_image": true, "path": "CHI18_paper477-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174051", "caption": "Figure 2: Example of text selection using FORCESELECT. The user per-", "name": "CHI18_paper477-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "name": "CHI18_paper634-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper564-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174138", "caption": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "name": "CHI18_paper564-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper628-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.317420", "caption": "Figure 6. Highlighting interactions: Type 1 (left) & Type 2 (right)", "name": "CHI18_paper628-Figure6-1.png"}], "name": "transfer", "code": "lines and arrows@transfer", "textinfo": "Shows transition between two states of a model/system."}, {"children": [{"is_image": true, "path": "CHI18_paper18-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173592", "caption": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "name": "CHI18_paper18-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "name": "CHI18_paper42-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper547-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174121", "caption": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "name": "CHI18_paper547-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "name": "CHI18_paper579-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "name": "CHI18_paper436-Figure7-1.png"}, {"is_image": true, "path": "UIST18_paper853-Figure5-1.png", "doi": "https://doi.org/10.1145/3242587.3242597", "caption": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "name": "UIST18_paper853-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "name": "CHI18_paper380-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper436-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174010", "caption": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "name": "CHI18_paper436-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper579-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174153", "caption": "Figure 8. Fighter jet stand.", "name": "CHI18_paper579-Figure8-1.png"}], "name": "projection", "code": "lines and arrows@projection", "textinfo": "Shows the corresponding projection of an object."}], "name": "lines and arrows", "code": "lines and arrows", "textinfo": "Geometric shapes are used to help understanding the dynamic illustrated."}, {"children": [{"is_image": true, "path": "Ubicomp18_paper164-Figure1-1.png", "doi": "https://doi.org/10.1145/", "caption": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "name": "Ubicomp18_paper164-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "name": "CHI18_paper529-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper86-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3173660", "caption": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "name": "CHI18_paper86-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper378-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173952", "caption": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "name": "CHI18_paper378-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper27-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173601", "caption": "Figure 2. Editing operations and matching gestures. (a) Caret movement using horizontal directional gestures. (b) Text selection using caret movement and touching the screen with a second thumb. (c) After selection, users cycle through the clipboard operations using vertical directional gestures and keeping the other thumb on the screen.", "name": "CHI18_paper27-Figure2-1.png"}], "name": "Contact shapes", "code": "dynamic@Contact shapes", "textinfo": "Geometric shapes, such as circles, used to represent the region of contact between two elements."}], "name": "dynamic", "code": "dynamic", "textinfo": "Visual techniques used to describe dynamic content on a static support."}, {"children": [{"children": [{"children": [{"is_image": true, "path": "CHI18_paper437-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3174011", "caption": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "name": "CHI18_paper437-Figure3-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure12-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "name": "CHI18_paper634-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "name": "CHI18_paper634-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper634-Figure11-1.png", "doi": "https://doi.org/10.1145/3173574.3174208", "caption": "Figure 11: (a) TapZoom. (b1) Initial screen. (b2) User doubletaps on the left side of the screen. (b3) User holds contact for continuously zooming out.", "name": "CHI18_paper634-Figure11-1.png"}, {"is_image": true, "path": "CHI18_paper380-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3173954", "caption": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "name": "CHI18_paper380-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 1: In-house Macro Recorder.", "name": "CHI18_paper638-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 9. Basic workflow: 1. Sketch a design on fabric, 2. adhere Circuitry Stickers, 3. draw the circuit and electrical connections, 4. capture a picture of the sketch, 5. insert the fabric into the embroidery machine for stitching, and 6. replace the stickers with hardware components.", "name": "CHI18_paper82-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper638-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3174212", "caption": "Figure 2: In-house Smart Assistant.", "name": "CHI18_paper638-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper441-Figure21-1.png", "doi": "https://doi.org/10.1145/3173574.3174015", "caption": "Figure 21. Designers must consider model orientation in the printer. Structure 1 is more fragile than 2: (1) horizontally", "name": "CHI18_paper441-Figure21-1.png"}, {"is_image": true, "path": "CHI18_paper82-Figure7-1.png", "doi": "https://doi.org/10.1145/3173574.3173656", "caption": "Figure 7. Sensor Sticker process: 1. The user cuts the sticker, 2. adheres it to fabric, and draws connecting traces with trace color pen, 3. captures the design and removes the sticker, 4. the system stitches the sensor.", "name": "CHI18_paper82-Figure7-1.png"}], "name": "numbers", "code": "identifiers@numbers", "textinfo": "Numbers are used (often in a corner) to order the sequence of frames or the different parts of a figure."}, {"children": [{"is_image": true, "path": "UIST18_paper45-Figure4-1.png", "doi": "https://doi.org/10.1145/3242587.3242631", "caption": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "name": "UIST18_paper45-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper218-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173792", "caption": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "name": "CHI18_paper218-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper411-Figure3-1.png", "doi": "https://doi.org/10.1145/3173574.3173985", "caption": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "name": "CHI18_paper411-Figure3-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure10-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "name": "UIST18_paper19-Figure10-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "UIST18_paper913-Figure12-1.png", "doi": "https://doi.org/10.1145/3242587.3242667", "caption": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "name": "UIST18_paper913-Figure12-1.png"}, {"is_image": true, "path": "CHI18_paper19-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173593", "caption": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "name": "CHI18_paper19-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper529-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174103", "caption": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "name": "CHI18_paper529-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper248-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173822", "caption": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "name": "CHI18_paper248-Figure4-1.png"}], "name": "letters", "code": "identifiers@letters", "textinfo": "Letters are used (often in a corner) to order the sequence of frames or the different parts of a figure."}, {"children": [{"is_image": true, "path": "CHI18_paper202-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173776", "caption": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "name": "CHI18_paper202-Figure1-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure9-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "name": "Ubicomp18_paper162-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper210-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173784", "caption": "Figure 1. A vision for bodily games and play.", "name": "CHI18_paper210-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper825-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242652", "caption": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "name": "UIST18_paper825-Figure2-1.png"}, {"is_image": true, "path": "UIST18_paper485-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242638", "caption": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "name": "UIST18_paper485-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper653-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174227", "caption": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "name": "CHI18_paper653-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper132-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173706", "caption": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "name": "CHI18_paper132-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper310-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173884", "caption": "Figure 1. Five input modalities used in the study.", "name": "CHI18_paper310-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper497-Figure6-1.png", "doi": "https://doi.org/10.1145/3173574.3174071", "caption": "Figure 6. PolarTrack\u2019s tracking quality was evaluated with 1-finger, 1- hand, and 2-hands occlusion.", "name": "CHI18_paper497-Figure6-1.png"}, {"is_image": true, "path": "Ubicomp18_paper162-Figure8-1.png", "doi": "https://doi.org/10.1145/3287040", "caption": "Fig. 8. Input actions with various objects.", "name": "Ubicomp18_paper162-Figure8-1.png"}], "name": "title", "code": "identifiers@title", "textinfo": "A different title is used to annotate the sub frames."}], "name": "identifiers", "code": "identifiers", "textinfo": "Strategies used to identify, order, different parts of the same figure."}], "name": "structure", "code": "structure", "textinfo": "Visual techniques used to structure an illustration"}, {"children": [{"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper558-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3174132", "caption": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "name": "CHI18_paper558-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper199-Figure4-1.png", "doi": "https://doi.org/10.1145/3173574.3173773", "caption": "Figure 4. Abstracting attributes by demonstration. An arrow\u2019s \u201cangle\u201d attribute is abstracted as a variable using two constraint markers.", "name": "CHI18_paper199-Figure4-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 5. Experimental tasks for the three timing methods: Libet clock (left), Visual Alphabet (middle) and Audio Alphabet (right).", "name": "CHI18_paper541-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 9. Experimental tasks for the two timing methods: Libet Clock (left) and Haptic Clock (right).", "name": "CHI18_paper541-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper603-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174177", "caption": "Figure 1. (a) Lassoing icons. Icons whose entire area is inside of the loop are selected (highlighted in blue). The start and end points are automatically closed. The work presented here focuses on the straight stroking segments. (b-f) Steering through obstacles with various conditions.", "name": "CHI18_paper603-Figure1-1.png"}], "name": "Arrows", "code": "measure@Arrows", "textinfo": "Arrows are used to indicate angles, physical dimensions."}, {"children": [{"is_image": true, "path": "UIST18_paper711-Figure2-1.png", "doi": "https://doi.org/10.1145/3242587.3242642", "caption": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "name": "UIST18_paper711-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper291-Figure2-1.png", "doi": "https://doi.org/10.1145/3173574.3173865", "caption": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "name": "CHI18_paper291-Figure2-1.png"}, {"is_image": true, "path": "CHI18_paper42-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3173616", "caption": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "name": "CHI18_paper42-Figure1-1.png"}, {"is_image": true, "path": "UIST18_paper19-Figure6-1.png", "doi": "https://doi.org/10.1145/3242587.3242595", "caption": "Figure 6. Technical evaluation setups.", "name": "UIST18_paper19-Figure6-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "name": "CHI18_paper541-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 5. Experimental tasks for the three timing methods: Libet clock (left), Visual Alphabet (middle) and Audio Alphabet (right).", "name": "CHI18_paper541-Figure5-1.png"}, {"is_image": true, "path": "CHI18_paper541-Figure9-1.png", "doi": "https://doi.org/10.1145/3173574.3174115", "caption": "Figure 9. Experimental tasks for the two timing methods: Libet Clock (left) and Haptic Clock (right).", "name": "CHI18_paper541-Figure9-1.png"}, {"is_image": true, "path": "CHI18_paper603-Figure1-1.png", "doi": "https://doi.org/10.1145/3173574.3174177", "caption": "Figure 1. (a) Lassoing icons. Icons whose entire area is inside of the loop are selected (highlighted in blue). The start and end points are automatically closed. The work presented here focuses on the straight stroking segments. (b-f) Steering through obstacles with various conditions.", "name": "CHI18_paper603-Figure1-1.png"}, {"is_image": true, "path": "CHI18_paper188-Figure8-1.png", "doi": "https://doi.org/10.1145/3173574.3173762", "caption": "Figure 8. Stretchable input sensors compatible with our workflow: (a) a touch sensor, (b) a proximity sensor, (c) a slider, (d) a strain sensor, (e) a capacitive pressure sensor", "name": "CHI18_paper188-Figure8-1.png"}, {"is_image": true, "path": "CHI18_paper589-Figure5-1.png", "doi": "https://doi.org/10.1145/3173574.3174163", "caption": "Figure 5: Definition used for the pitch and roll angles.", "name": "CHI18_paper589-Figure5-1.png"}], "name": "Text indicator", "code": "measure@Text indicator", "textinfo": "Text is used to directly show physical dimensions."}], "name": "measure", "code": "measure", "textinfo": "Visual techniques used to illustrate physical units."}], "name": "Visual techniques", "code": "Visual techniques", "textinfo": "Visual techniques identified across multiple figures."}], "images": ["CHI18_paper107-Figure2-1.png", "UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper411-Figure3-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "Ubicomp18_paper170-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper241-Figure2-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper529-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper401-Figure3-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper98-Figure4-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper436-Figure6-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper339-Figure5-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper18-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper589-Figure1-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper499-Figure5-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper201-Figure5-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper401-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper446-Figure6-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper160-Figure3-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper423-Figure8-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper436-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper401-Figure7-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper54-Figure4-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper61-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper779-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper160-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper223-Figure2-1.png", "CHI18_paper163-Figure1-1.png", "UIST18_paper499-Figure11-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper927-Figure6-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper638-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper737-Figure8-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "codes": ["Interaction type@computer interaction ", "Interaction type@controllers interaction", "Interaction type@distal interaction", "Interaction type@gaze interaction", "Interaction type@mid-air interaction", "Interaction type@on body interaction", "Interaction type@pen interaction", "Interaction type@tangible interaction", "Interaction type@touch interaction", "Situation@Desktop", "Situation@Outdoors", "Situation@Private indoors", "Situation@Public indoors", "Situation@Public/private transport", "Specific part@fingers", "Specific part@foot", "Specific part@hand", "Specific part@head", "UI@drawing", "UI@rendering", "activity@2D/3D creation", "activity@communication", "activity@data manipulation", "activity@driving", "activity@entertainment", "activity@fabrication", "activity@medical", "activity@production", "anonymization@blur", "anonymization@filling", "audible@sound based", "audible@speech output", "background@grayed out/blurred", "background@removed", "body part@full body", "body part@lower body", "body part@upper body", "color@transparency", "continuous@deforming", "continuous@rotating", "continuous@scaling", "continuous@scrolling", "continuous@translating", "continuous@writing/drawing", "device based@AR", "device based@VR", "device based@controllers", "device based@desktop devices", "device based@laptop devices", "device based@large surfaces", "device based@mobile devices", "device based@tangible objects", "discrete@Key press", "discrete@Pointing", "discrete@Symbolic gesture", "dynamic@Contact shapes", "effects@motion blur", "effects@stroboscopic", "effects@waves", "element@arrows", "element@color highlight", "enclosing@circle/rectangle", "enclosing@exact contour line", "grouping and linking@color grouping", "grouping and linking@grouping lines", "grouping and linking@identifiers grouping", "grouping and linking@text annotation", "hue@colors", "hue@grayscale", "hue@monochrome", "identifiers@letters", "identifiers@numbers", "identifiers@title", "line style@dashed", "line style@width", "lines and arrows@direction", "lines and arrows@projection", "lines and arrows@trajectories", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "number of frames@multi frames", "number of frames@one frame", "number@Multi-users", "number@Solo-user", "output modality@haptic", "point of view@1st person", "point of view@3rd person", "point of view@UI only", "point of view@overshoulder 3/4", "point of view@top", "purpose@design space", "purpose@interaction sequence", "purpose@interactive system", "realism@realistic", "realism@simplistic", "region@color area", "sound based@speech input", "sub-framing@UI embedded", "sub-framing@UI overlay", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "sub-framing@magnification lens", "time@moving", "time@still", "tracking based@inside out", "tracking based@outside in", "type@clipart/icon", "type@data visualization", "type@photo", "type@text", "visual@computer displays", "visual@head mounted displays", "visual@large displays", "visual@lights", "visual@mobile displays", "visual@projected displays"], "what_codes": ["Interaction type@computer interaction ", "Interaction type@controllers interaction", "Interaction type@distal interaction", "Interaction type@gaze interaction", "Interaction type@mid-air interaction", "Interaction type@on body interaction", "Interaction type@pen interaction", "Interaction type@tangible interaction", "Interaction type@touch interaction", "Situation@Desktop", "Situation@Outdoors", "Situation@Private indoors", "Situation@Public indoors", "Situation@Public/private transport", "Specific part@fingers", "Specific part@foot", "Specific part@hand", "Specific part@head", "activity@2D/3D creation", "activity@communication", "activity@data manipulation", "activity@driving", "activity@entertainment", "activity@fabrication", "activity@medical", "activity@production", "audible@sound based", "audible@speech output", "body part@full body", "body part@lower body", "body part@upper body", "continuous@deforming", "continuous@rotating", "continuous@scaling", "continuous@scrolling", "continuous@translating", "continuous@writing/drawing", "device based@AR", "device based@VR", "device based@controllers", "device based@desktop devices", "device based@laptop devices", "device based@large surfaces", "device based@mobile devices", "device based@tangible objects", "discrete@Key press", "discrete@Pointing", "discrete@Symbolic gesture", "number@Multi-users", "number@Solo-user", "output modality@haptic", "purpose@design space", "purpose@interaction sequence", "purpose@interactive system", "sound based@speech input", "time@moving", "time@still", "tracking based@inside out", "tracking based@outside in", "visual@computer displays", "visual@head mounted displays", "visual@large displays", "visual@lights", "visual@mobile displays", "visual@projected displays"], "how_codes": ["UI@drawing", "UI@rendering", "anonymization@blur", "anonymization@filling", "background@grayed out/blurred", "background@removed", "color@transparency", "dynamic@Contact shapes", "effects@motion blur", "effects@stroboscopic", "effects@waves", "element@arrows", "element@color highlight", "enclosing@circle/rectangle", "enclosing@exact contour line", "grouping and linking@color grouping", "grouping and linking@grouping lines", "grouping and linking@identifiers grouping", "grouping and linking@text annotation", "hue@colors", "hue@grayscale", "hue@monochrome", "identifiers@letters", "identifiers@numbers", "identifiers@title", "line style@dashed", "line style@width", "lines and arrows@direction", "lines and arrows@projection", "lines and arrows@trajectories", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "number of frames@multi frames", "number of frames@one frame", "point of view@1st person", "point of view@3rd person", "point of view@UI only", "point of view@overshoulder 3/4", "point of view@top", "realism@realistic", "realism@simplistic", "region@color area", "sub-framing@UI embedded", "sub-framing@UI overlay", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "sub-framing@magnification lens", "type@clipart/icon", "type@data visualization", "type@photo", "type@text"], "what_codes_coding_dict": {"CHI18_paper107-Figure2-1.png": ["purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "UIST18_paper45-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "tracking based@inside out", "device based@AR", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper184-Figure10-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@writing/drawing", "discrete@Pointing", "Interaction type@computer interaction ", "Interaction type@pen interaction", "device based@tangible objects", "device based@desktop devices", "visual@computer displays", "device based@mobile devices", "Interaction type@touch interaction", "visual@mobile displays"], "Ubicomp18_paper164-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper218-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "continuous@translating", "Interaction type@mid-air interaction"], "CHI18_paper558-Figure6-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@full body", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in", "Situation@Private indoors"], "CHI18_paper477-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper202-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper437-Figure3-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture"], "CHI18_paper411-Figure3-1.png": ["purpose@design space", "time@still", "activity@2D/3D creation", "device based@AR", "visual@head mounted displays"], "CHI18_paper18-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction"], "UIST18_paper19-Figure10-1.png": ["purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "body part@full body", "continuous@translating", "device based@AR", "visual@head mounted displays", "Situation@Public indoors", "Interaction type@mid-air interaction", "Interaction type@distal interaction"], "UIST18_paper335-Figure7-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@scaling", "Interaction type@touch interaction", "Interaction type@on body interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays", "Situation@Desktop"], "CHI18_paper198-Figure9-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper87-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@tangible objects", "tracking based@outside in", "Situation@Private indoors"], "UIST18_paper711-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@head", "discrete@Pointing", "Interaction type@on body interaction", "tracking based@inside out", "device based@VR", "visual@head mounted displays"], "CHI18_paper234-Figure1-1.png": ["purpose@interaction sequence", "time@still", "number@Solo-user", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper132-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@computer interaction ", "device based@desktop devices", "device based@mobile devices", "Interaction type@touch interaction", "visual@computer displays", "Situation@Desktop"], "UIST18_paper745-Figure10-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "Ubicomp18_paper162-Figure9-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper913-Figure12-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@mid-air interaction", "device based@AR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper19-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@rotating", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "Interaction type@mid-air interaction"], "CHI18_paper529-Figure6-1.png": ["purpose@interaction sequence", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CSCW18_paper151-Figure10-1.png": ["discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "purpose@interaction sequence", "time@still"], "CHI18_paper210-Figure1-1.png": ["purpose@design space", "time@still", "number@Multi-users", "body part@full body", "discrete@Key press", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@computer interaction ", "Interaction type@on body interaction", "Interaction type@distal interaction", "device based@desktop devices", "tracking based@outside in", "visual@computer displays", "visual@large displays"], "CHI18_paper248-Figure4-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper529-Figure12-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper634-Figure12-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper429-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@communication", "number@Solo-user", "body part@full body", "Interaction type@on body interaction", "output modality@haptic"], "CHI18_paper362-Figure7-1.png": ["purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper360-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper251-Figure1-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper150-Figure23-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "output modality@haptic", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "UIST18_paper825-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "Interaction type@pen interaction", "device based@tangible objects"], "UIST18_paper5-Figure10-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@mid-air interaction", "tracking based@outside in", "visual@head mounted displays", "output modality@haptic", "device based@VR"], "CHI18_paper515-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@tangible interaction", "device based@AR", "device based@tangible objects", "visual@head mounted displays"], "CHI18_paper539-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@distal interaction", "device based@mobile devices", "visual@large displays"], "CHI18_paper291-Figure2-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper485-Figure6-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper653-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "tracking based@outside in", "device based@VR", "visual@large displays", "visual@head mounted displays"], "CHI18_paper634-Figure9-1.png": ["purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper446-Figure10-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@AR", "output modality@haptic", "visual@head mounted displays"], "Ubicomp18_paper170-Figure5-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "body part@upper body", "sound based@speech input"], "CSCW18_paper128-Figure2-1.png": ["purpose@design space", "time@still", "activity@communication", "activity@production", "number@Multi-users", "body part@full body", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@pen interaction", "visual@large displays", "visual@mobile displays", "device based@mobile devices", "device based@large surfaces"], "CHI18_paper529-Figure8-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper160-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "visual@lights", "visual@projected displays"], "CHI18_paper81-Figure1-1.png": ["purpose@design space", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@head", "body part@upper body", "Specific part@hand", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@AR", "device based@controllers", "visual@head mounted displays"], "UIST18_paper839-Figure3-1.png": ["purpose@interactive system", "purpose@design space", "time@still", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays", "output modality@haptic"], "UIST18_paper53-Figure6-1.png": ["purpose@design space", "time@still", "activity@entertainment", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "visual@mobile displays"], "CHI18_paper320-Figure11-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@controllers interaction", "Interaction type@on body interaction", "tracking based@outside in", "device based@VR", "device based@controllers", "visual@head mounted displays", "output modality@haptic"], "UIST18_paper825-Figure1-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@writing/drawing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "Interaction type@pen interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper613-Figure3-1.png": ["purpose@design space", "purpose@interactive system", "time@still", "activity@medical", "Specific part@hand", "visual@lights", "visual@mobile displays"], "CHI18_paper89-Figure14-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@full body"], "CHI18_paper76-Figure3-1.png": ["purpose@interaction sequence", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@scaling", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper86-Figure6-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@VR", "visual@head mounted displays", "output modality@haptic", "device based@controllers"], "CHI18_paper450-Figure10-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@touch interaction", "device based@mobile devices", "device based@tangible objects", "output modality@haptic", "visual@mobile displays"], "CHI18_paper202-Figure4-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper513-Figure9-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper368-Figure7-1.png": ["purpose@interaction sequence", "time@moving", "number@Multi-users", "Specific part@head", "discrete@Pointing", "Interaction type@gaze interaction", "device based@mobile devices"], "UIST18_paper675-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper46-Figure4-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@gaze interaction", "Interaction type@distal interaction", "Interaction type@tangible interaction", "device based@VR", "device based@AR", "device based@tangible objects", "visual@head mounted displays"], "CHI18_paper218-Figure7-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "discrete@Key press", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@controllers", "tracking based@outside in"], "CHI18_paper42-Figure5-1.png": ["time@moving", "purpose@interaction sequence", "number@Solo-user", "body part@full body", "continuous@rotating", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper185-Figure11-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@pen interaction", "device based@large surfaces", "visual@large displays"], "UIST18_paper365-Figure5-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@writing/drawing", "continuous@translating", "Interaction type@touch interaction", "Interaction type@pen interaction", "Interaction type@tangible interaction", "Interaction type@on body interaction", "tracking based@inside out", "output modality@haptic"], "Ubicomp18_paper184-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "audible@sound based", "visual@head mounted displays", "Situation@Public/private transport"], "CHI18_paper132-Figure1-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@scrolling", "Interaction type@touch interaction", "device based@mobile devices", "output modality@haptic", "visual@mobile displays"], "CHI18_paper378-Figure3-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "Specific part@foot", "discrete@Pointing", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "audible@sound based"], "CHI18_paper288-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@gaze interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper310-Figure1-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "Interaction type@mid-air interaction", "Interaction type@gaze interaction", "Interaction type@controllers interaction", "sound based@speech input", "device based@controllers", "tracking based@outside in"], "CHI18_paper27-Figure2-1.png": ["time@still", "purpose@design space", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper185-Figure7-1.png": ["purpose@design space", "time@still", "activity@production", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper497-Figure6-1.png": ["purpose@design space", "time@still", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper335-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@touch interaction", "tracking based@inside out", "device based@mobile devices", "device based@laptop devices", "visual@mobile displays", "visual@computer displays"], "CHI18_paper188-Figure5-1.png": ["purpose@interactive system", "purpose@interaction sequence", "time@still", "number@Solo-user", "activity@fabrication", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper177-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper21-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "Interaction type@mid-air interaction"], "CHI18_paper547-Figure2-1.png": ["purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "body part@upper body", "discrete@Pointing", "continuous@rotating", "continuous@scaling", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "device based@mobile devices", "device based@large surfaces", "visual@large displays"], "UIST18_paper737-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "output modality@haptic"], "UIST18_paper557-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "activity@communication", "activity@entertainment", "Specific part@hand", "body part@upper body", "discrete@Pointing", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "audible@speech output"], "CHI18_paper579-Figure9-1.png": ["purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "discrete@Pointing", "Interaction type@tangible interaction", "device based@AR", "device based@tangible objects", "activity@fabrication", "visual@head mounted displays"], "UIST18_paper321-Figure12-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper436-Figure7-1.png": ["purpose@interaction sequence", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@head", "discrete@Pointing", "Interaction type@gaze interaction", "device based@large surfaces", "visual@large displays", "Situation@Public/private transport"], "CHI18_paper89-Figure10-1.png": ["purpose@design space", "time@moving", "number@Solo-user", "body part@full body", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "CHI18_paper241-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "visual@head mounted displays"], "Ubicomp18_paper162-Figure8-1.png": ["purpose@design space", "time@still", "activity@2D/3D creation", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "continuous@writing/drawing", "continuous@scrolling", "continuous@translating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@controllers", "device based@large surfaces", "visual@large displays"], "CHI18_paper634-Figure11-1.png": ["purpose@interactive system", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CSCW18_paper192-Figure8-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "audible@sound based", "visual@mobile displays"], "CHI18_paper340-Figure3-1.png": ["tracking based@inside out", "device based@VR", "device based@controllers", "visual@head mounted displays", "Interaction type@mid-air interaction", "discrete@Pointing", "discrete@Symbolic gesture", "body part@upper body", "time@still", "purpose@design space"], "CHI18_paper629-Figure1-1.png": ["audible@sound based", "visual@projected displays", "device based@large surfaces", "Interaction type@touch interaction", "discrete@Pointing", "body part@upper body", "number@Solo-user", "activity@data manipulation", "time@still", "purpose@interactive system"], "CHI18_paper569-Figure27-1.png": ["time@moving", "purpose@design space", "number@Solo-user", "Specific part@fingers", "continuous@translating", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper89-Figure15-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper529-Figure3-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper241-Figure8-1.png": ["purpose@design space", "activity@entertainment", "time@still", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "discrete@Symbolic gesture", "device based@VR", "device based@controllers", "visual@head mounted displays"], "UIST18_paper675-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@fingers", "continuous@writing/drawing", "Interaction type@pen interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper401-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@communication", "activity@entertainment", "visual@projected displays"], "UIST18_paper853-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Multi-users", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper98-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@communication", "activity@medical", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "Ubicomp18_paper174-Figure1-1.png": ["purpose@design space", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "Interaction type@gaze interaction", "device based@desktop devices", "visual@computer displays", "tracking based@outside in"], "CHI18_paper86-Figure12-1.png": ["activity@fabrication", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@controllers interaction", "device based@VR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper219-Figure2-1.png": ["purpose@interactive system", "time@moving", "number@Solo-user", "body part@upper body", "device based@AR", "visual@head mounted displays"], "CHI18_paper54-Figure5-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@head", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays", "device based@mobile devices"], "CHI18_paper558-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Key press", "Interaction type@computer interaction ", "tracking based@outside in", "device based@desktop devices"], "CHI18_paper251-Figure2-1.png": ["time@still", "purpose@interactive system", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper867-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@medical", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "CHI18_paper380-Figure2-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "continuous@writing/drawing", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper5-Figure8-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper521-Figure1-1.png": ["purpose@interaction sequence", "time@still", "number@Solo-user", "Interaction type@controllers interaction", "Interaction type@distal interaction", "discrete@Pointing", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper90-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Multi-users", "body part@full body", "visual@head mounted displays", "device based@AR"], "CHI18_paper173-Figure3-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "Interaction type@touch interaction", "device based@mobile devices", "visual@projected displays"], "CHI18_paper363-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Pointing", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper529-Figure11-1.png": ["purpose@interaction sequence", "time@still", "discrete@Pointing", "Interaction type@touch interaction"], "UIST18_paper321-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "Interaction type@touch interaction", "visual@mobile displays", "device based@mobile devices"], "UIST18_paper697-Figure3-1.png": ["purpose@design space", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@on body interaction", "time@still"], "CHI18_paper613-Figure1-1.png": ["purpose@interactive system", "purpose@interaction sequence", "time@still", "activity@medical", "number@Solo-user", "body part@full body", "Specific part@fingers", "Interaction type@tangible interaction", "device based@controllers", "visual@lights"], "UIST18_paper745-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper151-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "Specific part@fingers", "discrete@Key press", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper529-Figure13-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper199-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper219-Figure14-1.png": ["device based@AR", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Interaction type@distal interaction", "visual@head mounted displays"], "CHI18_paper610-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper593-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays"], "CHI18_paper380-Figure5-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@entertainment", "activity@communication", "number@Multi-users", "Specific part@hand", "continuous@deforming", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper291-Figure3-1.png": ["purpose@design space", "activity@fabrication", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@scaling", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "device based@tangible objects", "Interaction type@tangible interaction", "discrete@Key press"], "CHI18_paper362-Figure3-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "activity@medical", "tracking based@inside out", "visual@mobile displays"], "CHI18_paper199-Figure13-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "activity@data manipulation", "Specific part@hand", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper436-Figure6-1.png": ["purpose@interaction sequence", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "tracking based@outside in", "Interaction type@gaze interaction", "device based@desktop devices", "visual@computer displays"], "UIST18_paper19-Figure15-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "device based@AR", "visual@head mounted displays"], "CHI18_paper634-Figure10-1.png": ["purpose@interactive system", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "discrete@Symbolic gesture"], "CHI18_paper89-Figure8-1.png": ["purpose@design space", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Specific part@hand", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays"], "CHI18_paper539-Figure9-1.png": ["purpose@design space", "activity@entertainment", "time@still", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper634-Figure3-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@scaling", "continuous@scrolling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "UIST18_paper499-Figure10-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@lower body"], "CHI18_paper11-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "audible@speech output"], "UIST18_paper913-Figure10-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "output modality@haptic"], "CHI18_paper564-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "visual@large displays"], "CHI18_paper567-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper579-Figure8-1.png": ["purpose@interactive system", "time@still", "activity@2D/3D creation", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Specific part@head", "discrete@Pointing", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "UIST18_paper127-Figure16-1.png": ["purpose@interactive system", "time@still", "activity@2D/3D creation", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "tracking based@outside in", "visual@computer displays"], "CHI18_paper150-Figure3-1.png": ["time@moving", "purpose@interaction sequence", "purpose@interactive system", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@tangible objects", "Interaction type@tangible interaction", "output modality@haptic", "visual@head mounted displays", "device based@VR"], "CHI18_paper439-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper241-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "discrete@Symbolic gesture", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper339-Figure5-1.png": ["purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "continuous@deforming", "continuous@scrolling", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper927-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@mid-air interaction", "discrete@Symbolic gesture", "tracking based@inside out", "device based@VR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper45-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "body part@upper body", "discrete@Pointing", "device based@AR", "visual@head mounted displays"], "UIST18_paper499-Figure9-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@controllers interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays"], "CHI18_paper42-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Solo-user", "body part@upper body", "Specific part@head", "discrete@Pointing", "device based@VR", "Interaction type@controllers interaction", "device based@controllers", "visual@head mounted displays"], "CHI18_paper638-Figure5-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper622-Figure4-1.png": ["purpose@design space", "time@still", "activity@production", "number@Multi-users", "body part@upper body", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays"], "UIST18_paper347-Figure3-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Key press", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "Interaction type@computer interaction ", "tracking based@outside in", "device based@desktop devices", "visual@computer displays"], "CHI18_paper18-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "Interaction type@mid-air interaction"], "CSCW18_paper185-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "continuous@scrolling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper209-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "Interaction type@mid-air interaction", "device based@AR", "visual@head mounted displays"], "CHI18_paper628-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction"], "CHI18_paper248-Figure6-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper5-Figure7-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper245-Figure12-1.png": ["purpose@interaction sequence", "purpose@design space", "time@still", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@scaling", "continuous@scrolling", "continuous@rotating", "continuous@translating", "Interaction type@distal interaction", "tracking based@outside in", "device based@large surfaces", "visual@large displays"], "UIST18_paper275-Figure4-1.png": ["purpose@interaction sequence", "purpose@interactive system", "time@moving", "activity@2D/3D creation", "number@Solo-user", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in"], "CHI18_paper289-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Multi-users", "body part@full body", "continuous@translating", "continuous@rotating", "discrete@Pointing", "Interaction type@tangible interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays"], "UIST18_paper839-Figure8-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "device based@VR", "visual@head mounted displays"], "CHI18_paper385-Figure1-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper291-Figure6-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@large surfaces", "output modality@haptic", "visual@large displays"], "CHI18_paper298-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper638-Figure1-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper164-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper877-Figure9-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper334-Figure5-1.png": ["purpose@design space", "number@Solo-user", "Specific part@foot", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "time@moving"], "CHI18_paper81-Figure10-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@controllers interaction", "Interaction type@gaze interaction", "discrete@Pointing", "device based@AR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper189-Figure11-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper589-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "Interaction type@mid-air interaction"], "CHI18_paper82-Figure9-1.png": ["purpose@interaction sequence", "activity@fabrication", "time@moving", "Interaction type@pen interaction", "Interaction type@tangible interaction", "device based@tangible objects", "continuous@writing/drawing", "discrete@Key press"], "UIST18_paper499-Figure5-1.png": ["purpose@interactive system", "purpose@design space", "time@still", "number@Solo-user", "body part@lower body", "Interaction type@tangible interaction", "continuous@translating", "tracking based@outside in", "device based@tangible objects", "device based@VR"], "CSCW18_paper192-Figure4-1.png": ["purpose@interaction sequence", "time@still", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@on body interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper913-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "Interaction type@tangible interaction", "output modality@haptic", "visual@computer displays", "device based@desktop devices"], "CHI18_paper372-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "visual@mobile displays", "visual@large displays"], "CHI18_paper123-Figure8-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "continuous@scrolling", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "UIST18_paper765-Figure1-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@on body interaction", "output modality@haptic", "device based@mobile devices"], "CHI18_paper336-Figure6-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper745-Figure8-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper499-Figure15-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "continuous@translating", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@tangible objects", "device based@controllers", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "discrete@Pointing"], "CHI18_paper241-Figure7-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper89-Figure16-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "device based@tangible objects", "visual@head mounted displays", "device based@VR"], "CHI18_paper558-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "body part@full body", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper87-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "continuous@writing/drawing"], "CHI18_paper245-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@mid-air interaction"], "CHI18_paper150-Figure9-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "device based@VR", "visual@head mounted displays"], "CHI18_paper644-Figure7-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "discrete@Pointing", "Interaction type@controllers interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper499-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "Interaction type@controllers interaction", "Interaction type@tangible interaction", "device based@VR", "device based@tangible objects", "visual@head mounted displays", "tracking based@outside in"], "CHI18_paper654-Figure5-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper867-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@scrolling", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "CHI18_paper374-Figure2-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper336-Figure14-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "device based@tangible objects", "Interaction type@tangible interaction", "output modality@haptic"], "CHI18_paper19-Figure3-1.png": ["Interaction type@distal interaction", "Interaction type@touch interaction", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@full body", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays", "visual@large displays"], "UIST18_paper913-Figure11-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@mobile devices", "output modality@haptic"], "CHI18_paper224-Figure4-1.png": ["purpose@interaction sequence", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays", "device based@large surfaces"], "UIST18_paper927-Figure7-1.png": ["time@still", "purpose@design space", "number@Solo-user", "body part@full body", "device based@VR", "Interaction type@mid-air interaction", "continuous@rotating", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper78-Figure6-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "device based@VR", "tracking based@outside in", "tracking based@inside out", "visual@head mounted displays"], "CHI18_paper411-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@rotating", "discrete@Key press", "Interaction type@tangible interaction", "device based@tangible objects", "device based@AR", "visual@projected displays"], "CHI18_paper5-Figure3-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "visual@lights", "device based@tangible objects", "discrete@Symbolic gesture"], "CHI18_paper477-Figure3-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper150-Figure20-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@tangible interaction", "activity@fabrication", "tracking based@outside in", "device based@tangible objects", "output modality@haptic"], "UIST18_paper825-Figure9-1.png": ["purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@large surfaces", "visual@large displays"], "CHI18_paper201-Figure5-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "body part@full body", "tracking based@outside in", "device based@controllers", "Interaction type@controllers interaction"], "UIST18_paper19-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@communication", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "device based@mobile devices"], "CHI18_paper65-Figure4-1.png": ["purpose@design space", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "time@still"], "CHI18_paper86-Figure13-1.png": ["discrete@Pointing", "discrete@Key press", "Interaction type@tangible interaction", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@controllers", "device based@tangible objects", "device based@VR", "visual@head mounted displays", "time@still", "purpose@design space", "activity@data manipulation"], "CHI18_paper354-Figure18-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "activity@fabrication", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper5-Figure9-1.png": ["purpose@interactive system", "activity@fabrication", "time@still", "number@Solo-user", "Specific part@hand", "continuous@deforming", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper547-Figure1-1.png": ["purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "body part@upper body", "continuous@translating", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays", "device based@large surfaces"], "CHI18_paper298-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@touch interaction", "device based@large surfaces", "device based@tangible objects", "visual@large displays"], "CHI18_paper603-Figure17-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper205-Figure14-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@touch interaction"], "CHI18_paper374-Figure4-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper401-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "device based@VR", "device based@controllers", "Interaction type@controllers interaction", "Interaction type@distal interaction", "visual@projected displays"], "UIST18_paper65-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Specific part@hand", "Specific part@foot", "Interaction type@on body interaction", "tracking based@inside out", "output modality@haptic"], "CHI18_paper423-Figure10-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper629-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@deforming", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper19-Figure10-1.png": ["purpose@interaction sequence", "purpose@design space", "time@still", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays", "device based@large surfaces"], "Ubicomp18_paper185-Figure3-1.png": ["purpose@design space", "time@moving", "number@Solo-user", "body part@full body", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper446-Figure6-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@tangible interaction", "Interaction type@distal interaction", "device based@AR", "device based@tangible objects", "tracking based@outside in", "output modality@haptic", "visual@head mounted displays", "Situation@Private indoors"], "UIST18_paper335-Figure9-1.png": ["purpose@design space", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "discrete@Symbolic gesture", "Interaction type@distal interaction", "Interaction type@touch interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper160-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@distal interaction", "sound based@speech input"], "Ubicomp18_paper161-Figure9-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "device based@mobile devices", "Interaction type@on body interaction"], "CHI18_paper529-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper297-Figure3-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating", "discrete@Key press", "Interaction type@tangible interaction", "Interaction type@touch interaction", "device based@tangible objects", "visual@mobile displays"], "CHI18_paper86-Figure8-1.png": ["purpose@interaction sequence", "purpose@design space", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic"], "CHI18_paper644-Figure9-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "continuous@rotating", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper446-Figure3-1.png": ["purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "continuous@rotating", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@AR", "tracking based@outside in", "visual@head mounted displays"], "CHI18_paper529-Figure7-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper89-Figure11-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@VR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper210-Figure3-1.png": ["purpose@interactive system", "time@still"], "CHI18_paper564-Figure6-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "continuous@translating", "Interaction type@on body interaction", "tracking based@outside in", "visual@computer displays", "Specific part@hand"], "CHI18_paper266-Figure7-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "Interaction type@computer interaction ", "Interaction type@tangible interaction", "device based@desktop devices", "device based@tangible objects", "number@Solo-user"], "CHI18_paper362-Figure4-1.png": ["purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper502-Figure7-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper248-Figure3-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@translating", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper117-Figure13-1.png": ["purpose@design space", "time@still", "activity@entertainment", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@writing/drawing", "continuous@translating", "Interaction type@pen interaction", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@large surfaces", "visual@computer displays"], "CHI18_paper281-Figure8-1.png": ["purpose@interactive system", "time@moving", "activity@communication", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in"], "CHI18_paper291-Figure4-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper411-Figure5-1.png": ["purpose@interaction sequence", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@scaling", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers", "visual@projected displays"], "CHI18_paper638-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper89-Figure4-1.png": ["activity@entertainment", "number@Solo-user", "body part@upper body"], "CHI18_paper162-Figure11-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper237-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper541-Figure1-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@medical", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Key press", "Interaction type@computer interaction ", "device based@tangible objects", "audible@sound based", "output modality@haptic"], "UIST18_paper963-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper541-Figure5-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "device based@controllers", "Interaction type@controllers interaction", "audible@sound based", "visual@computer displays"], "CHI18_paper248-Figure9-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@writing/drawing", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper595-Figure5-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "activity@fabrication", "device based@mobile devices"], "CHI18_paper236-Figure3-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "sound based@speech input", "device based@tangible objects", "audible@speech output"], "CHI18_paper433-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "body part@full body", "visual@mobile displays", "visual@projected displays"], "CHI18_paper508-Figure7-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "Interaction type@touch interaction", "discrete@Key press", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "tracking based@outside in"], "UIST18_paper725-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper89-Figure13-1.png": ["purpose@design space", "time@still", "tracking based@outside in"], "UIST18_paper473-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@touch interaction", "tracking based@outside in", "device based@tangible objects", "visual@computer displays"], "CHI18_paper237-Figure12-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "UIST18_paper99-Figure16-1.png": ["purpose@interaction sequence", "activity@fabrication", "time@moving", "number@Solo-user", "Specific part@hand", "continuous@deforming", "continuous@translating"], "UIST18_paper5-Figure11-1.png": ["purpose@design space", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@mid-air interaction", "device based@VR", "output modality@haptic"], "UIST18_paper247-Figure10-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "output modality@haptic"], "CHI18_paper185-Figure9-1.png": ["purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing"], "CHI18_paper209-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "body part@upper body", "discrete@Pointing", "discrete@Key press", "Interaction type@computer interaction ", "device based@AR", "device based@controllers", "Interaction type@controllers interaction", "device based@desktop devices", "visual@computer displays", "visual@head mounted displays"], "CHI18_paper411-Figure10-1.png": ["purpose@design space", "time@moving", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "Ubicomp18_paper200-Figure6-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@desktop devices", "device based@mobile devices", "device based@tangible objects", "visual@computer displays"], "CHI18_paper185-Figure12-1.png": ["purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@scaling", "discrete@Pointing", "Interaction type@pen interaction"], "CHI18_paper470-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@touch interaction", "discrete@Key press"], "CHI18_paper178-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "visual@mobile displays", "tracking based@outside in"], "CHI18_paper291-Figure7-1.png": ["purpose@interaction sequence", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "UIST18_paper737-Figure7-1.png": ["purpose@interaction sequence", "purpose@interactive system", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "Interaction type@touch interaction", "output modality@haptic", "device based@mobile devices"], "CHI18_paper446-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@mid-air interaction", "device based@AR", "tracking based@outside in", "visual@lights", "visual@head mounted displays"], "CHI18_paper644-Figure11-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@deforming", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers"], "UIST18_paper595-Figure15-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Symbolic gesture", "discrete@Pointing", "Interaction type@touch interaction", "device based@AR", "device based@mobile devices", "activity@fabrication", "visual@head mounted displays", "visual@mobile displays"], "CHI18_paper249-Figure4-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "discrete@Pointing", "continuous@scrolling", "discrete@Symbolic gesture", "Interaction type@touch interaction"], "CHI18_paper20-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "continuous@rotating", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays"], "CHI18_paper653-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper69-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "audible@sound based", "visual@mobile displays"], "CHI18_paper541-Figure9-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "device based@tangible objects", "output modality@haptic", "visual@computer displays", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper603-Figure1-1.png": ["discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "purpose@design space", "time@still"], "CSCW18_paper118-Figure6-1.png": ["purpose@design space", "time@still", "activity@production", "number@Multi-users", "body part@full body", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@touch interaction", "device based@mobile devices", "device based@large surfaces", "visual@mobile displays", "visual@large displays", "visual@projected displays"], "UIST18_paper649-Figure6-1.png": ["purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction"], "CHI18_paper378-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@touch interaction", "Interaction type@on body interaction", "Interaction type@computer interaction ", "device based@desktop devices", "tracking based@inside out"], "CHI18_paper241-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@communication", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper423-Figure8-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@distal interaction"], "UIST18_paper649-Figure5-1.png": ["purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction"], "CHI18_paper188-Figure8-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "Interaction type@touch interaction"], "CHI18_paper446-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@tangible objects", "tracking based@inside out", "visual@head mounted displays", "output modality@haptic"], "UIST18_paper853-Figure8-1.png": ["purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper589-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@rotating", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper477-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper53-Figure5-1.png": ["purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@mobile devices", "device based@tangible objects", "output modality@haptic", "visual@mobile displays"], "CHI18_paper150-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays", "device based@large surfaces"], "UIST18_paper141-Figure14-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper128-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@mid-air interaction", "Interaction type@controllers interaction", "device based@controllers", "device based@VR", "visual@head mounted displays"], "CHI18_paper219-Figure8-1.png": ["purpose@interactive system", "time@moving", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@mid-air interaction", "device based@mobile devices"], "CHI18_paper446-Figure12-1.png": ["purpose@interaction sequence", "activity@entertainment", "time@moving", "number@Solo-user", "body part@full body", "continuous@deforming", "discrete@Pointing", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "tracking based@inside out", "device based@AR", "device based@tangible objects", "visual@head mounted displays", "output modality@haptic"], "CHI18_paper82-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@production", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@touch interaction", "Interaction type@computer interaction ", "device based@mobile devices", "device based@desktop devices", "visual@mobile displays", "visual@computer displays"], "CHI18_paper436-Figure2-1.png": ["purpose@design space", "time@still", "discrete@Pointing"], "CHI18_paper42-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@rotating", "Interaction type@mid-air interaction", "Interaction type@controllers interaction", "tracking based@outside in", "device based@VR", "device based@controllers", "visual@head mounted displays"], "UIST18_paper87-Figure10-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper245-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "body part@full body", "discrete@Pointing", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays"], "UIST18_paper511-Figure12-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@controllers interaction", "Interaction type@distal interaction", "discrete@Pointing", "device based@VR", "device based@controllers", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper460-Figure1-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper188-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper502-Figure1-1.png": ["purpose@interactive system", "time@still", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper485-Figure7-1.png": ["purpose@interactive system", "purpose@design space", "time@still", "activity@entertainment", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "audible@sound based", "visual@mobile displays"], "CHI18_paper320-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "continuous@deforming", "Interaction type@on body interaction", "tracking based@inside out", "visual@computer displays"], "UIST18_paper213-Figure20-1.png": ["purpose@interaction sequence", "time@moving", "activity@medical", "number@Solo-user", "body part@upper body", "Specific part@hand", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays", "Interaction type@on body interaction", "Situation@Private indoors"], "CHI18_paper73-Figure4-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@scrolling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays", "Situation@Private indoors"], "CHI18_paper629-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@large surfaces", "audible@sound based", "visual@projected displays", "Situation@Desktop"], "CHI18_paper613-Figure8-1.png": ["purpose@design space", "purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "Specific part@fingers", "tracking based@inside out", "Interaction type@tangible interaction", "device based@tangible objects", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper162-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper508-Figure3-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Key press", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper142-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Multi-users", "Specific part@hand", "body part@upper body", "discrete@Pointing", "continuous@translating", "continuous@scaling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays", "Situation@Private indoors"], "CHI18_paper245-Figure6-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper291-Figure5-1.png": ["purpose@interactive system", "time@moving", "activity@data manipulation", "continuous@deforming", "Interaction type@tangible interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper336-Figure13-1.png": ["purpose@interactive system", "purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper613-Figure7-1.png": ["purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@fingers", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper202-Figure5-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Private indoors"], "CHI18_paper150-Figure13-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "tracking based@outside in", "visual@computer displays"], "CSCW18_paper159-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers", "device based@large surfaces", "visual@large displays", "continuous@scaling", "Situation@Private indoors"], "CHI18_paper150-Figure15-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "tracking based@outside in", "visual@computer displays"], "CHI18_paper300-Figure5-1.png": ["purpose@design space", "time@still", "activity@communication", "activity@production", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Public/private transport"], "CHI18_paper569-Figure15-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "visual@lights", "device based@tangible objects", "Interaction type@tangible interaction"], "CHI18_paper360-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper321-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@mobile devices", "device based@tangible objects", "visual@mobile displays"], "CHI18_paper25-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "continuous@scaling", "device based@large surfaces", "visual@large displays"], "UIST18_paper511-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors"], "CSCW18_paper192-Figure5-1.png": ["purpose@design space", "time@still", "activity@entertainment", "activity@communication", "number@Multi-users", "Specific part@hand", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "Ubicomp18_paper194-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@full body", "Specific part@hand", "Interaction type@distal interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper564-Figure2-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "tracking based@outside in", "visual@large displays", "visual@mobile displays", "visual@computer displays", "visual@head mounted displays"], "UIST18_paper853-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating"], "CHI18_paper406-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@communication", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@mid-air interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper353-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "device based@AR", "visual@mobile displays", "visual@computer displays", "Interaction type@computer interaction ", "device based@laptop devices"], "CHI18_paper661-Figure1-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "continuous@scrolling", "device based@mobile devices", "visual@large displays"], "CHI18_paper362-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "body part@upper body", "device based@mobile devices", "visual@mobile displays", "Interaction type@touch interaction", "discrete@Key press"], "CHI18_paper529-Figure1-1.png": ["purpose@interaction sequence", "time@moving", "activity@production", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper220-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "body part@full body", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper335-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction"], "CSCW18_paper072-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper33-Figure5-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@on body interaction", "device based@mobile devices"], "CHI18_paper425-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper76-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@mobile devices", "visual@mobile displays", "activity@data manipulation"], "CHI18_paper541-Figure8-1.png": ["purpose@design space", "purpose@interaction sequence", "time@still", "number@Solo-user", "Specific part@hand", "Specific part@foot", "body part@upper body", "discrete@Key press", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic", "visual@computer displays", "audible@sound based"], "CHI18_paper54-Figure10-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@full body", "device based@VR", "visual@head mounted displays"], "CHI18_paper579-Figure7-1.png": ["purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "Specific part@head", "discrete@Pointing", "discrete@Key press", "Interaction type@controllers interaction", "Interaction type@distal interaction", "tracking based@outside in", "device based@VR", "device based@controllers", "visual@head mounted displays", "activity@fabrication"], "UIST18_paper5-Figure7-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "output modality@haptic"], "CHI18_paper287-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in"], "CHI18_paper354-Figure17-1.png": ["purpose@design space", "time@still", "activity@entertainment", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper737-Figure6-1.png": ["purpose@interactive system", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "output modality@haptic"], "CHI18_paper89-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "device based@VR", "device based@tangible objects", "visual@head mounted displays"], "UIST18_paper5-Figure9-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction"], "UIST18_paper877-Figure3-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic"], "CHI18_paper401-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@driving", "activity@entertainment", "number@Solo-user", "body part@full body", "continuous@rotating", "discrete@Key press", "Interaction type@controllers interaction", "device based@controllers", "visual@large displays", "Situation@Private indoors"], "Ubicomp18_paper176-Figure3-1.png": ["purpose@design space", "time@still", "activity@communication", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays", "Situation@Desktop"], "CHI18_paper142-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "CHI18_paper547-Figure9-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@pen interaction", "Interaction type@distal interaction", "device based@AR"], "CHI18_paper61-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "tracking based@inside out", "visual@lights", "Situation@Public indoors"], "CHI18_paper428-Figure1-1.png": ["purpose@interactive system", "purpose@design space", "time@still", "number@Solo-user", "body part@full body", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper199-Figure17-1.png": ["purpose@interaction sequence", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper69-Figure2-1.png": ["purpose@interaction sequence", "purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "audible@speech output", "visual@mobile displays"], "CHI18_paper291-Figure9-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic", "tracking based@outside in", "device based@VR", "visual@head mounted displays", "Situation@Desktop"], "CHI18_paper150-Figure8-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper547-Figure8-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@large surfaces"], "CHI18_paper123-Figure7-1.png": ["time@still", "purpose@design space", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction "], "UIST18_paper53-Figure2-1.png": ["purpose@design space", "time@still", "output modality@haptic", "device based@tangible objects", "Interaction type@tangible interaction", "device based@mobile devices"], "CHI18_paper54-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "body part@upper body", "discrete@Pointing", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "CHI18_paper593-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays"], "CHI18_paper123-Figure9-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing"], "CHI18_paper443-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper43-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "Interaction type@distal interaction", "tracking based@outside in", "visual@mobile displays"], "Ubicomp18_paper181-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "visual@mobile displays"], "CHI18_paper54-Figure4-1.png": ["purpose@design space", "time@still", "activity@communication", "number@Multi-users", "body part@upper body", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@distal interaction", "Interaction type@mid-air interaction", "device based@mobile devices", "visual@head mounted displays", "device based@VR"], "Ubicomp18_paper162-Figure1-1.png": ["purpose@design space", "purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper626-Figure1-1.png": ["purpose@interaction sequence", "time@moving", "activity@production", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper496-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Interaction type@computer interaction ", "device based@desktop devices"], "UIST18_paper335-Figure1-1.png": ["purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "continuous@writing/drawing", "discrete@Symbolic gesture", "device based@mobile devices", "visual@mobile displays"], "Ubicomp18_paper164-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays", "tracking based@outside in"], "CHI18_paper189-Figure9-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Multi-users", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper613-Figure10-1.png": ["purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@hand", "body part@upper body", "Interaction type@tangible interaction", "Interaction type@on body interaction", "tracking based@inside out", "device based@tangible objects", "visual@lights"], "CHI18_paper593-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays", "Situation@Private indoors"], "CSCW18_paper185-Figure5-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays", "Situation@Private indoors"], "UIST18_paper485-Figure9-1.png": ["purpose@design space", "time@still", "activity@communication", "number@Multi-users", "Specific part@fingers", "body part@upper body", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper61-Figure9-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@on body interaction", "tracking based@inside out", "visual@lights", "Situation@Private indoors"], "Ubicomp18_paper164-Figure8-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "discrete@Pointing"], "CHI18_paper529-Figure14-1.png": ["purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper11-Figure1-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "Situation@Desktop"], "CHI18_paper362-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@controllers interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Outdoors"], "UIST18_paper87-Figure8-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@mobile devices", "Situation@Private indoors"], "UIST18_paper321-Figure8-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper401-Figure8-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@mid-air interaction", "discrete@Symbolic gesture", "tracking based@outside in", "visual@projected displays", "Situation@Private indoors"], "CHI18_paper95-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "tracking based@inside out", "visual@computer displays", "Situation@Desktop"], "CHI18_paper5-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "visual@lights"], "CHI18_paper199-Figure3-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "continuous@translating", "Interaction type@tangible interaction", "Interaction type@pen interaction", "device based@tangible objects"], "CHI18_paper45-Figure3-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@full body", "continuous@scrolling", "device based@desktop devices", "visual@head mounted displays", "Interaction type@controllers interaction", "Situation@Private indoors"], "CHI18_paper218-Figure14-1.png": ["activity@production", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@mid-air interaction", "Interaction type@pen interaction", "device based@VR", "device based@AR", "visual@head mounted displays", "purpose@design space", "time@still"], "CHI18_paper89-Figure9-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "continuous@rotating", "continuous@translating", "continuous@deforming", "device based@tangible objects", "visual@head mounted displays", "device based@VR", "tracking based@outside in"], "UIST18_paper637-Figure4-1.png": ["time@still", "purpose@design space", "activity@2D/3D creation", "number@Solo-user", "discrete@Pointing", "continuous@translating", "continuous@scaling", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper150-Figure4-1.png": ["purpose@interactive system", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@large surfaces", "device based@VR"], "CHI18_paper199-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper516-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@controllers interaction", "device based@controllers", "audible@sound based", "visual@computer displays"], "CHI18_paper238-Figure3-1.png": ["purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@mid-air interaction", "tracking based@outside in"], "UIST18_paper779-Figure3-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@mobile devices"], "UIST18_paper901-Figure9-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "CHI18_paper284-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "device based@tangible objects", "Interaction type@tangible interaction", "Interaction type@mid-air interaction"], "CHI18_paper336-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "activity@fabrication", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper73-Figure5-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Multi-users", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays"], "CHI18_paper406-Figure3-1.png": ["Interaction type@touch interaction", "continuous@translating", "discrete@Pointing", "visual@mobile displays", "device based@mobile devices", "number@Solo-user", "Specific part@hand", "activity@communication", "purpose@interaction sequence", "time@moving"], "CHI18_paper189-Figure8-1.png": ["purpose@interaction sequence", "time@moving", "activity@fabrication", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper411-Figure6-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "activity@fabrication", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "Interaction type@controllers interaction"], "CHI18_paper223-Figure7-1.png": ["purpose@design space", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@writing/drawing", "continuous@translating", "Interaction type@touch interaction", "Interaction type@pen interaction"], "UIST18_paper53-Figure7-1.png": ["purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "device based@mobile devices", "output modality@haptic", "visual@computer displays", "visual@mobile displays", "Interaction type@computer interaction ", "Interaction type@controllers interaction"], "UIST18_paper745-Figure11-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "Interaction type@on body interaction", "visual@mobile displays"], "UIST18_paper853-Figure6-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "discrete@Symbolic gesture"], "CHI18_paper18-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction"], "CHI18_paper219-Figure13-1.png": ["purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper426-Figure4-1.png": ["Situation@Desktop", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices"], "CHI18_paper248-Figure5-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper477-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction"], "UIST18_paper499-Figure6-1.png": ["purpose@design space", "time@still", "device based@VR", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper185-Figure6-1.png": ["purpose@design space", "time@still", "activity@production", "number@Solo-user", "body part@upper body", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@mid-air interaction", "device based@controllers", "device based@AR", "device based@mobile devices", "visual@head mounted displays", "visual@mobile displays", "Situation@Desktop"], "UIST18_paper839-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@scrolling", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper515-Figure8-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@large surfaces", "visual@mobile displays", "Situation@Desktop"], "CHI18_paper160-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Multi-users", "body part@full body", "visual@projected displays", "Situation@Private indoors"], "CHI18_paper165-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "visual@head mounted displays", "Situation@Public/private transport"], "CHI18_paper160-Figure6-1.png": ["purpose@interactive system", "time@still", "Situation@Private indoors"], "CHI18_paper426-Figure9-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "visual@mobile displays", "device based@mobile devices", "Situation@Desktop"], "UIST18_paper335-Figure6-1.png": ["purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays", "Situation@Desktop"], "UIST18_paper313-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating"], "CHI18_paper89-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "purpose@interaction sequence", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors", "discrete@Key press"], "CHI18_paper531-Figure2-1.png": ["purpose@interactive system", "activity@fabrication", "time@still", "number@Solo-user", "body part@upper body"], "CHI18_paper551-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays", "device based@large surfaces"], "CHI18_paper284-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "visual@lights", "Situation@Desktop"], "CHI18_paper69-Figure4-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "audible@speech output", "visual@mobile displays"], "UIST18_paper499-Figure12-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@full body", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper31-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@VR", "visual@head mounted displays", "tracking based@outside in"], "UIST18_paper153-Figure3-1.png": ["purpose@interactive system", "time@moving", "activity@data manipulation", "Interaction type@computer interaction ", "continuous@translating"], "CHI18_paper569-Figure17-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@controllers", "Interaction type@controllers interaction"], "CHI18_paper446-Figure11-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@full body", "continuous@translating", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@AR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper223-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "continuous@writing/drawing", "Interaction type@pen interaction"], "CHI18_paper163-Figure1-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "visual@mobile displays", "tracking based@outside in", "Interaction type@distal interaction"], "UIST18_paper499-Figure11-1.png": ["purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "device based@AR", "device based@VR", "visual@head mounted displays", "device based@tangible objects", "Interaction type@tangible interaction", "Interaction type@controllers interaction", "device based@controllers"], "Ubicomp18_paper201-Figure3-1.png": ["purpose@design space", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@computer interaction ", "device based@desktop devices", "visual@lights"], "UIST18_paper853-Figure3-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction"], "CHI18_paper11-Figure6-1.png": ["Specific part@hand", "purpose@design space", "time@still", "number@Solo-user", "discrete@Key press", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays"], "CHI18_paper143-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@controllers interaction", "Interaction type@computer interaction ", "device based@controllers", "device based@desktop devices", "tracking based@outside in", "device based@VR", "visual@head mounted displays", "visual@computer displays", "Situation@Desktop"], "CHI18_paper150-Figure21-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "output modality@haptic"], "CHI18_paper150-Figure22-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "tracking based@outside in", "device based@tangible objects", "Interaction type@tangible interaction", "output modality@haptic"], "CHI18_paper150-Figure5-1.png": ["purpose@interactive system", "time@still", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "tracking based@outside in", "visual@computer displays"], "CHI18_paper150-Figure6-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "Interaction type@tangible interaction", "device based@VR", "device based@tangible objects", "visual@head mounted displays"], "CHI18_paper189-Figure1-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights", "visual@computer displays", "activity@entertainment", "Situation@Desktop"], "CHI18_paper189-Figure10-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@deforming", "discrete@Key press", "Interaction type@tangible interaction", "Interaction type@computer interaction ", "device based@tangible objects", "device based@desktop devices", "visual@computer displays"], "CHI18_paper19-Figure7-1.png": ["Specific part@hand", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@mobile displays", "visual@large displays"], "CHI18_paper199-Figure16-1.png": ["Specific part@hand", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper199-Figure5-1.png": ["Specific part@hand", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper210-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@medical", "number@Multi-users", "body part@full body", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic"], "CHI18_paper210-Figure4-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "visual@head mounted displays", "visual@projected displays"], "CHI18_paper218-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "continuous@translating", "continuous@scaling", "Interaction type@mid-air interaction", "Interaction type@distal interaction"], "CHI18_paper218-Figure8-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays"], "CHI18_paper219-Figure12-1.png": ["purpose@interactive system", "time@still", "Interaction type@distal interaction", "device based@AR", "visual@head mounted displays"], "CHI18_paper238-Figure7-1.png": ["Specific part@hand", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "discrete@Pointing", "continuous@translating", "continuous@rotating", "Interaction type@mid-air interaction"], "CHI18_paper249-Figure2-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "discrete@Symbolic gesture"], "CHI18_paper255-Figure5-1.png": ["Specific part@hand", "purpose@design space", "time@still", "number@Solo-user", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper291-Figure8-1.png": ["Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects", "device based@VR", "visual@head mounted displays", "output modality@haptic"], "CHI18_paper299-Figure2-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Multi-users", "number@Solo-user", "Specific part@hand", "body part@upper body", "continuous@deforming", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@on body interaction", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@mobile devices", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper31-Figure1-1.png": ["Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "tracking based@outside in", "device based@mobile devices", "Interaction type@touch interaction"], "CHI18_paper336-Figure15-1.png": ["Specific part@fingers", "activity@fabrication", "purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper359-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Multi-users", "body part@upper body", "body part@full body", "Interaction type@controllers interaction", "device based@controllers", "device based@VR", "visual@head mounted displays"], "CHI18_paper362-Figure5-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper362-Figure6-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "device based@mobile devices", "visual@mobile displays", "Interaction type@controllers interaction"], "CHI18_paper407-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "visual@projected displays"], "CHI18_paper419-Figure1-1.png": ["Specific part@hand", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@AR", "device based@mobile devices", "visual@mobile displays", "visual@head mounted displays"], "CHI18_paper441-Figure21-1.png": ["purpose@design space", "time@moving", "activity@fabrication"], "CHI18_paper445-Figure4-1.png": ["purpose@interactive system", "time@moving"], "CHI18_paper446-Figure7-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@tangible interaction", "device based@AR", "device based@tangible objects", "tracking based@inside out", "visual@head mounted displays"], "CHI18_paper465-Figure7-1.png": ["purpose@interaction sequence", "time@still", "activity@medical", "visual@head mounted displays", "device based@AR"], "CHI18_paper530-Figure1-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "tracking based@outside in", "visual@head mounted displays"], "CHI18_paper54-Figure6-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "Specific part@head", "discrete@Pointing", "Interaction type@touch interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "device based@mobile devices"], "CHI18_paper541-Figure3-1.png": ["purpose@interaction sequence", "time@moving", "Interaction type@touch interaction", "audible@sound based"], "CHI18_paper547-Figure10-1.png": ["Specific part@hand", "purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "CHI18_paper547-Figure5-1.png": ["Specific part@hand", "purpose@design space", "time@still", "number@Solo-user", "continuous@translating", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "device based@large surfaces"], "CHI18_paper547-Figure7-1.png": ["Specific part@hand", "purpose@design space", "time@still", "purpose@interaction sequence", "number@Solo-user", "continuous@rotating", "continuous@translating", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@mid-air interaction"], "CHI18_paper558-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper564-Figure3-1.png": ["Specific part@hand", "purpose@design space", "time@moving", "number@Solo-user", "discrete@Pointing", "continuous@translating", "Interaction type@on body interaction", "tracking based@outside in"], "CHI18_paper579-Figure13-1.png": ["purpose@interaction sequence", "activity@2D/3D creation", "activity@fabrication", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@AR", "visual@head mounted displays"], "CHI18_paper61-Figure10-1.png": ["purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@on body interaction", "visual@lights"], "CHI18_paper634-Figure2-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper634-Figure8-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@rotating", "continuous@scaling", "continuous@scrolling", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper647-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "discrete@Key press", "Interaction type@distal interaction", "device based@VR", "device based@controllers", "visual@head mounted displays", "Interaction type@controllers interaction"], "CHI18_paper654-Figure4-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@deforming", "output modality@haptic", "device based@controllers", "Interaction type@controllers interaction"], "CHI18_paper69-Figure3-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "audible@speech output"], "CHI18_paper73-Figure2-1.png": ["purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "continuous@rotating", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "device based@large surfaces", "visual@large displays", "visual@mobile displays"], "CHI18_paper78-Figure2-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper82-Figure7-1.png": ["activity@fabrication", "purpose@interaction sequence", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@touch interaction", "visual@computer displays"], "CHI18_paper90-Figure7-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Multi-users", "body part@upper body", "Interaction type@mid-air interaction", "Interaction type@touch interaction", "device based@VR", "device based@large surfaces", "visual@large displays", "visual@head mounted displays"], "CHI18_paper95-Figure3-1.png": ["time@still", "purpose@interactive system", "number@Solo-user", "Specific part@hand", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper98-Figure2-1.png": ["purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays", "Interaction type@distal interaction"], "CSCW18_paper140-Figure3-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction"], "CSCW18_paper140-Figure5-1.png": ["purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CSCW18_paper185-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "CSCW18_paper192-Figure6-1.png": ["purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper31-Figure4-1.png": ["purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper321-Figure15-1.png": ["Specific part@fingers", "purpose@design space", "time@still", "activity@data manipulation", "activity@entertainment", "number@Solo-user", "continuous@rotating", "discrete@Symbolic gesture", "continuous@translating", "continuous@deforming", "Interaction type@tangible interaction", "device based@mobile devices", "device based@tangible objects", "visual@mobile displays"], "UIST18_paper335-Figure8-1.png": ["Specific part@hand", "purpose@interaction sequence", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "discrete@Symbolic gesture", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "tracking based@inside out", "visual@mobile displays"], "UIST18_paper347-Figure1-1.png": ["purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@mid-air interaction", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "discrete@Symbolic gesture"], "UIST18_paper485-Figure3-1.png": ["Specific part@hand", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "activity@fabrication", "number@Solo-user", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper499-Figure7-1.png": ["purpose@design space", "time@still", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@controllers interaction", "device based@VR", "visual@head mounted displays", "Situation@Private indoors"], "UIST18_paper581-Figure10-1.png": ["purpose@interactive system", "time@still", "activity@communication", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Public/private transport"], "UIST18_paper745-Figure13-1.png": ["Specific part@fingers", "purpose@interactive system", "time@still", "number@Solo-user", "discrete@Pointing", "device based@tangible objects", "Interaction type@tangible interaction", "visual@mobile displays"], "UIST18_paper779-Figure7-1.png": ["purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper839-Figure7-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper853-Figure7-1.png": ["purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Multi-users", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "device based@VR"], "UIST18_paper867-Figure3-1.png": ["purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@scaling", "continuous@scrolling", "continuous@translating", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper87-Figure11-1.png": ["purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers", "visual@lights", "Situation@Public indoors"], "UIST18_paper927-Figure6-1.png": ["purpose@design space", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "continuous@rotating", "Interaction type@mid-air interaction", "device based@VR", "tracking based@inside out", "visual@head mounted displays"], "UIST18_paper99-Figure15-1.png": ["purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "output modality@haptic"], "Ubicomp18_paper161-Figure2-1.png": ["purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "Ubicomp18_paper198-Figure3-1.png": ["Specific part@head", "purpose@design space", "time@still", "number@Solo-user", "continuous@translating", "continuous@rotating", "Interaction type@distal interaction"], "Ubicomp18_paper200-Figure7-1.png": ["purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects", "visual@computer displays"], "CHI18_paper107-Figure5-1.png": ["discrete@Symbolic gesture", "Situation@Desktop", "purpose@interaction sequence", "Interaction type@touch interaction", "Specific part@hand", "discrete@Pointing", "purpose@interactive system", "time@still", "activity@production"], "CHI18_paper11-Figure7-1.png": ["discrete@Key press", "Situation@Desktop", "purpose@interaction sequence", "audible@speech output", "Specific part@hand", "time@moving", "activity@data manipulation", "number@Solo-user", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays"], "CHI18_paper131-Figure1-1.png": ["Interaction type@touch interaction", "continuous@writing/drawing", "Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@pen interaction"], "CHI18_paper150-Figure16-1.png": ["Interaction type@mid-air interaction", "continuous@translating", "device based@VR", "Specific part@hand", "purpose@design space", "time@still", "number@Solo-user", "visual@head mounted displays"], "CHI18_paper165-Figure1-1.png": ["activity@driving", "body part@upper body", "device based@VR", "purpose@interactive system", "time@still", "number@Solo-user", "continuous@rotating", "Interaction type@controllers interaction", "device based@controllers", "Situation@Public/private transport", "visual@head mounted displays"], "CHI18_paper173-Figure1-1.png": ["Interaction type@mid-air interaction", "purpose@interaction sequence", "body part@upper body", "Interaction type@distal interaction", "Specific part@hand", "purpose@design space", "discrete@Pointing", "continuous@translating", "continuous@rotating", "time@moving", "activity@data manipulation", "number@Solo-user", "tracking based@inside out", "device based@mobile devices", "Situation@Private indoors"], "CHI18_paper179-Figure6-1.png": ["purpose@design space", "Specific part@hand", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects", "Interaction type@mid-air interaction", "visual@lights", "visual@mobile displays"], "CHI18_paper18-Figure6-1.png": ["purpose@design space", "discrete@Pointing", "continuous@translating", "Specific part@hand", "time@still", "activity@data manipulation", "number@Solo-user", "Interaction type@touch interaction", "Interaction type@distal interaction"], "CHI18_paper188-Figure4-1.png": ["activity@fabrication", "purpose@interaction sequence", "Specific part@hand", "time@moving"], "CHI18_paper19-Figure8-1.png": ["activity@data manipulation", "Interaction type@touch interaction", "Specific part@hand", "purpose@design space", "discrete@Pointing", "continuous@translating", "continuous@rotating", "time@still", "number@Solo-user", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper19-Figure9-1.png": ["Interaction type@touch interaction", "Specific part@hand", "purpose@design space", "continuous@translating", "continuous@rotating", "time@still", "activity@data manipulation", "number@Solo-user", "device based@mobile devices", "visual@mobile displays", "visual@large displays", "Interaction type@distal interaction"], "CHI18_paper199-Figure11-1.png": ["continuous@translating", "Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper199-Figure12-1.png": ["Interaction type@distal interaction", "purpose@interaction sequence", "Specific part@hand", "time@moving", "activity@2D/3D creation", "number@Solo-user", "discrete@Pointing", "continuous@writing/drawing", "device based@tangible objects", "Interaction type@tangible interaction", "device based@controllers", "Interaction type@controllers interaction", "visual@projected displays"], "CHI18_paper199-Figure14-1.png": ["continuous@translating", "Specific part@hand", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "Situation@Desktop"], "CHI18_paper199-Figure15-1.png": ["purpose@interaction sequence", "Specific part@hand", "time@moving", "activity@data manipulation", "number@Solo-user", "continuous@scaling", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "Situation@Desktop"], "CHI18_paper199-Figure6-1.png": ["purpose@interaction sequence", "Specific part@hand", "continuous@translating", "continuous@rotating", "time@moving", "activity@data manipulation", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper21-Figure1-1.png": ["discrete@Key press", "Interaction type@mid-air interaction", "Situation@Desktop", "continuous@rotating", "Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@computer interaction ", "visual@computer displays", "device based@laptop devices"], "CHI18_paper218-Figure3-1.png": ["Interaction type@mid-air interaction", "discrete@Symbolic gesture", "purpose@interaction sequence", "continuous@scaling", "Specific part@hand", "Interaction type@distal interaction", "body part@full body", "time@moving", "activity@data manipulation", "number@Solo-user"], "CHI18_paper218-Figure5-1.png": ["body part@upper body", "Interaction type@distal interaction", "continuous@translating", "purpose@interactive system", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Multi-users"], "CHI18_paper218-Figure6-1.png": ["Interaction type@mid-air interaction", "Specific part@head", "device based@VR", "Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "visual@head mounted displays"], "CHI18_paper223-Figure3-1.png": ["purpose@interaction sequence", "Interaction type@touch interaction", "continuous@writing/drawing", "Specific part@hand", "purpose@design space", "discrete@Pointing", "continuous@translating", "Interaction type@pen interaction", "time@still", "activity@production", "activity@2D/3D creation", "number@Solo-user"], "CHI18_paper223-Figure5-1.png": ["Interaction type@touch interaction", "continuous@writing/drawing", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@pen interaction", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "continuous@rotating", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper236-Figure2-1.png": ["discrete@Pointing", "Interaction type@on body interaction", "body part@full body", "Specific part@hand", "purpose@interactive system", "time@still", "activity@communication", "number@Solo-user", "sound based@speech input", "audible@speech output"], "CHI18_paper246-Figure3-1.png": ["activity@driving", "body part@upper body", "continuous@rotating", "Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@controllers interaction", "device based@controllers", "visual@large displays"], "CHI18_paper248-Figure7-1.png": ["Specific part@fingers", "continuous@scaling", "continuous@scrolling", "Interaction type@touch interaction", "purpose@design space", "discrete@Pointing", "continuous@translating", "time@still", "number@Solo-user", "activity@data manipulation", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper258-Figure1-1.png": ["Interaction type@mid-air interaction", "body part@upper body", "Specific part@hand", "Interaction type@distal interaction", "purpose@design space", "Interaction type@on body interaction", "continuous@translating", "time@moving", "discrete@Pointing", "device based@mobile devices", "tracking based@inside out", "visual@mobile displays", "output modality@haptic"], "CHI18_paper258-Figure2-1.png": ["purpose@design space", "Interaction type@on body interaction", "Interaction type@touch interaction", "Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "device based@mobile devices", "output modality@haptic"], "CHI18_paper281-Figure1-1.png": ["body part@upper body", "Interaction type@distal interaction", "purpose@design space", "time@still", "number@Solo-user", "tracking based@outside in", "device based@laptop devices", "visual@computer displays"], "CHI18_paper297-Figure2-1.png": ["Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper300-Figure4-1.png": ["discrete@Pointing", "Interaction type@gaze interaction", "body part@upper body", "purpose@design space", "time@still", "activity@communication", "number@Multi-users", "Situation@Private indoors"], "CHI18_paper336-Figure5-1.png": ["Specific part@fingers", "activity@fabrication", "purpose@interaction sequence", "Interaction type@on body interaction", "Specific part@foot", "time@moving", "continuous@deforming", "device based@tangible objects", "Interaction type@tangible interaction"], "CHI18_paper349-Figure3-1.png": ["discrete@Key press", "Situation@Desktop", "discrete@Pointing", "Interaction type@gaze interaction", "body part@upper body", "purpose@interactive system", "time@still", "number@Multi-users", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper354-Figure16-1.png": ["Interaction type@touch interaction", "Specific part@hand", "discrete@Pointing", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper367-Figure1-1.png": ["purpose@design space", "continuous@translating", "continuous@rotating", "Specific part@hand", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper367-Figure7-1.png": ["purpose@design space", "Situation@Desktop", "discrete@Pointing", "Interaction type@touch interaction", "continuous@rotating", "Specific part@hand", "time@still", "activity@data manipulation", "number@Solo-user", "Interaction type@controllers interaction", "device based@desktop devices", "device based@large surfaces", "device based@controllers", "visual@computer displays"], "CHI18_paper374-Figure5-1.png": ["purpose@design space", "continuous@deforming", "activity@fabrication", "Specific part@hand", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper397-Figure5-1.png": ["Interaction type@distal interaction", "discrete@Pointing", "body part@upper body", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "tracking based@outside in", "visual@mobile displays"], "CHI18_paper4-Figure3-1.png": ["discrete@Pointing", "Interaction type@touch interaction", "Specific part@hand", "purpose@interactive system", "time@still", "number@Solo-user", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper407-Figure1-1.png": ["purpose@interaction sequence", "body part@upper body", "Interaction type@touch interaction", "Specific part@hand", "purpose@design space", "discrete@Pointing", "time@moving", "activity@data manipulation", "number@Multi-users", "device based@mobile devices", "visual@mobile displays", "device based@tangible objects", "Interaction type@controllers interaction"], "CHI18_paper46-Figure3-1.png": ["device based@VR", "time@still", "visual@head mounted displays", "device based@AR", "tracking based@outside in", "purpose@design space", "Interaction type@distal interaction", "Situation@Private indoors", "number@Multi-users", "body part@upper body", "body part@full body"], "CHI18_paper86-Figure9-1.png": ["device based@VR", "Specific part@hand", "time@still", "visual@head mounted displays", "Interaction type@touch interaction", "number@Solo-user", "continuous@translating", "purpose@design space", "discrete@Pointing"], "CHI18_paper638-Figure3-1.png": ["visual@computer displays", "device based@desktop devices", "continuous@translating", "Interaction type@computer interaction ", "activity@data manipulation", "time@moving", "purpose@interaction sequence", "discrete@Pointing", "number@Solo-user"], "CSCW18_paper140-Figure2-1.png": ["Specific part@hand", "time@still", "visual@mobile displays", "discrete@Pointing", "purpose@interactive system", "activity@data manipulation", "Interaction type@touch interaction", "continuous@scrolling", "number@Multi-users", "device based@mobile devices"], "CHI18_paper89-Figure6-1.png": ["purpose@interactive system", "number@Solo-user", "time@still", "body part@upper body"], "CHI18_paper46-Figure2-1.png": ["Specific part@hand", "visual@head mounted displays", "device based@AR", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "Interaction type@gaze interaction", "Situation@Private indoors", "discrete@Pointing", "purpose@interaction sequence", "time@moving", "activity@communication", "number@Solo-user"], "CHI18_paper579-Figure4-1.png": ["Specific part@hand", "visual@head mounted displays", "activity@2D/3D creation", "device based@AR", "tracking based@outside in", "body part@upper body", "continuous@translating", "device based@controllers", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "number@Solo-user", "Interaction type@controllers interaction", "continuous@rotating", "time@moving", "purpose@interaction sequence", "discrete@Pointing", "activity@fabrication"], "CHI18_paper99-Figure3-1.png": ["device based@VR", "time@still", "visual@head mounted displays", "purpose@interactive system", "body part@full body", "tracking based@outside in", "activity@data manipulation", "number@Solo-user", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@controllers"], "UIST18_paper913-Figure8-1.png": ["time@still", "Interaction type@on body interaction", "purpose@interactive system", "number@Solo-user", "Interaction type@touch interaction", "device based@controllers", "body part@upper body", "output modality@haptic"], "UIST18_paper473-Figure3-1.png": ["time@still", "Specific part@hand", "device based@tangible objects", "purpose@design space", "Interaction type@tangible interaction", "activity@communication", "number@Multi-users", "tracking based@inside out", "audible@speech output", "discrete@Pointing"], "CHI18_paper465-Figure5-1.png": ["Specific part@hand", "time@still", "purpose@interactive system", "device based@tangible objects", "number@Solo-user", "Interaction type@tangible interaction", "activity@medical", "activity@fabrication", "tracking based@outside in"], "CHI18_paper43-Figure1-1.png": ["time@still", "visual@mobile displays", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@mobile devices", "purpose@design space", "activity@data manipulation", "number@Solo-user", "body part@upper body", "Specific part@hand", "Interaction type@mid-air interaction"], "CHI18_paper446-Figure18-1.png": ["Specific part@hand", "time@still", "visual@mobile displays", "visual@head mounted displays", "purpose@interactive system", "device based@tangible objects", "body part@upper body", "output modality@haptic", "number@Solo-user", "activity@entertainment", "Interaction type@tangible interaction", "tracking based@inside out", "Situation@Public indoors", "device based@AR", "Interaction type@on body interaction", "continuous@rotating", "Interaction type@mid-air interaction"], "UIST18_paper335-Figure10-1.png": ["Specific part@hand", "visual@mobile displays", "continuous@translating", "activity@data manipulation", "number@Solo-user", "continuous@scaling", "tracking based@inside out", "purpose@interaction sequence", "discrete@Pointing", "Interaction type@touch interaction", "continuous@rotating", "time@moving", "device based@mobile devices", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "UIST18_paper261-Figure4-1.png": ["time@still", "activity@data manipulation", "device based@tangible objects", "Specific part@fingers", "purpose@design space", "number@Multi-users", "Interaction type@tangible interaction", "discrete@Pointing"], "UIST18_paper853-Figure9-1.png": ["device based@VR", "visual@head mounted displays", "body part@upper body", "Interaction type@distal interaction", "discrete@Pointing", "purpose@design space", "activity@data manipulation", "time@still", "number@Multi-users"], "UIST18_paper737-Figure8-1.png": ["time@still", "activity@data manipulation", "number@Solo-user", "device based@tangible objects", "Specific part@fingers", "continuous@rotating", "purpose@design space", "continuous@translating", "Interaction type@tangible interaction", "discrete@Key press", "output modality@haptic"], "Ubicomp18_paper201-Figure1-1.png": ["visual@computer displays", "device based@controllers", "Interaction type@distal interaction", "number@Solo-user", "Interaction type@controllers interaction", "time@moving", "activity@communication", "purpose@interaction sequence", "Specific part@hand"], "UIST18_paper745-Figure4-1.png": ["time@still", "purpose@interactive system", "Interaction type@touch interaction", "Specific part@fingers", "number@Solo-user", "discrete@Pointing"], "CHI18_paper446-Figure8-1.png": ["visual@head mounted displays", "device based@tangible objects", "body part@upper body", "activity@data manipulation", "number@Solo-user", "Situation@Private indoors", "purpose@interaction sequence", "tracking based@inside out", "Interaction type@tangible interaction", "device based@AR", "continuous@rotating", "time@moving"], "CHI18_paper65-Figure2-1.png": ["Specific part@hand", "tracking based@outside in", "continuous@translating", "device based@controllers", "number@Solo-user", "Interaction type@controllers interaction", "continuous@rotating", "time@moving", "purpose@interaction sequence", "discrete@Pointing", "purpose@design space", "activity@data manipulation", "Interaction type@distal interaction", "output modality@haptic", "visual@head mounted displays", "device based@VR"], "CHI18_paper446-Figure2-1.png": ["visual@head mounted displays", "device based@tangible objects", "body part@upper body", "output modality@haptic", "activity@data manipulation", "number@Solo-user", "Situation@Private indoors", "Interaction type@tangible interaction", "purpose@interaction sequence", "tracking based@inside out", "device based@AR", "continuous@rotating", "visual@lights", "time@moving"], "CHI18_paper655-Figure2-1.png": ["time@still", "Specific part@hand", "purpose@interactive system", "device based@tangible objects", "number@Solo-user", "continuous@rotating", "Interaction type@tangible interaction"], "UIST18_paper141-Figure10-1.png": ["Specific part@hand", "time@still", "continuous@deforming", "device based@tangible objects", "purpose@design space", "activity@fabrication", "number@Solo-user", "continuous@rotating", "Interaction type@tangible interaction"], "CHI18_paper603-Figure6-1.png": ["Specific part@hand", "purpose@interactive system", "visual@large displays", "number@Solo-user", "Interaction type@pen interaction", "discrete@Pointing", "time@still", "device based@large surfaces", "device based@controllers"], "UIST18_paper485-Figure5-1.png": ["Specific part@hand", "time@still", "continuous@deforming", "device based@tangible objects", "purpose@design space", "number@Solo-user", "discrete@Symbolic gesture", "Interaction type@tangible interaction", "Interaction type@mid-air interaction"], "UIST18_paper867-Figure4-1.png": ["device based@VR", "time@still", "Specific part@hand", "visual@head mounted displays", "purpose@interactive system", "activity@medical", "Interaction type@mid-air interaction", "number@Solo-user", "discrete@Symbolic gesture"], "CHI18_paper76-Figure1-1.png": ["time@still", "visual@mobile displays", "purpose@interactive system", "Interaction type@touch interaction", "body part@upper body", "number@Solo-user", "device based@mobile devices", "Situation@Desktop"], "Ubicomp18_paper201-Figure9-1.png": ["Specific part@hand", "time@still", "activity@production", "device based@tangible objects", "visual@large displays", "number@Solo-user", "Situation@Private indoors", "Interaction type@tangible interaction", "discrete@Pointing", "Interaction type@mid-air interaction", "visual@lights", "Interaction type@pen interaction", "discrete@Symbolic gesture", "device based@mobile devices", "purpose@design space", "visual@mobile displays"], "UIST18_paper5-Figure2-1.png": ["Specific part@hand", "time@still", "continuous@deforming", "device based@tangible objects", "purpose@design space", "continuous@translating", "activity@data manipulation", "number@Solo-user", "Interaction type@tangible interaction"], "UIST18_paper19-Figure2-1.png": ["visual@mobile displays", "purpose@interactive system", "device based@AR", "body part@upper body", "continuous@translating", "Interaction type@distal interaction", "number@Multi-users", "time@moving", "device based@mobile devices"], "CHI18_paper42-Figure9-1.png": ["device based@VR", "time@still", "visual@head mounted displays", "purpose@interactive system", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "discrete@Pointing"], "UIST18_paper19-Figure9-1.png": ["time@still", "visual@mobile displays", "visual@head mounted displays", "purpose@interactive system", "body part@full body", "number@Solo-user", "Situation@Private indoors", "discrete@Pointing", "device based@AR", "Interaction type@mid-air interaction", "continuous@rotating", "device based@mobile devices"], "CHI18_paper599-Figure8-1.png": ["time@still", "visual@head mounted displays", "body part@full body", "tracking based@outside in", "device based@controllers", "visual@large displays", "activity@entertainment", "number@Multi-users", "discrete@Pointing", "device based@VR", "purpose@design space", "Interaction type@mid-air interaction", "device based@large surfaces", "Interaction type@distal interaction", "Interaction type@controllers interaction", "discrete@Symbolic gesture"], "UIST18_paper19-Figure12-1.png": ["visual@mobile displays", "body part@full body", "activity@entertainment", "number@Multi-users", "purpose@interaction sequence", "discrete@Pointing", "Situation@Public indoors", "Interaction type@touch interaction", "device based@AR", "time@moving", "device based@mobile devices", "Interaction type@distal interaction"], "UIST18_paper757-Figure1-1.png": ["Specific part@hand", "time@still", "continuous@deforming", "purpose@interactive system", "device based@tangible objects", "number@Solo-user", "Interaction type@tangible interaction"], "CHI18_paper89-Figure3-1.png": ["Specific part@hand", "time@still", "visual@head mounted displays", "purpose@interactive system", "body part@full body", "device based@tangible objects", "number@Solo-user", "Situation@Private indoors", "activity@entertainment", "tracking based@inside out", "Interaction type@tangible interaction", "device based@VR", "purpose@design space", "Interaction type@mid-air interaction", "Interaction type@controllers interaction", "discrete@Symbolic gesture", "tracking based@outside in"], "UIST18_paper637-Figure6-1.png": ["time@still", "purpose@interactive system", "activity@production", "activity@2D/3D creation", "visual@computer displays", "device based@desktop devices", "continuous@translating", "Interaction type@computer interaction ", "continuous@rotating", "discrete@Pointing"]}, "how_codes_coding_dict": {"CHI18_paper107-Figure2-1.png": ["point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "lines and arrows@trajectories"], "UIST18_paper45-Figure4-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters", "sub-framing@UI overlay"], "CHI18_paper184-Figure10-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors"], "Ubicomp18_paper164-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@inset (PiP)", "type@photo", "hue@colors", "dynamic@Contact shapes"], "CHI18_paper218-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "line style@width", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper558-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "UI@rendering", "type@photo", "hue@colors"], "CHI18_paper477-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@magnification lens", "sub-framing@UI embedded", "UI@rendering", "realism@simplistic", "hue@colors", "type@clipart/icon", "region@color area", "lines and arrows@transfer", "grouping and linking@text annotation", "grouping and linking@identifiers grouping"], "CHI18_paper202-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "identifiers@title", "identifiers@letters"], "CHI18_paper437-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "identifiers@numbers"], "CHI18_paper411-Figure3-1.png": ["point of view@1st person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "hue@colors", "lines and arrows@trajectories", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper18-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@clipart/icon", "hue@colors", "lines and arrows@projection"], "UIST18_paper19-Figure10-1.png": ["point of view@1st person", "point of view@top", "number of frames@multi frames", "sub-framing@UI overlay", "realism@simplistic", "type@photo", "color@transparency", "hue@colors", "line style@dashed", "line style@width", "region@color area", "element@color highlight", "grouping and linking@color grouping", "effects@stroboscopic", "lines and arrows@trajectories", "identifiers@letters"], "UIST18_paper335-Figure7-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction"], "CHI18_paper198-Figure9-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "UIST18_paper87-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "UIST18_paper711-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "type@photo", "hue@colors", "measure@Arrows", "measure@Text indicator", "grouping and linking@text annotation"], "CHI18_paper234-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "UI@rendering", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "grouping and linking@color grouping"], "CHI18_paper132-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "type@text", "hue@colors", "grouping and linking@text annotation"], "UIST18_paper745-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "Ubicomp18_paper162-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@title", "type@text", "identifiers@letters"], "UIST18_paper913-Figure12-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters"], "CHI18_paper19-Figure2-1.png": ["number of frames@multi frames", "realism@simplistic", "hue@colors", "identifiers@letters", "grouping and linking@color grouping", "lines and arrows@direction", "point of view@1st person"], "CHI18_paper529-Figure6-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "color@transparency", "hue@colors", "realism@simplistic", "grouping and linking@color grouping", "identifiers@letters", "lines and arrows@transfer"], "CSCW18_paper151-Figure10-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@monochrome", "sub-framing@UI embedded"], "CHI18_paper210-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@realistic", "hue@monochrome", "identifiers@title", "type@text"], "CHI18_paper248-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "UI@rendering", "type@photo", "color@transparency", "hue@colors", "dynamic@Contact shapes", "element@color highlight", "identifiers@letters"], "CHI18_paper529-Figure12-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper634-Figure12-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@photo", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction", "lines and arrows@transfer", "dynamic@Contact shapes", "identifiers@numbers", "identifiers@letters"], "CHI18_paper429-Figure1-1.png": ["type@clipart/icon", "realism@simplistic", "point of view@3rd person", "number of frames@one frame", "sub-framing@magnification lens", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper362-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper360-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@text", "identifiers@letters"], "CHI18_paper251-Figure1-1.png": ["point of view@top", "point of view@UI only", "number of frames@one frame", "sub-framing@UI embedded", "sub-framing@magnification lens", "type@photo", "realism@simplistic", "hue@colors", "type@text"], "CHI18_paper150-Figure23-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "UI@rendering", "point of view@UI only", "effects@stroboscopic", "lines and arrows@direction"], "UIST18_paper825-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "region@color area", "type@text", "grouping and linking@color grouping", "dynamic@Contact shapes", "identifiers@letters", "identifiers@title", "grouping and linking@text annotation"], "UIST18_paper5-Figure10-1.png": ["point of view@1st person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters", "point of view@UI only"], "CHI18_paper515-Figure7-1.png": ["point of view@1st person", "point of view@top", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "hue@colors"], "CHI18_paper539-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper291-Figure2-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "grouping and linking@color grouping", "identifiers@letters", "lines and arrows@direction", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "effects@stroboscopic"], "UIST18_paper485-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "identifiers@title"], "CHI18_paper653-Figure5-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@rendering", "type@photo", "type@text", "hue@colors", "identifiers@title", "identifiers@letters", "point of view@UI only"], "CHI18_paper634-Figure9-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "type@photo", "hue@colors", "lines and arrows@direction", "dynamic@Contact shapes", "identifiers@letters", "identifiers@numbers"], "CHI18_paper446-Figure10-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "color@transparency", "line style@width", "line style@dashed", "lines and arrows@direction", "effects@waves", "identifiers@letters"], "Ubicomp18_paper170-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CSCW18_paper128-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "effects@waves", "point of view@overshoulder 3/4"], "CHI18_paper529-Figure8-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "color@transparency", "identifiers@letters", "lines and arrows@direction", "dynamic@Contact shapes", "element@color highlight"], "CHI18_paper160-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper81-Figure1-1.png": ["point of view@overshoulder 3/4", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "realism@simplistic", "type@data visualization", "type@photo", "UI@drawing", "hue@colors", "enclosing@circle/rectangle", "grouping and linking@identifiers grouping", "type@text", "lines and arrows@direction", "identifiers@letters"], "UIST18_paper839-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "effects@stroboscopic", "identifiers@letters", "point of view@1st person", "point of view@UI only"], "UIST18_paper53-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper320-Figure11-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "point of view@UI only"], "UIST18_paper825-Figure1-1.png": ["point of view@top", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "enclosing@circle/rectangle", "element@color highlight", "lines and arrows@direction", "line style@dashed", "identifiers@letters", "region@color area"], "CHI18_paper613-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper89-Figure14-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "line style@width", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper76-Figure3-1.png": ["number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "anonymization@blur", "point of view@3rd person"], "CHI18_paper86-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "lines and arrows@direction", "lines and arrows@projection", "dynamic@Contact shapes", "point of view@UI only"], "CHI18_paper450-Figure10-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper202-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "enclosing@circle/rectangle", "lines and arrows@direction", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper513-Figure9-1.png": ["point of view@overshoulder 3/4", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper368-Figure7-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@data visualization", "hue@colors", "color@transparency", "grouping and linking@color grouping", "lines and arrows@direction", "effects@stroboscopic", "effects@waves", "type@clipart/icon", "type@text", "grouping and linking@text annotation"], "UIST18_paper675-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@trajectories"], "CHI18_paper46-Figure4-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters", "point of view@3rd person"], "CHI18_paper218-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "enclosing@exact contour line", "identifiers@letters", "grouping and linking@text annotation"], "CHI18_paper42-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "UI@rendering", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "lines and arrows@trajectories", "lines and arrows@projection", "effects@stroboscopic", "grouping and linking@identifiers grouping"], "CHI18_paper185-Figure11-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "lines and arrows@trajectories", "identifiers@letters"], "UIST18_paper365-Figure5-1.png": ["point of view@1st person", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@magnification lens", "type@photo", "type@clipart/icon", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "effects@waves", "identifiers@letters", "grouping and linking@text annotation"], "Ubicomp18_paper184-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper132-Figure1-1.png": ["point of view@3rd person", "point of view@top", "point of view@UI only", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "line style@width", "region@color area", "dynamic@Contact shapes", "element@color highlight", "lines and arrows@transfer", "effects@waves", "effects@stroboscopic", "type@data visualization", "identifiers@letters", "identifiers@title", "sub-framing@juxtaposition"], "CHI18_paper378-Figure3-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@one frame", "type@clipart/icon", "type@data visualization", "realism@simplistic", "hue@monochrome", "dynamic@Contact shapes"], "CHI18_paper288-Figure1-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "type@photo", "sub-framing@UI embedded", "hue@colors", "identifiers@letters"], "CHI18_paper310-Figure1-1.png": ["sub-framing@juxtaposition", "point of view@3rd person", "number of frames@one frame", "sub-framing@magnification lens", "type@photo", "background@removed", "hue@colors", "color@transparency", "line style@dashed", "enclosing@circle/rectangle", "effects@stroboscopic", "type@text", "lines and arrows@direction", "identifiers@title"], "CHI18_paper27-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "lines and arrows@direction", "dynamic@Contact shapes", "identifiers@letters"], "CHI18_paper185-Figure7-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters"], "CHI18_paper497-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "identifiers@title"], "UIST18_paper335-Figure3-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper188-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@data visualization", "hue@colors", "identifiers@letters", "type@text", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper177-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "type@text", "hue@colors", "grouping and linking@text annotation"], "CHI18_paper21-Figure2-1.png": ["point of view@top", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "grouping and linking@color grouping", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper547-Figure2-1.png": ["point of view@1st person", "point of view@3rd person", "point of view@overshoulder 3/4", "point of view@top", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "region@color area", "line style@dashed", "lines and arrows@projection", "lines and arrows@direction", "grouping and linking@color grouping", "dynamic@Contact shapes", "effects@waves", "effects@stroboscopic", "element@color highlight", "identifiers@letters", "grouping and linking@text annotation"], "UIST18_paper737-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "line style@dashed", "lines and arrows@direction"], "UIST18_paper557-Figure1-1.png": ["point of view@1st person", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@juxtaposition", "type@photo", "hue@colors"], "CHI18_paper579-Figure9-1.png": ["point of view@1st person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "hue@colors", "color@transparency", "lines and arrows@projection", "identifiers@letters"], "UIST18_paper321-Figure12-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper436-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@drawing", "realism@simplistic", "hue@grayscale", "line style@dashed", "lines and arrows@projection", "dynamic@Contact shapes", "identifiers@letters"], "CHI18_paper89-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "line style@width", "effects@waves", "effects@stroboscopic", "hue@grayscale", "lines and arrows@trajectories", "identifiers@letters"], "CHI18_paper241-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@grayscale", "line style@dashed", "line style@width", "lines and arrows@direction"], "Ubicomp18_paper162-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "type@text", "hue@colors", "identifiers@letters", "identifiers@title"], "CHI18_paper634-Figure11-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "dynamic@Contact shapes", "region@color area", "realism@realistic", "identifiers@letters", "identifiers@numbers"], "CSCW18_paper192-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper340-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "point of view@3rd person", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "hue@colors", "identifiers@title", "identifiers@letters", "point of view@UI only"], "CHI18_paper629-Figure1-1.png": ["hue@colors", "type@photo", "number of frames@one frame", "point of view@3rd person"], "CHI18_paper569-Figure27-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "lines and arrows@direction", "identifiers@letters", "effects@stroboscopic"], "CHI18_paper89-Figure15-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "line style@width", "hue@monochrome", "effects@waves", "lines and arrows@trajectories", "line style@dashed"], "CHI18_paper529-Figure3-1.png": ["point of view@3rd person", "point of view@1st person", "sub-framing@UI embedded", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper241-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "UI@drawing", "type@photo", "region@color area", "hue@colors", "identifiers@letters"], "UIST18_paper675-Figure4-1.png": ["point of view@1st person", "sub-framing@UI embedded", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper401-Figure3-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "UIST18_paper853-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "line style@dashed", "color@transparency", "lines and arrows@transfer", "effects@stroboscopic", "lines and arrows@trajectories", "lines and arrows@projection", "element@color highlight", "identifiers@letters"], "CHI18_paper98-Figure4-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "hue@colors"], "Ubicomp18_paper174-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "line style@dashed", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@letters", "point of view@UI only"], "CHI18_paper86-Figure12-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "hue@colors", "type@text", "identifiers@title", "point of view@UI only", "grouping and linking@text annotation"], "CHI18_paper219-Figure2-1.png": ["point of view@top", "number of frames@one frame", "type@text", "UI@drawing", "hue@colors", "grouping and linking@color grouping", "line style@dashed", "lines and arrows@trajectories", "grouping and linking@identifiers grouping"], "CHI18_paper54-Figure5-1.png": ["point of view@1st person", "sub-framing@magnification lens", "sub-framing@UI embedded", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "line style@width", "enclosing@circle/rectangle", "background@removed", "identifiers@letters", "type@text", "point of view@3rd person", "point of view@UI only", "grouping and linking@text annotation"], "CHI18_paper558-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "element@color highlight", "grouping and linking@identifiers grouping", "grouping and linking@color grouping", "line style@dashed", "measure@Arrows"], "CHI18_paper251-Figure2-1.png": ["point of view@UI only", "type@text", "UI@rendering", "type@clipart/icon", "hue@colors", "dynamic@Contact shapes", "lines and arrows@direction", "number of frames@one frame", "grouping and linking@text annotation"], "UIST18_paper867-Figure2-1.png": ["point of view@1st person", "number of frames@one frame", "UI@rendering", "hue@colors", "point of view@UI only"], "CHI18_paper380-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@trajectories", "lines and arrows@projection"], "UIST18_paper5-Figure8-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper521-Figure1-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors"], "CHI18_paper90-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "sub-framing@UI overlay", "UI@rendering", "type@photo", "color@transparency", "hue@colors", "background@removed"], "CHI18_paper173-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper363-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "line style@width", "enclosing@exact contour line", "element@color highlight", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper529-Figure11-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "dynamic@Contact shapes", "lines and arrows@trajectories", "element@color highlight", "identifiers@letters"], "UIST18_paper321-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction", "identifiers@letters"], "UIST18_paper697-Figure3-1.png": ["type@clipart/icon", "realism@simplistic", "hue@colors", "point of view@3rd person", "element@color highlight", "lines and arrows@direction", "identifiers@letters", "region@color area", "number of frames@multi frames"], "CHI18_paper613-Figure1-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@magnification lens", "realism@simplistic", "type@photo", "hue@colors", "element@color highlight", "enclosing@circle/rectangle", "line style@dashed"], "UIST18_paper745-Figure2-1.png": ["type@data visualization", "realism@simplistic", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@letters", "type@text", "grouping and linking@identifiers grouping", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper151-Figure4-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper529-Figure13-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper199-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "line style@dashed", "element@color highlight", "type@text", "grouping and linking@color grouping", "measure@Arrows", "lines and arrows@direction", "identifiers@letters", "grouping and linking@text annotation"], "CHI18_paper219-Figure14-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "type@photo", "UI@rendering", "color@transparency", "hue@colors", "enclosing@circle/rectangle", "identifiers@letters", "lines and arrows@direction", "element@color highlight", "point of view@UI only"], "CHI18_paper610-Figure1-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@clipart/icon", "hue@colors", "color@transparency", "effects@stroboscopic"], "CHI18_paper593-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper380-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "realism@simplistic", "type@photo", "hue@colors", "region@color area", "element@color highlight", "enclosing@exact contour line", "identifiers@letters", "identifiers@numbers", "identifiers@title"], "CHI18_paper291-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "identifiers@letters", "lines and arrows@direction"], "CHI18_paper362-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper199-Figure13-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper436-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@grayscale", "background@removed", "lines and arrows@transfer", "type@data visualization", "lines and arrows@projection", "line style@dashed"], "UIST18_paper19-Figure15-1.png": ["point of view@1st person", "point of view@UI only", "sub-framing@UI overlay", "number of frames@multi frames", "point of view@3rd person", "realism@simplistic", "type@photo", "UI@rendering", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "element@color highlight", "identifiers@letters"], "CHI18_paper634-Figure10-1.png": ["point of view@1st person", "sub-framing@UI embedded", "point of view@3rd person", "realism@realistic", "type@photo", "number of frames@multi frames", "hue@colors", "region@color area", "dynamic@Contact shapes", "lines and arrows@direction"], "CHI18_paper89-Figure8-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "point of view@UI only"], "CHI18_paper539-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "identifiers@title"], "CHI18_paper634-Figure3-1.png": ["point of view@top", "realism@realistic", "UI@drawing", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "element@color highlight", "dynamic@Contact shapes", "lines and arrows@transfer", "lines and arrows@trajectories"], "UIST18_paper499-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "color@transparency", "region@color area", "enclosing@circle/rectangle", "line style@width", "grouping and linking@text annotation"], "CHI18_paper11-Figure5-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@text", "type@clipart/icon", "hue@grayscale", "line style@width", "element@color highlight", "dynamic@Contact shapes", "identifiers@title"], "UIST18_paper913-Figure10-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper564-Figure1-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@one frame", "sub-framing@inset (PiP)", "sub-framing@magnification lens", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "type@text", "region@color area", "line style@width", "element@color highlight", "enclosing@circle/rectangle", "dynamic@Contact shapes", "identifiers@letters", "lines and arrows@transfer", "grouping and linking@color grouping"], "CHI18_paper567-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper579-Figure8-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "color@transparency", "lines and arrows@projection", "region@color area", "point of view@UI only"], "UIST18_paper127-Figure16-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper150-Figure3-1.png": ["point of view@top", "number of frames@multi frames", "realism@simplistic", "type@photo", "background@removed", "hue@colors", "color@transparency", "type@text", "lines and arrows@trajectories", "effects@stroboscopic", "element@color highlight", "identifiers@letters", "identifiers@title"], "CHI18_paper439-Figure6-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper241-Figure4-1.png": ["line style@width", "point of view@3rd person", "number of frames@one frame", "line style@dashed", "enclosing@exact contour line", "realism@simplistic", "hue@grayscale", "lines and arrows@direction"], "CHI18_paper339-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "identifiers@title"], "UIST18_paper927-Figure5-1.png": ["sub-framing@juxtaposition", "point of view@1st person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "lines and arrows@direction", "point of view@3rd person", "point of view@UI only"], "CHI18_paper45-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI overlay", "type@text", "hue@colors", "element@color highlight"], "UIST18_paper499-Figure9-1.png": ["point of view@1st person", "point of view@3rd person", "sub-framing@juxtaposition", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "region@color area", "enclosing@exact contour line", "background@grayed out/blurred", "identifiers@letters", "point of view@UI only"], "CHI18_paper42-Figure1-1.png": ["point of view@3rd person", "point of view@top", "number of frames@one frame", "sub-framing@inset (PiP)", "realism@simplistic", "sub-framing@UI embedded", "UI@drawing", "type@clipart/icon", "hue@colors", "region@color area", "element@color highlight", "measure@Arrows", "measure@Text indicator"], "CHI18_paper638-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@text", "type@clipart/icon", "hue@colors", "enclosing@exact contour line", "enclosing@circle/rectangle", "element@color highlight", "identifiers@title", "identifiers@letters"], "CHI18_paper622-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper347-Figure3-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@inset (PiP)", "realism@simplistic", "UI@rendering", "type@text", "hue@colors", "region@color area", "dynamic@Contact shapes", "identifiers@letters", "identifiers@title"], "CHI18_paper18-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "line style@dashed"], "CSCW18_paper185-Figure4-1.png": ["point of view@top", "sub-framing@UI embedded", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper209-Figure1-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "point of view@UI only"], "CHI18_paper628-Figure6-1.png": ["point of view@top", "number of frames@one frame", "type@text", "realism@simplistic", "type@data visualization", "hue@colors", "element@color highlight", "lines and arrows@transfer", "grouping and linking@color grouping", "dynamic@Contact shapes", "region@color area", "line style@dashed", "identifiers@title"], "CHI18_paper248-Figure6-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "type@text", "hue@colors", "color@transparency", "element@color highlight", "identifiers@letters", "grouping and linking@text annotation"], "CHI18_paper5-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper245-Figure12-1.png": ["number of frames@multi frames", "type@photo", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "lines and arrows@projection", "dynamic@Contact shapes", "identifiers@title", "identifiers@letters", "type@text", "point of view@1st person", "sub-framing@UI embedded"], "UIST18_paper275-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "effects@stroboscopic"], "CHI18_paper289-Figure1-1.png": ["anonymization@blur", "point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "point of view@UI only"], "UIST18_paper839-Figure8-1.png": ["point of view@1st person", "UI@rendering", "point of view@UI only", "number of frames@multi frames", "hue@colors", "identifiers@letters"], "CHI18_paper385-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@clipart/icon", "lines and arrows@transfer", "region@color area", "identifiers@title", "type@text", "effects@waves", "grouping and linking@identifiers grouping"], "CHI18_paper291-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@direction", "lines and arrows@trajectories", "grouping and linking@color grouping", "line style@dashed"], "CHI18_paper298-Figure3-1.png": ["point of view@1st person", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors"], "CHI18_paper638-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "type@clipart/icon", "hue@colors", "line style@dashed", "element@color highlight", "enclosing@circle/rectangle", "identifiers@numbers", "effects@waves", "effects@stroboscopic", "type@text", "type@data visualization", "UI@rendering"], "CHI18_paper164-Figure1-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "type@text", "hue@colors", "identifiers@letters", "identifiers@title"], "UIST18_paper877-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@monochrome", "identifiers@letters", "realism@simplistic"], "CHI18_paper334-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "identifiers@title", "realism@simplistic", "hue@colors", "enclosing@exact contour line", "effects@stroboscopic", "line style@dashed", "lines and arrows@direction", "lines and arrows@trajectories", "identifiers@letters"], "CHI18_paper81-Figure10-1.png": ["point of view@UI only", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "identifiers@letters"], "CHI18_paper189-Figure11-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper589-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@monochrome", "type@text", "line style@width", "lines and arrows@direction"], "CHI18_paper82-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@realistic", "UI@rendering", "hue@colors", "identifiers@numbers"], "UIST18_paper499-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "color@transparency", "region@color area", "identifiers@letters"], "CSCW18_paper192-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "UIST18_paper913-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper372-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper123-Figure8-1.png": ["point of view@UI only", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "region@color area", "type@data visualization"], "UIST18_paper765-Figure1-1.png": ["point of view@overshoulder 3/4", "sub-framing@UI embedded", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@text", "hue@colors", "element@color highlight"], "CHI18_paper336-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "lines and arrows@direction"], "UIST18_paper745-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper499-Figure15-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "region@color area", "identifiers@letters"], "CHI18_paper241-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper89-Figure16-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper558-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "line style@dashed", "hue@colors", "element@color highlight", "lines and arrows@trajectories"], "CHI18_paper87-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "hue@colors", "element@color highlight", "enclosing@circle/rectangle", "lines and arrows@trajectories"], "CHI18_paper245-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "lines and arrows@direction", "region@color area", "element@color highlight", "hue@colors"], "CHI18_paper150-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "point of view@UI only", "UI@rendering", "hue@colors", "type@photo"], "CHI18_paper644-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "color@transparency", "hue@colors", "sub-framing@UI overlay", "UI@rendering"], "UIST18_paper499-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "line style@width", "hue@colors", "color@transparency", "region@color area"], "CHI18_paper654-Figure5-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "lines and arrows@projection", "identifiers@letters"], "UIST18_paper867-Figure5-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@one frame", "UI@rendering", "hue@colors", "lines and arrows@direction"], "CHI18_paper374-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "UI@rendering", "type@text", "hue@grayscale", "identifiers@letters", "identifiers@title"], "CHI18_paper336-Figure14-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@data visualization", "type@photo", "hue@colors", "realism@simplistic", "identifiers@letters"], "CHI18_paper19-Figure3-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "type@text", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "enclosing@circle/rectangle", "identifiers@letters", "grouping and linking@text annotation"], "UIST18_paper913-Figure11-1.png": ["point of view@3rd person", "sub-framing@UI embedded", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper224-Figure4-1.png": ["point of view@UI only", "number of frames@one frame", "color@transparency", "hue@colors", "line style@dashed", "lines and arrows@transfer", "UI@rendering", "dynamic@Contact shapes", "lines and arrows@trajectories"], "UIST18_paper927-Figure7-1.png": ["sub-framing@inset (PiP)", "sub-framing@UI embedded", "point of view@1st person", "point of view@top", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "lines and arrows@direction"], "CHI18_paper78-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper411-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "effects@waves", "lines and arrows@trajectories"], "CHI18_paper5-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper477-Figure3-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@rendering", "type@photo", "hue@colors", "type@clipart/icon", "region@color area", "element@color highlight", "lines and arrows@transfer", "grouping and linking@identifiers grouping", "type@text", "grouping and linking@text annotation"], "CHI18_paper150-Figure20-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "enclosing@circle/rectangle"], "UIST18_paper825-Figure9-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "sub-framing@UI embedded", "type@photo", "hue@colors", "region@color area", "identifiers@letters"], "CHI18_paper201-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "type@text", "realism@simplistic"], "UIST18_paper19-Figure6-1.png": ["point of view@top", "number of frames@one frame", "realism@simplistic", "grouping and linking@color grouping", "region@color area", "line style@dashed", "measure@Text indicator", "measure@Arrows", "type@text", "hue@colors", "grouping and linking@text annotation", "grouping and linking@identifiers grouping"], "CHI18_paper65-Figure4-1.png": ["type@data visualization", "realism@simplistic", "point of view@top", "number of frames@multi frames", "type@text", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@letters", "identifiers@title"], "CHI18_paper86-Figure13-1.png": ["point of view@UI only", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "type@text", "hue@colors", "dynamic@Contact shapes", "region@color area", "grouping and linking@text annotation"], "CHI18_paper354-Figure18-1.png": ["sub-framing@inset (PiP)", "sub-framing@UI embedded", "number of frames@one frame", "point of view@3rd person", "type@photo", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "identifiers@letters", "identifiers@title", "type@text", "background@removed", "grouping and linking@identifiers grouping", "grouping and linking@text annotation"], "CHI18_paper5-Figure9-1.png": ["point of view@3rd person", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper547-Figure1-1.png": ["point of view@1st person", "point of view@3rd person", "point of view@top", "number of frames@multi frames", "realism@simplistic", "type@photo", "hue@colors", "color@transparency", "hue@monochrome", "hue@grayscale", "line style@dashed", "region@color area", "element@color highlight", "lines and arrows@direction", "effects@stroboscopic", "lines and arrows@trajectories", "dynamic@Contact shapes", "lines and arrows@projection", "type@text", "grouping and linking@text annotation"], "CHI18_paper298-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "grouping and linking@color grouping"], "CHI18_paper603-Figure17-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "region@color area", "grouping and linking@color grouping", "element@color highlight", "lines and arrows@trajectories", "identifiers@letters", "lines and arrows@transfer"], "CHI18_paper205-Figure14-1.png": ["point of view@UI only", "type@clipart/icon", "number of frames@one frame", "UI@rendering", "hue@colors", "grouping and linking@identifiers grouping", "lines and arrows@direction"], "CHI18_paper374-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "element@color highlight", "lines and arrows@direction"], "CHI18_paper401-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "UIST18_paper65-Figure2-1.png": ["sub-framing@magnification lens", "point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "lines and arrows@transfer", "line style@width", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper423-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper629-Figure7-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper19-Figure10-1.png": ["point of view@1st person", "sub-framing@UI embedded", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@text", "hue@colors", "enclosing@circle/rectangle", "sub-framing@magnification lens", "lines and arrows@direction", "identifiers@letters"], "Ubicomp18_paper185-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "color@transparency", "hue@colors", "effects@stroboscopic", "enclosing@circle/rectangle"], "CHI18_paper446-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "region@color area", "type@clipart/icon", "identifiers@letters", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction"], "UIST18_paper335-Figure9-1.png": ["point of view@top", "sub-framing@UI embedded", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper160-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "Ubicomp18_paper161-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@data visualization", "type@photo", "hue@colors", "hue@grayscale", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper529-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "hue@colors", "color@transparency", "element@color highlight", "identifiers@letters", "lines and arrows@transfer", "UI@rendering"], "CHI18_paper297-Figure3-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper86-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "lines and arrows@direction", "identifiers@title", "sub-framing@juxtaposition"], "CHI18_paper644-Figure9-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "type@text", "hue@colors", "identifiers@title"], "CHI18_paper446-Figure3-1.png": ["point of view@1st person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "type@photo", "type@text", "hue@colors", "color@transparency", "region@color area", "identifiers@letters", "lines and arrows@trajectories", "grouping and linking@text annotation"], "CHI18_paper529-Figure7-1.png": ["point of view@top", "sub-framing@UI embedded", "number of frames@multi frames", "realism@simplistic", "UI@drawing", "hue@colors", "color@transparency", "element@color highlight", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper89-Figure11-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "UI@rendering", "hue@colors"], "CHI18_paper210-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "type@text"], "CHI18_paper564-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "enclosing@circle/rectangle", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@letters", "region@color area"], "CHI18_paper266-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "hue@colors", "type@photo", "identifiers@letters"], "CHI18_paper362-Figure4-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper502-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper248-Figure3-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "color@transparency", "hue@colors", "enclosing@circle/rectangle", "lines and arrows@trajectories", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper117-Figure13-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper281-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors", "effects@waves"], "CHI18_paper291-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "UI@drawing", "realism@simplistic", "hue@colors", "type@text", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@title", "line style@dashed"], "CHI18_paper411-Figure5-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "enclosing@circle/rectangle", "type@text", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper638-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "hue@colors", "UI@drawing", "element@color highlight", "enclosing@exact contour line", "effects@waves", "identifiers@numbers"], "CHI18_paper89-Figure4-1.png": ["point of view@UI only", "number of frames@one frame", "realism@simplistic", "type@text", "hue@monochrome", "line style@dashed", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper162-Figure11-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper237-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "type@text", "hue@colors", "color@transparency", "identifiers@letters", "identifiers@title", "region@color area", "element@color highlight", "effects@stroboscopic", "lines and arrows@trajectories", "type@data visualization", "grouping and linking@text annotation"], "CHI18_paper541-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@magnification lens", "sub-framing@UI embedded", "sub-framing@inset (PiP)", "UI@drawing", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "dynamic@Contact shapes", "type@clipart/icon", "measure@Arrows", "measure@Text indicator", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@transfer", "identifiers@title", "grouping and linking@text annotation"], "UIST18_paper963-Figure4-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "dynamic@Contact shapes", "element@color highlight", "line style@dashed", "identifiers@letters"], "CHI18_paper541-Figure5-1.png": ["point of view@3rd person", "point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "realism@simplistic", "type@clipart/icon", "UI@rendering", "hue@colors", "element@color highlight", "identifiers@title", "dynamic@Contact shapes", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper248-Figure9-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@text", "hue@colors", "color@transparency", "enclosing@exact contour line", "region@color area", "identifiers@letters", "grouping and linking@text annotation"], "UIST18_paper595-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "lines and arrows@direction", "effects@waves", "dynamic@Contact shapes", "identifiers@letters", "background@removed"], "CHI18_paper236-Figure3-1.png": ["anonymization@blur", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "enclosing@circle/rectangle", "identifiers@letters"], "CHI18_paper433-Figure1-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "type@photo", "sub-framing@UI embedded", "hue@colors", "identifiers@letters"], "CHI18_paper508-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "identifiers@title"], "UIST18_paper725-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "UI@drawing", "hue@colors", "color@transparency", "element@color highlight"], "CHI18_paper89-Figure13-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@data visualization", "hue@colors"], "UIST18_paper473-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@UI embedded", "type@photo", "realism@simplistic", "hue@colors", "identifiers@letters"], "CHI18_paper237-Figure12-1.png": ["point of view@UI only", "sub-framing@juxtaposition", "type@data visualization", "UI@drawing", "number of frames@one frame", "type@text", "hue@colors", "grouping and linking@color grouping", "identifiers@letters", "effects@waves", "lines and arrows@direction", "grouping and linking@text annotation"], "UIST18_paper99-Figure16-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@transfer", "identifiers@letters"], "UIST18_paper5-Figure11-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters"], "UIST18_paper247-Figure10-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper185-Figure9-1.png": ["point of view@UI only", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters"], "CHI18_paper209-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors"], "CHI18_paper411-Figure10-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "type@text", "identifiers@title", "grouping and linking@color grouping", "lines and arrows@trajectories"], "Ubicomp18_paper200-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper185-Figure12-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "hue@colors", "element@color highlight"], "CHI18_paper470-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper178-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "point of view@top", "sub-framing@UI embedded", "UI@rendering", "type@photo", "realism@simplistic", "hue@colors", "identifiers@letters", "grouping and linking@color grouping", "lines and arrows@transfer"], "CHI18_paper291-Figure7-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "lines and arrows@trajectories", "effects@stroboscopic", "line style@dashed", "element@color highlight", "grouping and linking@color grouping", "grouping and linking@identifiers grouping"], "UIST18_paper737-Figure7-1.png": ["point of view@3rd person", "type@data visualization", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper446-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "line style@dashed", "lines and arrows@direction", "sub-framing@UI overlay"], "CHI18_paper644-Figure11-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "UIST18_paper595-Figure15-1.png": ["point of view@UI only", "point of view@overshoulder 3/4", "point of view@1st person", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "type@text", "grouping and linking@text annotation"], "CHI18_paper249-Figure4-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "element@color highlight", "region@color area"], "CHI18_paper20-Figure2-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@text", "type@photo", "hue@colors", "hue@grayscale", "line style@dashed", "identifiers@letters", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper653-Figure3-1.png": ["point of view@UI only", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@title", "identifiers@letters", "type@text", "element@color highlight"], "CHI18_paper69-Figure5-1.png": ["point of view@UI only", "sub-framing@UI embedded", "number of frames@one frame", "type@text", "type@clipart/icon", "hue@colors", "lines and arrows@transfer"], "CHI18_paper541-Figure9-1.png": ["point of view@3rd person", "point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "sub-framing@magnification lens", "type@text", "type@clipart/icon", "lines and arrows@transfer", "grouping and linking@identifiers grouping", "dynamic@Contact shapes", "measure@Text indicator", "measure@Arrows", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction", "identifiers@title"], "CHI18_paper603-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "hue@colors", "element@color highlight", "lines and arrows@trajectories", "region@color area", "measure@Text indicator", "measure@Arrows", "identifiers@letters"], "CSCW18_paper118-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "type@text", "color@transparency", "hue@colors", "grouping and linking@identifiers grouping", "identifiers@letters"], "UIST18_paper649-Figure6-1.png": ["point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@data visualization", "identifiers@letters", "lines and arrows@trajectories"], "CHI18_paper378-Figure4-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper241-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@grayscale", "line style@width", "lines and arrows@direction"], "CHI18_paper423-Figure8-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "UIST18_paper649-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@clipart/icon", "hue@colors", "lines and arrows@trajectories", "grouping and linking@color grouping", "element@color highlight", "identifiers@letters"], "CHI18_paper188-Figure8-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@text", "hue@colors", "identifiers@letters", "region@color area", "measure@Text indicator"], "CHI18_paper446-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "point of view@1st person", "type@photo", "UI@rendering", "color@transparency", "hue@colors", "effects@waves", "lines and arrows@direction", "grouping and linking@identifiers grouping", "identifiers@letters"], "UIST18_paper853-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "color@transparency", "lines and arrows@direction", "line style@width", "element@color highlight", "identifiers@letters", "lines and arrows@projection", "lines and arrows@trajectories", "enclosing@circle/rectangle"], "CHI18_paper589-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@data visualization", "hue@colors", "line style@dashed", "type@text", "lines and arrows@direction", "measure@Text indicator", "effects@stroboscopic", "measure@Arrows"], "CHI18_paper477-Figure4-1.png": ["sub-framing@magnification lens", "point of view@top", "type@clipart/icon", "grouping and linking@identifiers grouping", "number of frames@one frame", "hue@colors", "element@color highlight", "region@color area", "sub-framing@UI embedded", "UI@rendering", "realism@simplistic", "type@text"], "UIST18_paper53-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "color@transparency", "identifiers@letters", "effects@stroboscopic", "region@color area", "line style@dashed", "lines and arrows@direction"], "CHI18_paper150-Figure2-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "realism@simplistic", "hue@colors", "lines and arrows@trajectories", "identifiers@title", "type@text", "identifiers@letters", "effects@stroboscopic"], "UIST18_paper141-Figure14-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper128-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@grayscale", "color@transparency", "effects@stroboscopic", "measure@Arrows", "line style@dashed", "identifiers@letters", "lines and arrows@direction", "grouping and linking@identifiers grouping"], "CHI18_paper219-Figure8-1.png": ["point of view@top", "number of frames@one frame", "type@text", "hue@colors", "color@transparency", "region@color area", "element@color highlight", "line style@dashed", "lines and arrows@trajectories", "grouping and linking@identifiers grouping", "grouping and linking@color grouping", "effects@stroboscopic"], "CHI18_paper446-Figure12-1.png": ["point of view@3rd person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "lines and arrows@trajectories", "line style@dashed", "identifiers@letters"], "CHI18_paper82-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "grouping and linking@identifiers grouping", "realism@simplistic", "sub-framing@UI embedded", "UI@drawing", "hue@colors", "color@transparency", "grouping and linking@color grouping"], "CHI18_paper436-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@monochrome", "lines and arrows@trajectories", "line style@dashed", "identifiers@letters"], "CHI18_paper42-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "type@data visualization", "sub-framing@juxtaposition", "hue@colors", "lines and arrows@direction"], "UIST18_paper87-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper245-Figure3-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "measure@Arrows", "measure@Text indicator", "grouping and linking@identifiers grouping", "lines and arrows@trajectories"], "UIST18_paper511-Figure12-1.png": ["point of view@1st person", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "line style@dashed", "identifiers@letters", "lines and arrows@trajectories"], "CHI18_paper460-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper188-Figure2-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper502-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors"], "UIST18_paper485-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper320-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper213-Figure20-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper73-Figure4-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "enclosing@circle/rectangle"], "CHI18_paper629-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper613-Figure8-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "type@data visualization", "hue@colors", "identifiers@letters"], "CHI18_paper162-Figure6-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper508-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "line style@dashed", "line style@width", "identifiers@title", "sub-framing@juxtaposition", "grouping and linking@text annotation", "grouping and linking@color grouping"], "CHI18_paper142-Figure1-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "type@text", "hue@colors", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper245-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper291-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "UI@drawing", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction", "grouping and linking@color grouping"], "CHI18_paper336-Figure13-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@data visualization", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper613-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "identifiers@letters", "identifiers@title"], "CHI18_paper202-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper150-Figure13-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "line style@dashed", "measure@Text indicator", "measure@Arrows", "UI@drawing", "type@photo", "hue@colors", "identifiers@letters"], "CSCW18_paper159-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors"], "CHI18_paper150-Figure15-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "measure@Arrows", "measure@Text indicator", "identifiers@letters"], "CHI18_paper300-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "anonymization@blur", "enclosing@circle/rectangle", "lines and arrows@trajectories", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper569-Figure15-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "identifiers@title"], "CHI18_paper360-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper321-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "effects@waves", "identifiers@letters", "grouping and linking@text annotation"], "CHI18_paper25-Figure2-1.png": ["point of view@top", "point of view@UI only", "number of frames@multi frames", "UI@drawing", "realism@simplistic", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "dynamic@Contact shapes", "element@color highlight"], "UIST18_paper511-Figure2-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters"], "CSCW18_paper192-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors"], "Ubicomp18_paper194-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@magnification lens", "line style@dashed", "hue@colors", "realism@simplistic", "realism@realistic", "type@data visualization", "type@text", "grouping and linking@text annotation", "grouping and linking@color grouping", "type@clipart/icon", "lines and arrows@direction", "effects@waves", "measure@Arrows", "measure@Text indicator"], "CHI18_paper564-Figure2-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "element@color highlight", "identifiers@letters", "color@transparency"], "UIST18_paper853-Figure4-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@magnification lens", "type@data visualization", "realism@simplistic", "hue@colors", "line style@dashed", "effects@stroboscopic", "identifiers@letters"], "CHI18_paper406-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@text", "realism@simplistic", "UI@drawing", "hue@colors", "enclosing@circle/rectangle", "element@color highlight", "effects@stroboscopic", "lines and arrows@direction", "identifiers@title", "identifiers@letters"], "CHI18_paper353-Figure4-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "UI@rendering", "hue@colors"], "CHI18_paper661-Figure1-1.png": ["point of view@1st person", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "number of frames@multi frames", "element@color highlight", "lines and arrows@trajectories", "sub-framing@magnification lens", "color@transparency", "effects@stroboscopic"], "CHI18_paper362-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper529-Figure1-1.png": ["point of view@top", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "color@transparency", "hue@colors", "lines and arrows@transfer", "number of frames@one frame", "sub-framing@juxtaposition"], "CHI18_paper220-Figure2-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@monochrome", "type@text", "identifiers@title"], "UIST18_paper335-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@juxtaposition", "type@text", "type@data visualization", "hue@colors", "line style@dashed", "dynamic@Contact shapes", "grouping and linking@color grouping", "lines and arrows@trajectories", "effects@stroboscopic", "grouping and linking@identifiers grouping"], "CSCW18_paper072-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper33-Figure5-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@data visualization", "type@text", "measure@Text indicator", "measure@Arrows", "identifiers@letters", "hue@colors"], "CHI18_paper425-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper76-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper541-Figure8-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "lines and arrows@transfer", "element@color highlight", "grouping and linking@color grouping", "lines and arrows@trajectories", "UI@drawing", "realism@simplistic", "point of view@UI only", "sub-framing@magnification lens", "sub-framing@UI embedded", "hue@colors", "type@text", "grouping and linking@text annotation", "identifiers@title"], "CHI18_paper54-Figure10-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper579-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "point of view@3rd person", "sub-framing@UI overlay", "type@photo", "color@transparency", "UI@rendering", "hue@colors", "grouping and linking@text annotation", "identifiers@letters"], "UIST18_paper5-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper287-Figure2-1.png": ["number of frames@multi frames", "type@photo", "point of view@3rd person", "type@data visualization", "hue@colors", "identifiers@letters"], "CHI18_paper354-Figure17-1.png": ["point of view@3rd person", "sub-framing@juxtaposition", "number of frames@one frame", "identifiers@letters", "identifiers@title", "grouping and linking@text annotation", "hue@colors", "sub-framing@UI embedded", "type@photo"], "UIST18_paper737-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "grouping and linking@text annotation", "identifiers@letters", "lines and arrows@trajectories"], "CHI18_paper89-Figure5-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "point of view@3rd person", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters"], "UIST18_paper5-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "color@transparency", "region@color area", "element@color highlight", "enclosing@exact contour line", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@letters"], "UIST18_paper877-Figure3-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "line style@dashed", "line style@width", "lines and arrows@direction", "identifiers@letters", "identifiers@title"], "CHI18_paper401-Figure7-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "Ubicomp18_paper176-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper142-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "type@data visualization", "type@text", "element@color highlight", "lines and arrows@direction"], "CHI18_paper547-Figure9-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "line style@dashed", "element@color highlight", "effects@stroboscopic", "lines and arrows@direction", "lines and arrows@trajectories"], "CHI18_paper61-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper428-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "type@photo", "hue@colors", "element@color highlight", "type@text", "identifiers@title", "grouping and linking@text annotation", "enclosing@exact contour line", "grouping and linking@color grouping", "effects@stroboscopic"], "CHI18_paper199-Figure17-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "type@data visualization", "type@text", "hue@colors", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper69-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "UI@rendering", "type@clipart/icon", "type@text", "hue@colors", "effects@stroboscopic", "color@transparency", "lines and arrows@transfer", "identifiers@letters", "identifiers@title"], "CHI18_paper291-Figure9-1.png": ["point of view@3rd person", "point of view@1st person", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "grouping and linking@text annotation"], "CHI18_paper150-Figure8-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "measure@Arrows", "measure@Text indicator", "identifiers@letters"], "CHI18_paper547-Figure8-1.png": ["point of view@top", "point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@direction", "dynamic@Contact shapes", "line style@dashed"], "CHI18_paper123-Figure7-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "realism@simplistic", "hue@colors", "element@color highlight", "identifiers@letters"], "UIST18_paper53-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "region@color area", "lines and arrows@trajectories", "effects@stroboscopic", "identifiers@letters", "effects@waves"], "CHI18_paper54-Figure7-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "point of view@UI only", "number of frames@multi frames", "sub-framing@magnification lens", "type@photo", "UI@rendering", "sub-framing@UI embedded", "hue@colors", "enclosing@circle/rectangle", "identifiers@letters", "grouping and linking@text annotation"], "CHI18_paper593-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper123-Figure9-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "realism@simplistic", "hue@colors", "identifiers@letters", "element@color highlight"], "CHI18_paper443-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "anonymization@blur"], "CHI18_paper43-Figure3-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "sub-framing@UI embedded", "sub-framing@inset (PiP)", "number of frames@one frame", "type@photo", "hue@colors"], "Ubicomp18_paper181-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "enclosing@circle/rectangle"], "CHI18_paper54-Figure4-1.png": ["number of frames@one frame", "point of view@3rd person", "sub-framing@UI embedded", "line style@dashed", "type@photo", "hue@colors", "color@transparency", "region@color area", "enclosing@circle/rectangle", "grouping and linking@text annotation", "identifiers@letters"], "Ubicomp18_paper162-Figure1-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors"], "CHI18_paper626-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper496-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@clipart/icon", "effects@waves", "type@text", "measure@Arrows", "measure@Text indicator"], "UIST18_paper335-Figure1-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "realism@simplistic", "hue@colors", "color@transparency", "line style@dashed", "element@color highlight", "region@color area", "effects@stroboscopic", "lines and arrows@trajectories", "lines and arrows@direction", "dynamic@Contact shapes"], "Ubicomp18_paper164-Figure2-1.png": ["point of view@1st person", "point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper189-Figure9-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "sub-framing@magnification lens", "identifiers@letters"], "CHI18_paper613-Figure10-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "grouping and linking@text annotation", "identifiers@letters"], "CHI18_paper593-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CSCW18_paper185-Figure5-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "UIST18_paper485-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "realism@realistic", "hue@colors"], "CHI18_paper61-Figure9-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "Ubicomp18_paper164-Figure8-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper529-Figure14-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper11-Figure1-1.png": ["point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "enclosing@circle/rectangle", "point of view@top"], "CHI18_paper362-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "UIST18_paper87-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper321-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@data visualization", "type@photo", "hue@colors", "color@transparency"], "CHI18_paper401-Figure8-1.png": ["point of view@3rd person", "sub-framing@UI embedded", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper95-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors"], "CHI18_paper5-Figure1-1.png": ["point of view@3rd person", "point of view@top", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper199-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@drawing", "realism@simplistic", "hue@colors", "color@transparency", "line style@dashed", "lines and arrows@trajectories", "identifiers@letters", "element@color highlight", "grouping and linking@color grouping"], "CHI18_paper45-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper218-Figure14-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters"], "CHI18_paper89-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "UIST18_paper637-Figure4-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters"], "CHI18_paper150-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "UI@drawing", "hue@colors", "color@transparency", "identifiers@title", "identifiers@letters", "effects@stroboscopic", "lines and arrows@trajectories", "element@color highlight"], "CHI18_paper199-Figure7-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper516-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper238-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "color@transparency", "effects@stroboscopic", "type@data visualization", "identifiers@letters"], "UIST18_paper779-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "identifiers@letters", "identifiers@title"], "UIST18_paper901-Figure9-1.png": ["point of view@1st person", "point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "UI@rendering", "type@text", "hue@colors", "identifiers@title", "element@color highlight", "measure@Arrows"], "CHI18_paper284-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper336-Figure4-1.png": ["point of view@1st person", "number of frames@one frame", "type@text", "type@photo", "hue@colors", "grouping and linking@text annotation", "lines and arrows@direction"], "CHI18_paper73-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "CHI18_paper406-Figure3-1.png": ["lines and arrows@direction", "identifiers@letters", "identifiers@title", "element@color highlight", "UI@drawing", "realism@simplistic", "hue@colors", "number of frames@multi frames", "point of view@top"], "CHI18_paper189-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper411-Figure6-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "effects@waves", "grouping and linking@text annotation"], "CHI18_paper223-Figure7-1.png": ["number of frames@multi frames", "point of view@UI only", "realism@simplistic", "UI@drawing", "type@text", "hue@colors", "identifiers@title", "lines and arrows@trajectories", "lines and arrows@direction", "element@color highlight", "enclosing@circle/rectangle"], "UIST18_paper53-Figure7-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper745-Figure11-1.png": ["point of view@1st person", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded"], "UIST18_paper853-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "color@transparency", "hue@colors", "line style@dashed", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters", "grouping and linking@text annotation", "lines and arrows@direction", "type@text"], "CHI18_paper18-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "lines and arrows@projection"], "CHI18_paper219-Figure13-1.png": ["point of view@1st person", "type@photo", "sub-framing@UI overlay", "number of frames@multi frames", "hue@colors", "identifiers@letters"], "CHI18_paper426-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper248-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "type@text", "color@transparency", "hue@colors", "line style@dashed", "grouping and linking@text annotation", "identifiers@letters", "lines and arrows@trajectories", "effects@stroboscopic"], "CHI18_paper477-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@magnification lens", "UI@drawing", "realism@simplistic", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "grouping and linking@text annotation", "type@text"], "UIST18_paper499-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "sub-framing@UI overlay", "UI@drawing", "type@photo", "hue@colors", "color@transparency", "region@color area", "identifiers@letters", "grouping and linking@identifiers grouping"], "CHI18_paper185-Figure6-1.png": ["point of view@overshoulder 3/4", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors"], "UIST18_paper839-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@data visualization", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper515-Figure8-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded"], "CHI18_paper160-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper165-Figure3-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors"], "CHI18_paper160-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors"], "CHI18_paper426-Figure9-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@clipart/icon", "type@photo", "hue@colors", "lines and arrows@transfer", "effects@waves", "identifiers@letters"], "UIST18_paper335-Figure6-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters"], "UIST18_paper313-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "realism@simplistic", "hue@colors", "identifiers@letters", "lines and arrows@direction"], "CHI18_paper89-Figure7-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters"], "CHI18_paper531-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors"], "CHI18_paper551-Figure3-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors", "anonymization@blur"], "CHI18_paper284-Figure7-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors"], "CHI18_paper69-Figure4-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@magnification lens", "realism@simplistic", "UI@drawing", "hue@colors", "type@clipart/icon", "type@text", "grouping and linking@text annotation"], "UIST18_paper499-Figure12-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "hue@grayscale", "lines and arrows@trajectories", "identifiers@letters"], "UIST18_paper31-Figure3-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "realism@simplistic", "type@photo", "hue@colors", "grouping and linking@color grouping", "identifiers@letters", "effects@stroboscopic", "color@transparency", "lines and arrows@trajectories"], "UIST18_paper153-Figure3-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@juxtaposition", "hue@colors", "type@text", "element@color highlight", "lines and arrows@transfer", "identifiers@title", "grouping and linking@identifiers grouping", "effects@stroboscopic", "color@transparency", "UI@drawing"], "CHI18_paper569-Figure17-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters"], "CHI18_paper446-Figure11-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "effects@waves", "lines and arrows@direction"], "CHI18_paper223-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "realism@simplistic", "UI@drawing", "hue@colors"], "CHI18_paper163-Figure1-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors"], "UIST18_paper499-Figure11-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@UI overlay", "point of view@3rd person", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "region@color area", "identifiers@letters"], "Ubicomp18_paper201-Figure3-1.png": ["point of view@top", "point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "type@text", "region@color area", "grouping and linking@text annotation"], "UIST18_paper853-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "identifiers@title", "grouping and linking@text annotation", "lines and arrows@direction", "lines and arrows@projection"], "CHI18_paper11-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@rendering", "type@clipart/icon", "type@text", "hue@grayscale", "identifiers@title", "region@color area"], "CHI18_paper143-Figure7-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@inset (PiP)", "point of view@1st person", "UI@rendering", "hue@colors"], "CHI18_paper150-Figure21-1.png": ["type@photo", "point of view@1st person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "hue@colors", "type@clipart/icon", "enclosing@circle/rectangle"], "CHI18_paper150-Figure22-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "hue@colors", "enclosing@circle/rectangle", "grouping and linking@identifiers grouping", "identifiers@letters"], "CHI18_paper150-Figure5-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@inset (PiP)", "hue@colors", "enclosing@circle/rectangle"], "CHI18_paper150-Figure6-1.png": ["type@text", "type@photo", "point of view@3rd person", "type@data visualization", "number of frames@multi frames", "hue@colors", "identifiers@letters", "measure@Arrows", "measure@Text indicator"], "CHI18_paper189-Figure1-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@letters"], "CHI18_paper189-Figure10-1.png": ["type@text", "type@photo", "point of view@UI only", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "identifiers@letters"], "CHI18_paper19-Figure7-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@text", "type@data visualization", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper199-Figure16-1.png": ["type@photo", "point of view@top", "number of frames@multi frames", "hue@colors", "lines and arrows@direction"], "CHI18_paper199-Figure5-1.png": ["effects@stroboscopic", "color@transparency", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "element@color highlight", "lines and arrows@direction", "lines and arrows@trajectories", "measure@Arrows", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper210-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors"], "CHI18_paper210-Figure4-1.png": ["type@text", "type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "hue@colors"], "CHI18_paper218-Figure4-1.png": ["effects@stroboscopic", "point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@grayscale", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@projection", "identifiers@letters"], "CHI18_paper218-Figure8-1.png": ["type@photo", "point of view@3rd person", "sub-framing@inset (PiP)", "number of frames@multi frames", "UI@rendering", "hue@colors", "effects@motion blur", "identifiers@letters"], "CHI18_paper219-Figure12-1.png": ["type@text", "type@photo", "point of view@1st person", "sub-framing@UI overlay", "number of frames@one frame", "UI@rendering", "hue@colors", "color@transparency"], "CHI18_paper238-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "point of view@UI only", "UI@rendering", "hue@colors", "line style@dashed", "element@color highlight", "region@color area", "dynamic@Contact shapes", "identifiers@letters"], "CHI18_paper249-Figure2-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@rendering", "hue@colors", "lines and arrows@trajectories"], "CHI18_paper255-Figure5-1.png": ["hue@monochrome", "point of view@top", "number of frames@multi frames", "realism@simplistic"], "CHI18_paper291-Figure8-1.png": ["color@transparency", "type@photo", "point of view@3rd person", "sub-framing@UI overlay", "number of frames@multi frames", "hue@colors", "lines and arrows@direction"], "CHI18_paper299-Figure2-1.png": ["type@text", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@rendering", "realism@simplistic", "hue@colors", "element@color highlight", "identifiers@title"], "CHI18_paper31-Figure1-1.png": ["type@photo", "point of view@1st person", "number of frames@one frame", "hue@colors"], "CHI18_paper336-Figure15-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "identifiers@letters"], "CHI18_paper359-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "anonymization@blur"], "CHI18_paper362-Figure5-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "hue@colors"], "CHI18_paper362-Figure6-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "hue@colors"], "CHI18_paper407-Figure2-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@multi frames", "hue@colors", "enclosing@exact contour line", "element@color highlight"], "CHI18_paper419-Figure1-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@realistic", "UI@drawing", "hue@grayscale"], "CHI18_paper441-Figure21-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "type@clipart/icon", "hue@monochrome", "lines and arrows@transfer", "effects@waves", "identifiers@numbers"], "CHI18_paper445-Figure4-1.png": ["type@text", "point of view@UI only", "number of frames@one frame", "UI@rendering", "hue@colors", "effects@stroboscopic", "grouping and linking@text annotation"], "CHI18_paper446-Figure7-1.png": ["color@transparency", "type@photo", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI overlay", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "identifiers@letters", "UI@drawing", "type@clipart/icon"], "CHI18_paper465-Figure7-1.png": ["type@photo", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters"], "CHI18_paper530-Figure1-1.png": ["UI@drawing", "type@photo", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "lines and arrows@trajectories", "effects@motion blur", "identifiers@letters"], "CHI18_paper54-Figure6-1.png": ["type@text", "point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "type@photo", "hue@colors", "enclosing@circle/rectangle", "identifiers@letters", "background@removed", "line style@dashed", "grouping and linking@text annotation"], "CHI18_paper541-Figure3-1.png": ["type@text", "number of frames@multi frames", "type@data visualization", "hue@colors", "grouping and linking@color grouping", "element@color highlight", "type@clipart/icon", "lines and arrows@transfer", "measure@Arrows", "dynamic@Contact shapes"], "CHI18_paper547-Figure10-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "line style@dashed", "color@transparency", "element@color highlight", "lines and arrows@trajectories", "effects@stroboscopic"], "CHI18_paper547-Figure5-1.png": ["type@text", "point of view@top", "point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "line style@dashed", "lines and arrows@direction", "identifiers@letters", "identifiers@title"], "CHI18_paper547-Figure7-1.png": ["hue@monochrome", "type@text", "effects@stroboscopic", "point of view@3rd person", "point of view@top", "number of frames@multi frames", "realism@simplistic", "dynamic@Contact shapes", "lines and arrows@direction", "identifiers@letters", "identifiers@title"], "CHI18_paper558-Figure2-1.png": ["hue@monochrome", "point of view@3rd person", "number of frames@one frame", "realism@simplistic", "line style@dashed", "measure@Arrows", "grouping and linking@identifiers grouping"], "CHI18_paper564-Figure3-1.png": ["type@text", "color@transparency", "type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@inset (PiP)", "hue@colors", "line style@dashed", "enclosing@exact contour line", "grouping and linking@identifiers grouping", "dynamic@Contact shapes", "lines and arrows@trajectories"], "CHI18_paper579-Figure13-1.png": ["UI@drawing", "type@photo", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "hue@colors", "color@transparency", "identifiers@letters", "lines and arrows@projection"], "CHI18_paper61-Figure10-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "effects@stroboscopic"], "CHI18_paper634-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors"], "CHI18_paper634-Figure8-1.png": ["type@text", "effects@stroboscopic", "type@data visualization", "realism@simplistic", "UI@drawing", "point of view@3rd person", "sub-framing@inset (PiP)", "number of frames@one frame", "hue@colors", "grouping and linking@color grouping", "lines and arrows@direction", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "grouping and linking@identifiers grouping"], "CHI18_paper647-Figure2-1.png": ["type@text", "UI@drawing", "point of view@3rd person", "number of frames@one frame", "sub-framing@UI overlay", "realism@realistic", "hue@colors", "element@color highlight", "measure@Text indicator", "measure@Arrows", "dynamic@Contact shapes", "lines and arrows@projection", "grouping and linking@identifiers grouping"], "CHI18_paper654-Figure4-1.png": ["type@text", "UI@drawing", "point of view@3rd person", "sub-framing@juxtaposition", "number of frames@one frame", "hue@colors", "line style@dashed", "dynamic@Contact shapes", "lines and arrows@direction"], "CHI18_paper69-Figure3-1.png": ["type@text", "point of view@UI only", "number of frames@multi frames", "type@clipart/icon", "hue@colors", "identifiers@letters"], "CHI18_paper73-Figure2-1.png": ["type@text", "point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "UI@rendering", "hue@colors", "element@color highlight", "line style@dashed", "lines and arrows@direction", "grouping and linking@text annotation", "identifiers@letters", "identifiers@title"], "CHI18_paper78-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors"], "CHI18_paper82-Figure7-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@numbers"], "CHI18_paper90-Figure7-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "background@removed", "sub-framing@UI overlay", "color@transparency"], "CHI18_paper95-Figure3-1.png": ["type@text", "type@photo", "point of view@top", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "color@transparency", "region@color area", "identifiers@title", "measure@Arrows", "measure@Text indicator"], "CHI18_paper98-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors"], "CSCW18_paper140-Figure3-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "hue@colors", "enclosing@circle/rectangle"], "CSCW18_paper140-Figure5-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "hue@colors"], "CSCW18_paper185-Figure6-1.png": ["type@photo", "point of view@top", "number of frames@one frame", "hue@colors"], "CSCW18_paper192-Figure6-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors"], "UIST18_paper31-Figure4-1.png": ["type@text", "color@transparency", "UI@drawing", "type@photo", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "identifiers@title", "effects@stroboscopic", "enclosing@exact contour line"], "UIST18_paper321-Figure15-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "hue@colors", "lines and arrows@direction", "identifiers@letters", "effects@waves"], "UIST18_paper335-Figure8-1.png": ["type@photo", "point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "type@text", "identifiers@letters", "identifiers@title", "lines and arrows@direction"], "UIST18_paper347-Figure1-1.png": ["type@text", "UI@drawing", "point of view@top", "point of view@UI only", "sub-framing@juxtaposition", "number of frames@multi frames", "realism@simplistic", "hue@colors", "region@color area", "effects@waves", "lines and arrows@direction", "identifiers@title", "identifiers@letters"], "UIST18_paper485-Figure3-1.png": ["UI@drawing", "point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "type@data visualization", "hue@colors", "grouping and linking@color grouping", "grouping and linking@identifiers grouping", "line style@dashed", "lines and arrows@direction", "realism@realistic"], "UIST18_paper499-Figure7-1.png": ["color@transparency", "type@photo", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "UI@drawing", "hue@colors", "region@color area", "identifiers@letters"], "UIST18_paper581-Figure10-1.png": ["type@photo", "background@grayed out/blurred", "point of view@3rd person", "number of frames@one frame", "hue@colors"], "UIST18_paper745-Figure13-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors"], "UIST18_paper779-Figure7-1.png": ["effects@stroboscopic", "point of view@UI only", "sub-framing@juxtaposition", "number of frames@multi frames", "point of view@top", "point of view@3rd person", "realism@simplistic", "UI@rendering", "hue@colors", "element@color highlight", "lines and arrows@direction", "lines and arrows@transfer", "identifiers@letters"], "UIST18_paper839-Figure7-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "point of view@UI only", "UI@rendering", "hue@colors", "color@transparency", "effects@stroboscopic", "identifiers@letters"], "UIST18_paper853-Figure7-1.png": ["UI@drawing", "point of view@3rd person", "number of frames@multi frames", "color@transparency", "hue@colors", "grouping and linking@color grouping", "lines and arrows@projection", "identifiers@letters"], "UIST18_paper867-Figure3-1.png": ["point of view@UI only", "number of frames@one frame", "type@clipart/icon", "UI@rendering", "hue@colors", "lines and arrows@direction", "dynamic@Contact shapes"], "UIST18_paper87-Figure11-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "hue@colors"], "UIST18_paper927-Figure6-1.png": ["type@photo", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "UI@rendering", "hue@colors", "lines and arrows@transfer", "lines and arrows@direction", "identifiers@letters"], "UIST18_paper99-Figure15-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors"], "Ubicomp18_paper161-Figure2-1.png": ["type@text", "UI@drawing", "type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "identifiers@title", "background@removed"], "Ubicomp18_paper198-Figure3-1.png": ["type@text", "point of view@3rd person", "line style@dashed", "line style@width", "hue@monochrome", "type@clipart/icon", "identifiers@title", "lines and arrows@direction", "realism@simplistic", "number of frames@multi frames"], "Ubicomp18_paper200-Figure7-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@letters"], "CHI18_paper107-Figure5-1.png": ["point of view@3rd person", "type@photo", "hue@colors", "number of frames@multi frames", "point of view@UI only", "identifiers@letters"], "CHI18_paper11-Figure7-1.png": ["identifiers@title", "point of view@1st person", "line style@width", "type@text", "hue@grayscale", "region@color area", "sub-framing@UI embedded", "number of frames@multi frames", "realism@simplistic", "UI@rendering", "type@clipart/icon"], "CHI18_paper131-Figure1-1.png": ["type@photo", "point of view@1st person", "hue@colors", "number of frames@one frame"], "CHI18_paper150-Figure16-1.png": ["point of view@1st person", "hue@colors", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "lines and arrows@direction", "measure@Text indicator"], "CHI18_paper165-Figure1-1.png": ["point of view@3rd person", "type@photo", "hue@colors", "number of frames@one frame"], "CHI18_paper173-Figure1-1.png": ["effects@stroboscopic", "line style@width", "point of view@3rd person", "hue@grayscale", "hue@colors", "measure@Arrows", "number of frames@multi frames", "realism@simplistic", "type@text", "line style@dashed", "lines and arrows@direction", "lines and arrows@projection", "color@transparency", "identifiers@letters"], "CHI18_paper179-Figure6-1.png": ["point of view@3rd person", "type@photo", "hue@colors", "number of frames@multi frames"], "CHI18_paper18-Figure6-1.png": ["point of view@1st person", "hue@colors", "number of frames@multi frames", "realism@simplistic", "lines and arrows@projection"], "CHI18_paper188-Figure4-1.png": ["point of view@3rd person", "type@photo", "point of view@1st person", "hue@colors", "number of frames@multi frames", "identifiers@letters"], "CHI18_paper19-Figure8-1.png": ["point of view@1st person", "UI@drawing", "realism@realistic", "hue@colors", "number of frames@multi frames", "sub-framing@UI embedded", "element@color highlight", "lines and arrows@trajectories", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper19-Figure9-1.png": ["point of view@1st person", "type@photo", "hue@colors", "sub-framing@juxtaposition", "number of frames@multi frames", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper199-Figure11-1.png": ["point of view@3rd person", "point of view@top", "type@photo", "hue@colors", "point of view@UI only", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "color@transparency", "identifiers@letters"], "CHI18_paper199-Figure12-1.png": ["point of view@top", "type@photo", "hue@colors", "number of frames@multi frames"], "CHI18_paper199-Figure14-1.png": ["point of view@top", "type@photo", "hue@colors", "number of frames@multi frames", "identifiers@letters"], "CHI18_paper199-Figure15-1.png": ["point of view@top", "type@photo", "hue@colors", "number of frames@multi frames", "identifiers@letters"], "CHI18_paper199-Figure6-1.png": ["effects@stroboscopic", "point of view@3rd person", "UI@drawing", "hue@colors", "measure@Arrows", "number of frames@multi frames", "realism@simplistic", "element@color highlight", "line style@dashed", "lines and arrows@direction", "grouping and linking@color grouping", "identifiers@letters"], "CHI18_paper21-Figure1-1.png": ["point of view@top", "type@photo", "hue@colors", "number of frames@one frame", "lines and arrows@direction"], "CHI18_paper218-Figure3-1.png": ["point of view@3rd person", "UI@drawing", "hue@colors", "point of view@1st person", "number of frames@multi frames", "sub-framing@inset (PiP)", "realism@simplistic", "hue@grayscale", "element@color highlight", "grouping and linking@color grouping", "line style@dashed", "lines and arrows@direction", "identifiers@letters", "lines and arrows@projection"], "CHI18_paper218-Figure5-1.png": ["point of view@top", "effects@stroboscopic", "type@text", "hue@grayscale", "measure@Arrows", "number of frames@multi frames", "realism@simplistic", "line style@dashed", "measure@Text indicator", "lines and arrows@projection"], "CHI18_paper218-Figure6-1.png": ["point of view@1st person", "point of view@3rd person", "type@photo", "type@text", "hue@colors", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "grouping and linking@text annotation", "identifiers@letters"], "CHI18_paper223-Figure3-1.png": ["identifiers@title", "point of view@top", "type@text", "UI@drawing", "hue@colors", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic"], "CHI18_paper223-Figure5-1.png": ["UI@drawing", "hue@colors", "point of view@UI only", "number of frames@one frame", "line style@dashed", "lines and arrows@trajectories"], "CHI18_paper236-Figure2-1.png": ["point of view@3rd person", "type@photo", "hue@colors", "point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@juxtaposition"], "CHI18_paper246-Figure3-1.png": ["point of view@3rd person", "type@photo", "point of view@1st person", "hue@colors", "number of frames@multi frames"], "CHI18_paper248-Figure7-1.png": ["effects@stroboscopic", "type@text", "color@transparency", "hue@colors", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "identifiers@letters", "lines and arrows@direction"], "CHI18_paper258-Figure1-1.png": ["identifiers@title", "effects@stroboscopic", "point of view@1st person", "point of view@3rd person", "type@text", "color@transparency", "sub-framing@UI embedded", "realism@realistic", "hue@colors", "measure@Arrows", "effects@waves", "number of frames@multi frames", "grouping and linking@color grouping", "lines and arrows@trajectories"], "CHI18_paper258-Figure2-1.png": ["color@transparency", "region@color area", "UI@drawing", "realism@realistic", "point of view@1st person", "sub-framing@UI overlay", "hue@colors", "line style@width", "point of view@top", "number of frames@multi frames"], "CHI18_paper281-Figure1-1.png": ["identifiers@title", "point of view@3rd person", "point of view@overshoulder 3/4", "type@text", "hue@colors", "number of frames@multi frames", "realism@simplistic", "element@color highlight", "grouping and linking@color grouping"], "CHI18_paper297-Figure2-1.png": ["point of view@top", "type@photo", "hue@colors", "number of frames@one frame"], "CHI18_paper300-Figure4-1.png": ["point of view@3rd person", "identifiers@title", "type@photo", "type@text", "hue@colors", "number of frames@multi frames", "element@color highlight", "lines and arrows@projection", "anonymization@blur"], "CHI18_paper336-Figure5-1.png": ["point of view@3rd person", "type@photo", "hue@colors", "number of frames@multi frames", "lines and arrows@direction", "identifiers@letters"], "CHI18_paper349-Figure3-1.png": ["point of view@3rd person", "hue@monochrome", "type@text", "hue@colors", "lines and arrows@projection", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "element@color highlight"], "CHI18_paper354-Figure16-1.png": ["point of view@1st person", "type@photo", "type@text", "hue@colors", "background@removed", "grouping and linking@identifiers grouping", "number of frames@one frame", "grouping and linking@text annotation"], "CHI18_paper367-Figure1-1.png": ["point of view@3rd person", "identifiers@title", "type@text", "hue@colors", "number of frames@multi frames", "realism@simplistic", "lines and arrows@direction"], "CHI18_paper367-Figure7-1.png": ["point of view@3rd person", "type@photo", "point of view@1st person", "hue@colors", "number of frames@multi frames", "sub-framing@juxtaposition"], "CHI18_paper374-Figure5-1.png": ["hue@monochrome", "type@photo", "point of view@1st person", "number of frames@multi frames"], "CHI18_paper397-Figure5-1.png": ["point of view@3rd person", "type@photo", "hue@colors", "number of frames@one frame", "anonymization@blur"], "CHI18_paper4-Figure3-1.png": ["type@photo", "point of view@1st person", "hue@colors", "number of frames@one frame"], "CHI18_paper407-Figure1-1.png": ["point of view@top", "sub-framing@magnification lens", "point of view@1st person", "point of view@3rd person", "region@color area", "hue@colors", "number of frames@multi frames", "realism@simplistic", "element@color highlight", "grouping and linking@color grouping", "UI@rendering", "identifiers@numbers", "identifiers@letters", "lines and arrows@transfer"], "CHI18_paper46-Figure3-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "color@transparency", "point of view@3rd person", "point of view@1st person", "identifiers@letters", "UI@rendering", "sub-framing@UI overlay"], "CHI18_paper86-Figure9-1.png": ["point of view@1st person", "hue@colors", "lines and arrows@trajectories", "number of frames@multi frames", "dynamic@Contact shapes", "UI@rendering"], "CHI18_paper638-Figure3-1.png": ["hue@colors", "line style@dashed", "type@text", "lines and arrows@trajectories", "identifiers@numbers", "point of view@UI only", "UI@rendering", "number of frames@one frame", "effects@waves", "type@clipart/icon", "effects@stroboscopic", "grouping and linking@text annotation"], "CSCW18_paper140-Figure2-1.png": ["hue@colors", "type@photo", "number of frames@one frame", "point of view@3rd person"], "CHI18_paper89-Figure6-1.png": ["hue@monochrome", "type@text", "lines and arrows@trajectories", "realism@simplistic", "point of view@top", "number of frames@one frame", "line style@dashed", "line style@width", "grouping and linking@text annotation"], "CHI18_paper46-Figure2-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "color@transparency", "point of view@1st person", "identifiers@letters", "UI@rendering", "sub-framing@UI overlay"], "CHI18_paper579-Figure4-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "color@transparency", "point of view@3rd person", "point of view@1st person", "identifiers@letters", "UI@rendering", "sub-framing@juxtaposition", "sub-framing@UI overlay", "lines and arrows@projection"], "CHI18_paper99-Figure3-1.png": ["sub-framing@inset (PiP)", "hue@colors", "number of frames@multi frames", "type@photo", "point of view@1st person", "point of view@3rd person", "UI@rendering", "point of view@UI only"], "UIST18_paper913-Figure8-1.png": ["hue@colors", "point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "identifiers@letters", "sub-framing@juxtaposition", "point of view@top"], "UIST18_paper473-Figure3-1.png": ["sub-framing@inset (PiP)", "hue@colors", "number of frames@multi frames", "type@photo", "point of view@3rd person", "type@text", "identifiers@letters"], "CHI18_paper465-Figure5-1.png": ["measure@Text indicator", "hue@colors", "number of frames@multi frames", "type@photo", "type@text", "identifiers@title", "identifiers@letters", "measure@Arrows", "line style@width", "point of view@3rd person", "point of view@1st person", "grouping and linking@text annotation"], "CHI18_paper43-Figure1-1.png": ["effects@stroboscopic", "sub-framing@inset (PiP)", "hue@monochrome", "hue@colors", "number of frames@multi frames", "type@photo", "realism@realistic", "point of view@3rd person", "point of view@1st person"], "CHI18_paper446-Figure18-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "identifiers@letters", "UI@rendering", "sub-framing@UI overlay", "color@transparency", "point of view@3rd person", "point of view@1st person"], "UIST18_paper335-Figure10-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "identifiers@letters", "point of view@top"], "UIST18_paper261-Figure4-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "point of view@top"], "UIST18_paper853-Figure9-1.png": ["hue@colors", "number of frames@multi frames", "color@transparency", "point of view@3rd person", "line style@dashed", "lines and arrows@projection", "element@color highlight", "realism@simplistic", "grouping and linking@color grouping", "sub-framing@juxtaposition", "type@text", "identifiers@title"], "UIST18_paper737-Figure8-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "identifiers@letters", "point of view@3rd person"], "Ubicomp18_paper201-Figure1-1.png": ["line style@width", "hue@colors", "point of view@1st person", "type@text", "identifiers@title", "realism@simplistic", "sub-framing@juxtaposition", "type@clipart/icon", "type@data visualization", "number of frames@one frame", "line style@dashed", "grouping and linking@color grouping", "element@color highlight"], "UIST18_paper745-Figure4-1.png": ["hue@colors", "type@photo", "point of view@top", "number of frames@one frame"], "CHI18_paper446-Figure8-1.png": ["hue@colors", "point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "line style@dashed", "lines and arrows@direction", "identifiers@letters", "line style@width", "sub-framing@UI overlay", "UI@drawing"], "CHI18_paper65-Figure2-1.png": ["effects@stroboscopic", "measure@Text indicator", "hue@colors", "number of frames@multi frames", "type@photo", "point of view@3rd person", "lines and arrows@direction", "lines and arrows@projection", "UI@drawing", "identifiers@letters", "measure@Arrows", "sub-framing@UI overlay"], "CHI18_paper446-Figure2-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "point of view@overshoulder 3/4", "line style@dashed", "lines and arrows@direction", "type@text", "realism@simplistic", "identifiers@letters", "measure@Arrows", "line style@width", "sub-framing@UI overlay", "type@clipart/icon", "color@transparency", "UI@drawing", "lines and arrows@trajectories", "grouping and linking@text annotation"], "CHI18_paper655-Figure2-1.png": ["sub-framing@inset (PiP)", "hue@colors", "type@photo", "point of view@3rd person", "point of view@1st person", "number of frames@one frame"], "UIST18_paper141-Figure10-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "background@removed", "point of view@3rd person", "type@text", "identifiers@title", "identifiers@letters"], "CHI18_paper603-Figure6-1.png": ["measure@Text indicator", "hue@colors", "number of frames@multi frames", "type@photo", "point of view@3rd person", "type@text", "identifiers@letters", "measure@Arrows", "type@data visualization", "grouping and linking@text annotation", "lines and arrows@trajectories"], "UIST18_paper485-Figure5-1.png": ["hue@colors", "realism@realistic", "point of view@3rd person", "type@text", "identifiers@title", "measure@Arrows", "number of frames@multi frames", "type@data visualization"], "UIST18_paper867-Figure4-1.png": ["sub-framing@inset (PiP)", "enclosing@exact contour line", "hue@colors", "type@photo", "type@text", "point of view@UI only", "UI@rendering", "number of frames@one frame", "realism@simplistic", "lines and arrows@direction"], "CHI18_paper76-Figure1-1.png": ["hue@colors", "type@photo", "point of view@overshoulder 3/4", "number of frames@one frame"], "Ubicomp18_paper201-Figure9-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "line style@dashed", "identifiers@letters", "sub-framing@inset (PiP)", "enclosing@circle/rectangle", "sub-framing@magnification lens", "point of view@3rd person", "point of view@1st person"], "UIST18_paper5-Figure2-1.png": ["hue@colors", "number of frames@multi frames", "point of view@3rd person", "line style@dashed", "lines and arrows@direction", "type@text", "identifiers@title", "element@color highlight", "realism@simplistic", "grouping and linking@color grouping"], "UIST18_paper19-Figure2-1.png": ["effects@stroboscopic", "measure@Text indicator", "hue@colors", "grouping and linking@identifiers grouping", "line style@dashed", "lines and arrows@trajectories", "realism@simplistic", "point of view@top", "number of frames@one frame", "measure@Arrows", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper42-Figure9-1.png": ["hue@colors", "element@color highlight", "point of view@UI only", "UI@rendering", "number of frames@one frame"], "UIST18_paper19-Figure9-1.png": ["hue@colors", "type@photo", "number of frames@multi frames", "line style@dashed", "lines and arrows@direction", "type@text", "UI@rendering", "number of frames@one frame", "enclosing@circle/rectangle", "sub-framing@UI overlay", "sub-framing@magnification lens", "color@transparency", "point of view@1st person", "element@color highlight", "point of view@top", "grouping and linking@color grouping", "sub-framing@inset (PiP)"], "CHI18_paper599-Figure8-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "identifiers@letters", "sub-framing@juxtaposition", "point of view@3rd person"], "UIST18_paper19-Figure12-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "identifiers@letters", "UI@rendering", "sub-framing@UI overlay", "point of view@UI only"], "UIST18_paper757-Figure1-1.png": ["line style@width", "hue@colors", "type@photo", "color@transparency", "line style@dashed", "point of view@1st person", "type@text", "realism@simplistic", "number of frames@one frame", "grouping and linking@color grouping", "element@color highlight", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper89-Figure3-1.png": ["hue@colors", "number of frames@multi frames", "type@photo", "identifiers@letters", "UI@rendering", "point of view@3rd person", "point of view@1st person", "point of view@UI only"], "UIST18_paper637-Figure6-1.png": ["hue@colors", "number of frames@multi frames", "point of view@UI only", "UI@rendering", "type@clipart/icon", "identifiers@letters"]}, "coding_dict": {"CHI18_paper107-Figure2-1.png": ["point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "lines and arrows@trajectories", "purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "UIST18_paper45-Figure4-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "tracking based@inside out", "device based@AR", "visual@head mounted displays", "identifiers@letters", "sub-framing@UI overlay", "Situation@Private indoors"], "CHI18_paper184-Figure10-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@writing/drawing", "discrete@Pointing", "Interaction type@computer interaction ", "Interaction type@pen interaction", "device based@tangible objects", "device based@desktop devices", "visual@computer displays", "device based@mobile devices", "Interaction type@touch interaction", "visual@mobile displays"], "Ubicomp18_paper164-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@inset (PiP)", "type@photo", "hue@colors", "dynamic@Contact shapes", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper218-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "line style@width", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "continuous@translating", "Interaction type@mid-air interaction"], "CHI18_paper558-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "UI@rendering", "type@photo", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "body part@full body", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in", "Situation@Private indoors"], "CHI18_paper477-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@magnification lens", "sub-framing@UI embedded", "UI@rendering", "realism@simplistic", "hue@colors", "type@clipart/icon", "region@color area", "lines and arrows@transfer", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "grouping and linking@text annotation", "grouping and linking@identifiers grouping"], "CHI18_paper202-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "identifiers@title", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper437-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "identifiers@numbers", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture"], "CHI18_paper411-Figure3-1.png": ["point of view@1st person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "hue@colors", "lines and arrows@trajectories", "lines and arrows@direction", "purpose@design space", "time@still", "activity@2D/3D creation", "identifiers@letters", "device based@AR", "visual@head mounted displays"], "CHI18_paper18-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@clipart/icon", "hue@colors", "lines and arrows@projection", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction"], "UIST18_paper19-Figure10-1.png": ["point of view@1st person", "point of view@top", "number of frames@multi frames", "sub-framing@UI overlay", "realism@simplistic", "type@photo", "color@transparency", "hue@colors", "line style@dashed", "line style@width", "region@color area", "element@color highlight", "grouping and linking@color grouping", "effects@stroboscopic", "lines and arrows@trajectories", "identifiers@letters", "purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "body part@full body", "continuous@translating", "device based@AR", "visual@head mounted displays", "Situation@Public indoors", "Interaction type@mid-air interaction", "Interaction type@distal interaction"], "UIST18_paper335-Figure7-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@scaling", "Interaction type@touch interaction", "Interaction type@on body interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays", "Situation@Desktop"], "CHI18_paper198-Figure9-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper87-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@tangible objects", "tracking based@outside in", "Situation@Private indoors"], "UIST18_paper711-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "type@photo", "hue@colors", "measure@Arrows", "measure@Text indicator", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@head", "discrete@Pointing", "Interaction type@on body interaction", "tracking based@inside out", "device based@VR", "visual@head mounted displays", "grouping and linking@text annotation"], "CHI18_paper234-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "UI@rendering", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "grouping and linking@color grouping", "purpose@interaction sequence", "time@still", "number@Solo-user", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper132-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "type@text", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@computer interaction ", "device based@desktop devices", "device based@mobile devices", "Interaction type@touch interaction", "visual@computer displays", "grouping and linking@text annotation", "Situation@Desktop"], "UIST18_paper745-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "Ubicomp18_paper162-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@title", "type@text", "identifiers@letters", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper913-Figure12-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@mid-air interaction", "device based@AR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper19-Figure2-1.png": ["number of frames@multi frames", "realism@simplistic", "hue@colors", "identifiers@letters", "grouping and linking@color grouping", "lines and arrows@direction", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@rotating", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "Interaction type@mid-air interaction", "point of view@1st person"], "CHI18_paper529-Figure6-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "color@transparency", "hue@colors", "realism@simplistic", "grouping and linking@color grouping", "identifiers@letters", "lines and arrows@transfer", "purpose@interaction sequence", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CSCW18_paper151-Figure10-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@monochrome", "sub-framing@UI embedded", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "purpose@interaction sequence", "time@still"], "CHI18_paper210-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@realistic", "hue@monochrome", "identifiers@title", "type@text", "purpose@design space", "time@still", "number@Multi-users", "body part@full body", "discrete@Key press", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@computer interaction ", "Interaction type@on body interaction", "Interaction type@distal interaction", "device based@desktop devices", "tracking based@outside in", "visual@computer displays", "visual@large displays"], "CHI18_paper248-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "UI@rendering", "type@photo", "color@transparency", "hue@colors", "dynamic@Contact shapes", "element@color highlight", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper529-Figure12-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper634-Figure12-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@photo", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction", "lines and arrows@transfer", "dynamic@Contact shapes", "identifiers@numbers", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper429-Figure1-1.png": ["type@clipart/icon", "realism@simplistic", "point of view@3rd person", "number of frames@one frame", "sub-framing@magnification lens", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "grouping and linking@grouping arrows", "purpose@interactive system", "time@still", "activity@communication", "number@Solo-user", "body part@full body", "Interaction type@on body interaction", "output modality@haptic", "grouping and linking@text annotation"], "CHI18_paper362-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper360-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@text", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "identifiers@letters"], "CHI18_paper251-Figure1-1.png": ["point of view@top", "point of view@UI only", "number of frames@one frame", "sub-framing@UI embedded", "sub-framing@magnification lens", "type@photo", "realism@simplistic", "hue@colors", "purpose@interaction sequence", "type@text", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper150-Figure23-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "UI@rendering", "point of view@UI only", "effects@stroboscopic", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "output modality@haptic", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "UIST18_paper825-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "region@color area", "type@text", "grouping and linking@color grouping", "dynamic@Contact shapes", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "Interaction type@pen interaction", "device based@tangible objects", "grouping and linking@text annotation"], "UIST18_paper5-Figure10-1.png": ["point of view@1st person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@mid-air interaction", "tracking based@outside in", "visual@head mounted displays", "output modality@haptic", "device based@VR", "point of view@UI only"], "CHI18_paper515-Figure7-1.png": ["point of view@1st person", "point of view@top", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@tangible interaction", "device based@AR", "device based@tangible objects", "visual@head mounted displays"], "CHI18_paper539-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@distal interaction", "device based@mobile devices", "visual@large displays"], "CHI18_paper291-Figure2-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "grouping and linking@color grouping", "identifiers@letters", "lines and arrows@direction", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "purpose@design space", "purpose@interaction sequence", "time@moving", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@mobile devices", "visual@mobile displays", "effects@stroboscopic"], "UIST18_paper485-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper653-Figure5-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@rendering", "type@photo", "type@text", "hue@colors", "identifiers@title", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "tracking based@outside in", "device based@VR", "visual@large displays", "visual@head mounted displays", "point of view@UI only"], "CHI18_paper634-Figure9-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "type@photo", "hue@colors", "lines and arrows@direction", "dynamic@Contact shapes", "identifiers@letters", "identifiers@numbers", "purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper446-Figure10-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "color@transparency", "line style@width", "line style@dashed", "lines and arrows@direction", "effects@waves", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@AR", "output modality@haptic", "visual@head mounted displays"], "Ubicomp18_paper170-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "body part@upper body", "sound based@speech input", "hue@colors"], "CSCW18_paper128-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "effects@waves", "purpose@design space", "time@still", "activity@communication", "activity@production", "number@Multi-users", "body part@full body", "body part@upper body", "point of view@overshoulder 3/4", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@pen interaction", "visual@large displays", "visual@mobile displays", "device based@mobile devices", "device based@large surfaces"], "CHI18_paper529-Figure8-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "color@transparency", "identifiers@letters", "lines and arrows@direction", "dynamic@Contact shapes", "element@color highlight", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper160-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "visual@lights", "visual@projected displays"], "CHI18_paper81-Figure1-1.png": ["point of view@overshoulder 3/4", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "realism@simplistic", "type@data visualization", "type@photo", "UI@drawing", "hue@colors", "enclosing@circle/rectangle", "grouping and linking@identifiers grouping", "type@text", "lines and arrows@direction", "identifiers@letters", "purpose@design space", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@head", "body part@upper body", "Specific part@hand", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@AR", "device based@controllers", "visual@head mounted displays"], "UIST18_paper839-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "effects@stroboscopic", "identifiers@letters", "purpose@interactive system", "purpose@design space", "time@still", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays", "output modality@haptic", "point of view@1st person", "point of view@UI only"], "UIST18_paper53-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "visual@mobile displays"], "CHI18_paper320-Figure11-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@controllers interaction", "Interaction type@on body interaction", "tracking based@outside in", "device based@VR", "device based@controllers", "visual@head mounted displays", "output modality@haptic", "point of view@UI only"], "UIST18_paper825-Figure1-1.png": ["point of view@top", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "enclosing@circle/rectangle", "element@color highlight", "lines and arrows@direction", "line style@dashed", "identifiers@letters", "region@color area", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@writing/drawing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "Interaction type@pen interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper613-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "purpose@interactive system", "time@still", "activity@medical", "Specific part@hand", "visual@lights", "visual@mobile displays"], "CHI18_paper89-Figure14-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "line style@width", "lines and arrows@direction", "identifiers@letters", "purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@full body"], "CHI18_paper76-Figure3-1.png": ["number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "anonymization@blur", "purpose@interaction sequence", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@scaling", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "point of view@3rd person"], "CHI18_paper86-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "lines and arrows@direction", "lines and arrows@projection", "dynamic@Contact shapes", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@VR", "visual@head mounted displays", "output modality@haptic", "device based@controllers", "point of view@UI only"], "CHI18_paper450-Figure10-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@touch interaction", "device based@mobile devices", "device based@tangible objects", "output modality@haptic", "visual@mobile displays"], "CHI18_paper202-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "enclosing@circle/rectangle", "lines and arrows@direction", "grouping and linking@color grouping", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper513-Figure9-1.png": ["point of view@overshoulder 3/4", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper368-Figure7-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@data visualization", "hue@colors", "color@transparency", "grouping and linking@color grouping", "lines and arrows@direction", "effects@stroboscopic", "effects@waves", "type@clipart/icon", "type@text", "purpose@interaction sequence", "time@moving", "number@Multi-users", "Specific part@head", "discrete@Pointing", "Interaction type@gaze interaction", "device based@mobile devices", "grouping and linking@text annotation"], "UIST18_paper675-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@trajectories", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper46-Figure4-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@gaze interaction", "Interaction type@distal interaction", "Interaction type@tangible interaction", "device based@VR", "device based@AR", "device based@tangible objects", "visual@head mounted displays", "point of view@3rd person"], "CHI18_paper218-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "enclosing@exact contour line", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "discrete@Key press", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@controllers", "tracking based@outside in", "grouping and linking@text annotation"], "CHI18_paper42-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "UI@rendering", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "lines and arrows@trajectories", "lines and arrows@projection", "time@moving", "effects@stroboscopic", "purpose@interaction sequence", "number@Solo-user", "body part@full body", "continuous@rotating", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays", "grouping and linking@identifiers grouping"], "CHI18_paper185-Figure11-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "lines and arrows@trajectories", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@pen interaction", "device based@large surfaces", "visual@large displays"], "UIST18_paper365-Figure5-1.png": ["point of view@1st person", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@magnification lens", "type@photo", "type@clipart/icon", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "grouping and linking@grouping arrows", "effects@waves", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@writing/drawing", "continuous@translating", "Interaction type@touch interaction", "Interaction type@pen interaction", "Interaction type@tangible interaction", "Interaction type@on body interaction", "tracking based@inside out", "output modality@haptic", "grouping and linking@text annotation"], "Ubicomp18_paper184-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "audible@sound based", "visual@head mounted displays", "Situation@Public/private transport"], "CHI18_paper132-Figure1-1.png": ["point of view@3rd person", "point of view@top", "point of view@UI only", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "line style@width", "region@color area", "dynamic@Contact shapes", "element@color highlight", "lines and arrows@transfer", "effects@waves", "effects@stroboscopic", "type@data visualization", "identifiers@letters", "identifiers@title", "sub-framing@juxtaposition", "purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@scrolling", "Interaction type@touch interaction", "device based@mobile devices", "output modality@haptic", "visual@mobile displays"], "CHI18_paper378-Figure3-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@one frame", "type@clipart/icon", "type@data visualization", "realism@simplistic", "hue@monochrome", "dynamic@Contact shapes", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "Specific part@foot", "discrete@Pointing", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "audible@sound based"], "CHI18_paper288-Figure1-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "type@photo", "sub-framing@UI embedded", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@gaze interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper310-Figure1-1.png": ["sub-framing@juxtaposition", "point of view@3rd person", "number of frames@one frame", "sub-framing@magnification lens", "type@photo", "background@removed", "hue@colors", "color@transparency", "line style@dashed", "enclosing@circle/rectangle", "effects@stroboscopic", "type@text", "lines and arrows@direction", "identifiers@title", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "Interaction type@mid-air interaction", "Interaction type@gaze interaction", "Interaction type@controllers interaction", "sound based@speech input", "device based@controllers", "tracking based@outside in"], "CHI18_paper27-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "lines and arrows@direction", "dynamic@Contact shapes", "identifiers@letters", "time@still", "purpose@design space", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper185-Figure7-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@production", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper497-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "identifiers@title", "purpose@design space", "time@still", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper335-Figure3-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@touch interaction", "tracking based@inside out", "device based@mobile devices", "device based@laptop devices", "visual@mobile displays", "visual@computer displays"], "CHI18_paper188-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@data visualization", "hue@colors", "identifiers@letters", "purpose@interactive system", "purpose@interaction sequence", "time@still", "number@Solo-user", "activity@fabrication", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "type@text", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper177-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "type@text", "hue@colors", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "grouping and linking@text annotation"], "CHI18_paper21-Figure2-1.png": ["point of view@top", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "grouping and linking@color grouping", "lines and arrows@direction", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "Interaction type@mid-air interaction", "grouping and linking@text annotation"], "CHI18_paper547-Figure2-1.png": ["point of view@1st person", "point of view@3rd person", "point of view@overshoulder 3/4", "point of view@top", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "region@color area", "line style@dashed", "lines and arrows@projection", "lines and arrows@direction", "grouping and linking@color grouping", "dynamic@Contact shapes", "effects@waves", "effects@stroboscopic", "element@color highlight", "identifiers@letters", "purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "body part@upper body", "discrete@Pointing", "continuous@rotating", "continuous@scaling", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "device based@mobile devices", "device based@large surfaces", "visual@large displays", "grouping and linking@text annotation"], "UIST18_paper737-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "line style@dashed", "lines and arrows@direction", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "output modality@haptic"], "UIST18_paper557-Figure1-1.png": ["point of view@1st person", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@juxtaposition", "type@photo", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "activity@communication", "activity@entertainment", "Specific part@hand", "body part@upper body", "discrete@Pointing", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "audible@speech output"], "CHI18_paper579-Figure9-1.png": ["point of view@1st person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "hue@colors", "color@transparency", "lines and arrows@projection", "identifiers@letters", "purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "discrete@Pointing", "Interaction type@tangible interaction", "device based@AR", "device based@tangible objects", "activity@fabrication", "visual@head mounted displays"], "UIST18_paper321-Figure12-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper436-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@drawing", "realism@simplistic", "hue@grayscale", "line style@dashed", "lines and arrows@projection", "dynamic@Contact shapes", "identifiers@letters", "purpose@interaction sequence", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@head", "discrete@Pointing", "Interaction type@gaze interaction", "device based@large surfaces", "visual@large displays", "Situation@Public/private transport"], "CHI18_paper89-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "line style@width", "effects@waves", "effects@stroboscopic", "hue@grayscale", "lines and arrows@trajectories", "identifiers@letters", "purpose@design space", "time@moving", "number@Solo-user", "body part@full body", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "CHI18_paper241-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@grayscale", "line style@dashed", "line style@width", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "visual@head mounted displays"], "Ubicomp18_paper162-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "type@text", "hue@colors", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "activity@2D/3D creation", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "continuous@writing/drawing", "continuous@scrolling", "continuous@translating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@controllers", "device based@large surfaces", "visual@large displays"], "CHI18_paper634-Figure11-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "dynamic@Contact shapes", "region@color area", "realism@realistic", "identifiers@letters", "identifiers@numbers", "purpose@interactive system", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CSCW18_paper192-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "audible@sound based", "visual@mobile displays"], "CHI18_paper340-Figure3-1.png": ["tracking based@inside out", "device based@VR", "device based@controllers", "visual@head mounted displays", "Interaction type@mid-air interaction", "discrete@Pointing", "discrete@Symbolic gesture", "body part@upper body", "point of view@1st person", "number of frames@multi frames", "point of view@3rd person", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "hue@colors", "identifiers@title", "identifiers@letters", "time@still", "purpose@design space", "point of view@UI only"], "CHI18_paper629-Figure1-1.png": ["audible@sound based", "visual@projected displays", "device based@large surfaces", "Interaction type@touch interaction", "discrete@Pointing", "body part@upper body", "number@Solo-user", "activity@data manipulation", "time@still", "purpose@interactive system", "hue@colors", "type@photo", "number of frames@one frame", "point of view@3rd person"], "CHI18_paper569-Figure27-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "lines and arrows@direction", "identifiers@letters", "effects@stroboscopic", "time@moving", "purpose@design space", "number@Solo-user", "Specific part@fingers", "continuous@translating", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper89-Figure15-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "line style@width", "hue@monochrome", "effects@waves", "lines and arrows@trajectories", "line style@dashed", "purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper529-Figure3-1.png": ["point of view@3rd person", "point of view@1st person", "sub-framing@UI embedded", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper241-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "UI@drawing", "type@photo", "region@color area", "hue@colors", "identifiers@letters", "purpose@design space", "activity@entertainment", "time@still", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "discrete@Symbolic gesture", "device based@VR", "device based@controllers", "visual@head mounted displays"], "UIST18_paper675-Figure4-1.png": ["point of view@1st person", "sub-framing@UI embedded", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@fingers", "continuous@writing/drawing", "Interaction type@pen interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper401-Figure3-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@communication", "activity@entertainment", "visual@projected displays"], "UIST18_paper853-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "line style@dashed", "color@transparency", "lines and arrows@transfer", "effects@stroboscopic", "lines and arrows@trajectories", "lines and arrows@projection", "element@color highlight", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Multi-users", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper98-Figure4-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@communication", "activity@medical", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "Ubicomp18_paper174-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "line style@dashed", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@letters", "purpose@design space", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "Interaction type@gaze interaction", "device based@desktop devices", "visual@computer displays", "tracking based@outside in", "point of view@UI only"], "CHI18_paper86-Figure12-1.png": ["activity@fabrication", "point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "hue@colors", "type@text", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@controllers interaction", "device based@VR", "output modality@haptic", "visual@head mounted displays", "point of view@UI only", "grouping and linking@text annotation"], "CHI18_paper219-Figure2-1.png": ["point of view@top", "number of frames@one frame", "type@text", "UI@drawing", "hue@colors", "grouping and linking@color grouping", "line style@dashed", "lines and arrows@trajectories", "grouping and linking@identifiers grouping", "purpose@interactive system", "time@moving", "number@Solo-user", "body part@upper body", "device based@AR", "visual@head mounted displays"], "CHI18_paper54-Figure5-1.png": ["point of view@1st person", "sub-framing@magnification lens", "sub-framing@UI embedded", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "line style@width", "enclosing@circle/rectangle", "background@removed", "identifiers@letters", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@head", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays", "device based@mobile devices", "type@text", "point of view@3rd person", "point of view@UI only", "grouping and linking@text annotation"], "CHI18_paper558-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "element@color highlight", "grouping and linking@identifiers grouping", "grouping and linking@color grouping", "line style@dashed", "measure@Arrows", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Key press", "Interaction type@computer interaction ", "tracking based@outside in", "device based@desktop devices"], "CHI18_paper251-Figure2-1.png": ["point of view@UI only", "type@text", "UI@rendering", "type@clipart/icon", "hue@colors", "dynamic@Contact shapes", "lines and arrows@direction", "time@still", "purpose@interactive system", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "number of frames@one frame", "grouping and linking@text annotation"], "UIST18_paper867-Figure2-1.png": ["point of view@1st person", "number of frames@one frame", "UI@rendering", "hue@colors", "purpose@interactive system", "time@still", "activity@medical", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays", "point of view@UI only"], "CHI18_paper380-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@trajectories", "lines and arrows@projection", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "continuous@writing/drawing", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper5-Figure8-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper521-Figure1-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "purpose@interaction sequence", "time@still", "number@Solo-user", "Interaction type@controllers interaction", "Interaction type@distal interaction", "discrete@Pointing", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper90-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "sub-framing@UI overlay", "UI@rendering", "type@photo", "color@transparency", "hue@colors", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Multi-users", "body part@full body", "visual@head mounted displays", "background@removed", "device based@AR"], "CHI18_paper173-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "Interaction type@touch interaction", "device based@mobile devices", "visual@projected displays"], "CHI18_paper363-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "line style@width", "enclosing@exact contour line", "element@color highlight", "grouping and linking@color grouping", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Pointing", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@VR", "device based@controllers", "visual@head mounted displays", "grouping and linking@text annotation"], "CHI18_paper529-Figure11-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "dynamic@Contact shapes", "lines and arrows@trajectories", "element@color highlight", "identifiers@letters", "purpose@interaction sequence", "time@still", "discrete@Pointing", "Interaction type@touch interaction"], "UIST18_paper321-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "Interaction type@touch interaction", "visual@mobile displays", "device based@mobile devices"], "UIST18_paper697-Figure3-1.png": ["type@clipart/icon", "realism@simplistic", "hue@colors", "point of view@3rd person", "element@color highlight", "lines and arrows@direction", "identifiers@letters", "region@color area", "number of frames@multi frames", "purpose@design space", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@on body interaction", "time@still"], "CHI18_paper613-Figure1-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@magnification lens", "realism@simplistic", "type@photo", "hue@colors", "element@color highlight", "enclosing@circle/rectangle", "line style@dashed", "purpose@interactive system", "purpose@interaction sequence", "time@still", "activity@medical", "number@Solo-user", "body part@full body", "Specific part@fingers", "Interaction type@tangible interaction", "device based@controllers", "visual@lights"], "UIST18_paper745-Figure2-1.png": ["type@data visualization", "realism@simplistic", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@letters", "type@text", "grouping and linking@identifiers grouping", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "grouping and linking@text annotation"], "CHI18_paper151-Figure4-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@fabrication", "Specific part@fingers", "discrete@Key press", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper529-Figure13-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper199-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "line style@dashed", "element@color highlight", "type@text", "grouping and linking@color grouping", "measure@Arrows", "lines and arrows@direction", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "grouping and linking@text annotation"], "CHI18_paper219-Figure14-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "device based@AR", "type@photo", "UI@rendering", "color@transparency", "hue@colors", "enclosing@circle/rectangle", "identifiers@letters", "lines and arrows@direction", "element@color highlight", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Interaction type@distal interaction", "visual@head mounted displays", "point of view@UI only"], "CHI18_paper610-Figure1-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@clipart/icon", "hue@colors", "color@transparency", "effects@stroboscopic", "purpose@design space", "time@still", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper593-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays"], "CHI18_paper380-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "realism@simplistic", "type@photo", "hue@colors", "region@color area", "element@color highlight", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@entertainment", "activity@communication", "number@Multi-users", "Specific part@hand", "continuous@deforming", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "enclosing@exact contour line", "identifiers@letters", "identifiers@numbers", "identifiers@title"], "CHI18_paper291-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "identifiers@letters", "lines and arrows@direction", "purpose@design space", "activity@fabrication", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@scaling", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "device based@tangible objects", "Interaction type@tangible interaction", "discrete@Key press"], "CHI18_paper362-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "activity@medical", "tracking based@inside out", "visual@mobile displays"], "CHI18_paper199-Figure13-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "number@Solo-user", "activity@data manipulation", "Specific part@hand", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper436-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@grayscale", "background@removed", "lines and arrows@transfer", "type@data visualization", "purpose@interaction sequence", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "tracking based@outside in", "Interaction type@gaze interaction", "lines and arrows@projection", "line style@dashed", "device based@desktop devices", "visual@computer displays"], "UIST18_paper19-Figure15-1.png": ["point of view@1st person", "point of view@UI only", "sub-framing@UI overlay", "number of frames@multi frames", "point of view@3rd person", "realism@simplistic", "type@photo", "UI@rendering", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "element@color highlight", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "device based@AR", "visual@head mounted displays"], "CHI18_paper634-Figure10-1.png": ["point of view@1st person", "sub-framing@UI embedded", "point of view@3rd person", "realism@realistic", "type@photo", "number of frames@multi frames", "hue@colors", "region@color area", "dynamic@Contact shapes", "lines and arrows@direction", "purpose@interactive system", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "discrete@Symbolic gesture"], "CHI18_paper89-Figure8-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@design space", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Specific part@hand", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "point of view@UI only"], "CHI18_paper539-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "identifiers@title", "activity@entertainment", "time@still", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper634-Figure3-1.png": ["point of view@top", "realism@realistic", "UI@drawing", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "element@color highlight", "dynamic@Contact shapes", "lines and arrows@transfer", "lines and arrows@trajectories", "purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@scaling", "continuous@scrolling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "UIST18_paper499-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "color@transparency", "region@color area", "enclosing@circle/rectangle", "purpose@interactive system", "time@still", "number@Solo-user", "body part@lower body", "line style@width", "grouping and linking@text annotation"], "CHI18_paper11-Figure5-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@text", "type@clipart/icon", "hue@grayscale", "line style@width", "element@color highlight", "dynamic@Contact shapes", "identifiers@title", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "audible@speech output"], "UIST18_paper913-Figure10-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "output modality@haptic"], "CHI18_paper564-Figure1-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@one frame", "sub-framing@inset (PiP)", "sub-framing@magnification lens", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "type@text", "region@color area", "line style@width", "element@color highlight", "enclosing@circle/rectangle", "dynamic@Contact shapes", "identifiers@letters", "lines and arrows@transfer", "grouping and linking@color grouping", "grouping and linking@grouping arrows", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "visual@large displays"], "CHI18_paper567-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper579-Figure8-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "color@transparency", "lines and arrows@projection", "region@color area", "purpose@interactive system", "time@still", "activity@2D/3D creation", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Specific part@head", "discrete@Pointing", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@VR", "device based@controllers", "visual@head mounted displays", "point of view@UI only"], "UIST18_paper127-Figure16-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "activity@2D/3D creation", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "tracking based@outside in", "visual@computer displays"], "CHI18_paper150-Figure3-1.png": ["point of view@top", "number of frames@multi frames", "realism@simplistic", "type@photo", "background@removed", "hue@colors", "color@transparency", "type@text", "lines and arrows@trajectories", "effects@stroboscopic", "element@color highlight", "identifiers@letters", "identifiers@title", "time@moving", "purpose@interaction sequence", "purpose@interactive system", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@tangible objects", "Interaction type@tangible interaction", "output modality@haptic", "visual@head mounted displays", "device based@VR"], "CHI18_paper439-Figure6-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper241-Figure4-1.png": ["line style@width", "point of view@3rd person", "number of frames@one frame", "line style@dashed", "enclosing@exact contour line", "realism@simplistic", "hue@grayscale", "lines and arrows@direction", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "discrete@Symbolic gesture", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper339-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "identifiers@title", "purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "continuous@deforming", "continuous@scrolling", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper927-Figure5-1.png": ["sub-framing@juxtaposition", "point of view@1st person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@mid-air interaction", "discrete@Symbolic gesture", "tracking based@inside out", "device based@VR", "output modality@haptic", "visual@head mounted displays", "point of view@3rd person", "point of view@UI only"], "CHI18_paper45-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI overlay", "type@text", "hue@colors", "element@color highlight", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "body part@upper body", "discrete@Pointing", "device based@AR", "visual@head mounted displays"], "UIST18_paper499-Figure9-1.png": ["point of view@1st person", "point of view@3rd person", "sub-framing@juxtaposition", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "region@color area", "enclosing@exact contour line", "background@grayed out/blurred", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@controllers interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "point of view@UI only"], "CHI18_paper42-Figure1-1.png": ["point of view@3rd person", "point of view@top", "number of frames@one frame", "sub-framing@inset (PiP)", "realism@simplistic", "sub-framing@UI embedded", "UI@drawing", "type@clipart/icon", "hue@colors", "region@color area", "element@color highlight", "measure@Arrows", "measure@Text indicator", "purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Solo-user", "body part@upper body", "Specific part@head", "discrete@Pointing", "device based@VR", "Interaction type@controllers interaction", "device based@controllers", "visual@head mounted displays"], "CHI18_paper638-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@text", "type@clipart/icon", "hue@colors", "enclosing@exact contour line", "enclosing@circle/rectangle", "element@color highlight", "identifiers@title", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper622-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@production", "number@Multi-users", "body part@upper body", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays"], "UIST18_paper347-Figure3-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@inset (PiP)", "realism@simplistic", "UI@rendering", "type@text", "hue@colors", "region@color area", "dynamic@Contact shapes", "identifiers@letters", "identifiers@title", "purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Key press", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "Interaction type@computer interaction ", "tracking based@outside in", "device based@desktop devices", "visual@computer displays"], "CHI18_paper18-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "line style@dashed", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "Interaction type@mid-air interaction"], "CSCW18_paper185-Figure4-1.png": ["point of view@top", "sub-framing@UI embedded", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "continuous@scrolling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper209-Figure1-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "Interaction type@mid-air interaction", "device based@AR", "visual@head mounted displays", "point of view@UI only"], "CHI18_paper628-Figure6-1.png": ["point of view@top", "number of frames@one frame", "type@text", "realism@simplistic", "type@data visualization", "hue@colors", "element@color highlight", "lines and arrows@transfer", "grouping and linking@color grouping", "dynamic@Contact shapes", "region@color area", "line style@dashed", "identifiers@title", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction"], "CHI18_paper248-Figure6-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "type@text", "hue@colors", "color@transparency", "element@color highlight", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "grouping and linking@text annotation"], "CHI18_paper5-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper245-Figure12-1.png": ["number of frames@multi frames", "type@photo", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "lines and arrows@projection", "dynamic@Contact shapes", "identifiers@title", "identifiers@letters", "type@text", "purpose@interaction sequence", "purpose@design space", "time@still", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@scaling", "continuous@scrolling", "continuous@rotating", "continuous@translating", "Interaction type@distal interaction", "tracking based@outside in", "device based@large surfaces", "visual@large displays", "point of view@1st person", "sub-framing@UI embedded"], "UIST18_paper275-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "purpose@interactive system", "effects@stroboscopic", "time@moving", "activity@2D/3D creation", "number@Solo-user", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in"], "CHI18_paper289-Figure1-1.png": ["anonymization@blur", "point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Multi-users", "body part@full body", "continuous@translating", "continuous@rotating", "discrete@Pointing", "Interaction type@tangible interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "point of view@UI only"], "UIST18_paper839-Figure8-1.png": ["point of view@1st person", "UI@rendering", "point of view@UI only", "number of frames@multi frames", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "device based@VR", "visual@head mounted displays"], "CHI18_paper385-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@clipart/icon", "lines and arrows@transfer", "region@color area", "identifiers@title", "type@text", "effects@waves", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "grouping and linking@identifiers grouping"], "CHI18_paper291-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@direction", "lines and arrows@trajectories", "grouping and linking@color grouping", "line style@dashed", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@large surfaces", "output modality@haptic", "visual@large displays"], "CHI18_paper298-Figure3-1.png": ["point of view@1st person", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper638-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "type@clipart/icon", "hue@colors", "line style@dashed", "element@color highlight", "enclosing@circle/rectangle", "identifiers@numbers", "purpose@interaction sequence", "effects@waves", "effects@stroboscopic", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "type@text", "type@data visualization", "UI@rendering"], "CHI18_paper164-Figure1-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "type@text", "hue@colors", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper877-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@monochrome", "identifiers@letters", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "realism@simplistic"], "CHI18_paper334-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "identifiers@title", "realism@simplistic", "hue@colors", "enclosing@exact contour line", "effects@stroboscopic", "line style@dashed", "lines and arrows@direction", "lines and arrows@trajectories", "identifiers@letters", "purpose@design space", "number@Solo-user", "Specific part@foot", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "time@moving"], "CHI18_paper81-Figure10-1.png": ["point of view@UI only", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@controllers interaction", "Interaction type@gaze interaction", "discrete@Pointing", "device based@AR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper189-Figure11-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper589-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@monochrome", "type@text", "line style@width", "lines and arrows@direction", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "Interaction type@mid-air interaction"], "CHI18_paper82-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@realistic", "UI@rendering", "hue@colors", "identifiers@numbers", "purpose@interaction sequence", "activity@fabrication", "time@moving", "Interaction type@pen interaction", "Interaction type@tangible interaction", "device based@tangible objects", "continuous@writing/drawing", "discrete@Key press"], "UIST18_paper499-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "color@transparency", "region@color area", "identifiers@letters", "purpose@interactive system", "purpose@design space", "time@still", "number@Solo-user", "body part@lower body", "Interaction type@tangible interaction", "continuous@translating", "tracking based@outside in", "device based@tangible objects", "device based@VR"], "CSCW18_paper192-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interaction sequence", "time@still", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@on body interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper913-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "Interaction type@tangible interaction", "output modality@haptic", "visual@computer displays", "device based@desktop devices"], "CHI18_paper372-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "visual@mobile displays", "visual@large displays", "sub-framing@UI embedded"], "CHI18_paper123-Figure8-1.png": ["point of view@UI only", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "region@color area", "type@data visualization", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "continuous@scrolling", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "UIST18_paper765-Figure1-1.png": ["point of view@overshoulder 3/4", "sub-framing@UI embedded", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@text", "hue@colors", "element@color highlight", "purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@on body interaction", "output modality@haptic", "device based@mobile devices"], "CHI18_paper336-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "lines and arrows@direction", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper745-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper499-Figure15-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "region@color area", "identifiers@letters", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "continuous@translating", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@tangible objects", "device based@controllers", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "discrete@Pointing"], "CHI18_paper241-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper89-Figure16-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "device based@tangible objects", "visual@head mounted displays", "device based@VR"], "CHI18_paper558-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "line style@dashed", "hue@colors", "element@color highlight", "lines and arrows@trajectories", "purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "body part@full body", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper87-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "hue@colors", "element@color highlight", "enclosing@circle/rectangle", "lines and arrows@trajectories", "purpose@interactive system", "time@still", "number@Solo-user", "continuous@writing/drawing"], "CHI18_paper245-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "lines and arrows@direction", "region@color area", "element@color highlight", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@mid-air interaction"], "CHI18_paper150-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "point of view@UI only", "UI@rendering", "hue@colors", "type@photo", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "device based@VR", "visual@head mounted displays"], "CHI18_paper644-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "color@transparency", "hue@colors", "sub-framing@UI overlay", "UI@rendering", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "discrete@Pointing", "Interaction type@controllers interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper499-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "line style@width", "hue@colors", "color@transparency", "region@color area", "purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "Interaction type@controllers interaction", "Interaction type@tangible interaction", "device based@VR", "device based@tangible objects", "visual@head mounted displays", "tracking based@outside in"], "CHI18_paper654-Figure5-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "lines and arrows@projection", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays", "identifiers@letters"], "UIST18_paper867-Figure5-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@one frame", "UI@rendering", "hue@colors", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@scrolling", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "CHI18_paper374-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "UI@rendering", "type@text", "hue@grayscale", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper336-Figure14-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@data visualization", "type@photo", "hue@colors", "realism@simplistic", "identifiers@letters", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "device based@tangible objects", "Interaction type@tangible interaction", "output modality@haptic"], "CHI18_paper19-Figure3-1.png": ["Interaction type@distal interaction", "Interaction type@touch interaction", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "type@text", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "enclosing@circle/rectangle", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@full body", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays", "visual@large displays", "grouping and linking@text annotation"], "UIST18_paper913-Figure11-1.png": ["point of view@3rd person", "sub-framing@UI embedded", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@mobile devices", "output modality@haptic"], "CHI18_paper224-Figure4-1.png": ["point of view@UI only", "number of frames@one frame", "color@transparency", "hue@colors", "line style@dashed", "lines and arrows@transfer", "UI@rendering", "dynamic@Contact shapes", "lines and arrows@trajectories", "purpose@interaction sequence", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays", "device based@large surfaces"], "UIST18_paper927-Figure7-1.png": ["sub-framing@inset (PiP)", "sub-framing@UI embedded", "point of view@1st person", "point of view@top", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "lines and arrows@direction", "time@still", "purpose@design space", "number@Solo-user", "body part@full body", "device based@VR", "Interaction type@mid-air interaction", "continuous@rotating", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper78-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "device based@VR", "tracking based@outside in", "tracking based@inside out", "visual@head mounted displays"], "CHI18_paper411-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "effects@waves", "lines and arrows@trajectories", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@rotating", "discrete@Key press", "Interaction type@tangible interaction", "device based@tangible objects", "device based@AR", "visual@projected displays"], "CHI18_paper5-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "visual@lights", "device based@tangible objects", "discrete@Symbolic gesture", "sub-framing@UI embedded"], "CHI18_paper477-Figure3-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@rendering", "type@photo", "hue@colors", "type@clipart/icon", "region@color area", "element@color highlight", "lines and arrows@transfer", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "grouping and linking@identifiers grouping", "type@text", "grouping and linking@text annotation"], "CHI18_paper150-Figure20-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "enclosing@circle/rectangle", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@tangible interaction", "activity@fabrication", "tracking based@outside in", "device based@tangible objects", "output modality@haptic"], "UIST18_paper825-Figure9-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "sub-framing@UI embedded", "type@photo", "hue@colors", "region@color area", "identifiers@letters", "purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@large surfaces", "visual@large displays"], "CHI18_paper201-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "type@text", "realism@simplistic", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "body part@full body", "tracking based@outside in", "device based@controllers", "Interaction type@controllers interaction"], "UIST18_paper19-Figure6-1.png": ["point of view@top", "number of frames@one frame", "realism@simplistic", "grouping and linking@color grouping", "region@color area", "line style@dashed", "measure@Text indicator", "measure@Arrows", "type@text", "hue@colors", "purpose@interactive system", "time@still", "activity@communication", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "device based@mobile devices", "grouping and linking@text annotation", "grouping and linking@identifiers grouping"], "CHI18_paper65-Figure4-1.png": ["type@data visualization", "realism@simplistic", "point of view@top", "number of frames@multi frames", "type@text", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "purpose@design space", "lines and arrows@direction", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "time@still", "identifiers@letters", "identifiers@title"], "CHI18_paper86-Figure13-1.png": ["point of view@UI only", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "type@text", "hue@colors", "dynamic@Contact shapes", "region@color area", "discrete@Pointing", "discrete@Key press", "Interaction type@tangible interaction", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@controllers", "device based@tangible objects", "device based@VR", "visual@head mounted displays", "time@still", "purpose@design space", "activity@data manipulation", "grouping and linking@text annotation"], "CHI18_paper354-Figure18-1.png": ["sub-framing@inset (PiP)", "sub-framing@UI embedded", "number of frames@one frame", "point of view@3rd person", "type@photo", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "identifiers@letters", "identifiers@title", "type@text", "background@removed", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "activity@fabrication", "device based@mobile devices", "visual@mobile displays", "grouping and linking@identifiers grouping", "grouping and linking@text annotation"], "CHI18_paper5-Figure9-1.png": ["point of view@3rd person", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "activity@fabrication", "time@still", "number@Solo-user", "Specific part@hand", "continuous@deforming", "discrete@Pointing", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper547-Figure1-1.png": ["point of view@1st person", "point of view@3rd person", "point of view@top", "number of frames@multi frames", "realism@simplistic", "type@photo", "hue@colors", "color@transparency", "hue@monochrome", "hue@grayscale", "line style@dashed", "region@color area", "element@color highlight", "lines and arrows@direction", "effects@stroboscopic", "lines and arrows@trajectories", "dynamic@Contact shapes", "lines and arrows@projection", "type@text", "purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "body part@upper body", "continuous@translating", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays", "device based@large surfaces", "grouping and linking@text annotation"], "CHI18_paper298-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "grouping and linking@color grouping", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@touch interaction", "device based@large surfaces", "device based@tangible objects", "visual@large displays"], "CHI18_paper603-Figure17-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "region@color area", "grouping and linking@color grouping", "element@color highlight", "lines and arrows@trajectories", "identifiers@letters", "lines and arrows@transfer", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper205-Figure14-1.png": ["point of view@UI only", "type@clipart/icon", "number of frames@one frame", "UI@rendering", "hue@colors", "grouping and linking@identifiers grouping", "lines and arrows@direction", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@touch interaction"], "CHI18_paper374-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "element@color highlight", "lines and arrows@direction", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper401-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "device based@VR", "device based@controllers", "Interaction type@controllers interaction", "Interaction type@distal interaction", "visual@projected displays"], "UIST18_paper65-Figure2-1.png": ["sub-framing@magnification lens", "point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "lines and arrows@transfer", "line style@width", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Specific part@hand", "Specific part@foot", "Interaction type@on body interaction", "tracking based@inside out", "output modality@haptic", "grouping and linking@text annotation"], "CHI18_paper423-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper629-Figure7-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@deforming", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "sub-framing@UI embedded"], "CHI18_paper19-Figure10-1.png": ["point of view@1st person", "sub-framing@UI embedded", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@text", "hue@colors", "enclosing@circle/rectangle", "sub-framing@magnification lens", "lines and arrows@direction", "identifiers@letters", "purpose@interaction sequence", "purpose@design space", "time@still", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays", "device based@large surfaces"], "Ubicomp18_paper185-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "color@transparency", "hue@colors", "effects@stroboscopic", "enclosing@circle/rectangle", "purpose@design space", "time@moving", "number@Solo-user", "body part@full body", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper446-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "region@color area", "type@clipart/icon", "identifiers@letters", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@tangible interaction", "Interaction type@distal interaction", "device based@AR", "device based@tangible objects", "tracking based@outside in", "output modality@haptic", "visual@head mounted displays", "Situation@Private indoors"], "UIST18_paper335-Figure9-1.png": ["point of view@top", "sub-framing@UI embedded", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "discrete@Symbolic gesture", "Interaction type@distal interaction", "Interaction type@touch interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper160-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@distal interaction", "sound based@speech input"], "Ubicomp18_paper161-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@data visualization", "type@photo", "hue@colors", "hue@grayscale", "grouping and linking@color grouping", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "device based@mobile devices", "Interaction type@on body interaction"], "CHI18_paper529-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "hue@colors", "color@transparency", "element@color highlight", "identifiers@letters", "lines and arrows@transfer", "UI@rendering", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper297-Figure3-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@data manipulation", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating", "discrete@Key press", "Interaction type@tangible interaction", "Interaction type@touch interaction", "device based@tangible objects", "visual@mobile displays"], "CHI18_paper86-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "lines and arrows@direction", "identifiers@title", "sub-framing@juxtaposition", "purpose@interaction sequence", "purpose@design space", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic"], "CHI18_paper644-Figure9-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "type@photo", "type@text", "hue@colors", "identifiers@title", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "continuous@rotating", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper446-Figure3-1.png": ["point of view@1st person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "type@photo", "type@text", "hue@colors", "color@transparency", "region@color area", "identifiers@letters", "lines and arrows@trajectories", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "continuous@rotating", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@AR", "tracking based@outside in", "visual@head mounted displays", "grouping and linking@text annotation"], "CHI18_paper529-Figure7-1.png": ["point of view@top", "sub-framing@UI embedded", "number of frames@multi frames", "realism@simplistic", "UI@drawing", "hue@colors", "color@transparency", "element@color highlight", "lines and arrows@direction", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper89-Figure11-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "UI@rendering", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@VR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper210-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "type@text", "purpose@interactive system", "time@still"], "CHI18_paper564-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "enclosing@circle/rectangle", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@letters", "region@color area", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "continuous@translating", "Interaction type@on body interaction", "tracking based@outside in", "visual@computer displays", "Specific part@hand"], "CHI18_paper266-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@text", "hue@colors", "type@photo", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "Interaction type@computer interaction ", "Interaction type@tangible interaction", "device based@desktop devices", "device based@tangible objects", "number@Solo-user"], "CHI18_paper362-Figure4-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper502-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper248-Figure3-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "color@transparency", "hue@colors", "enclosing@circle/rectangle", "lines and arrows@trajectories", "lines and arrows@direction", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@translating", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper117-Figure13-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@writing/drawing", "continuous@translating", "Interaction type@pen interaction", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@large surfaces", "visual@computer displays"], "CHI18_paper281-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors", "effects@waves", "purpose@interactive system", "time@moving", "activity@communication", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in"], "CHI18_paper291-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "UI@drawing", "realism@simplistic", "hue@colors", "type@text", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@title", "line style@dashed", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper411-Figure5-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "enclosing@circle/rectangle", "type@text", "lines and arrows@direction", "purpose@interaction sequence", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@scaling", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers", "visual@projected displays", "grouping and linking@text annotation"], "CHI18_paper638-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "hue@colors", "UI@drawing", "element@color highlight", "enclosing@exact contour line", "effects@waves", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "identifiers@numbers"], "CHI18_paper89-Figure4-1.png": ["point of view@UI only", "number of frames@one frame", "realism@simplistic", "type@text", "hue@monochrome", "line style@dashed", "lines and arrows@direction", "activity@entertainment", "number@Solo-user", "body part@upper body", "grouping and linking@text annotation"], "CHI18_paper162-Figure11-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper237-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "type@text", "hue@colors", "color@transparency", "identifiers@letters", "identifiers@title", "region@color area", "element@color highlight", "effects@stroboscopic", "lines and arrows@trajectories", "type@data visualization", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "grouping and linking@text annotation"], "CHI18_paper541-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@magnification lens", "sub-framing@UI embedded", "sub-framing@inset (PiP)", "UI@drawing", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "dynamic@Contact shapes", "type@clipart/icon", "measure@Arrows", "measure@Text indicator", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@transfer", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@medical", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Key press", "Interaction type@computer interaction ", "device based@tangible objects", "audible@sound based", "output modality@haptic", "identifiers@title", "grouping and linking@text annotation"], "UIST18_paper963-Figure4-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "dynamic@Contact shapes", "element@color highlight", "line style@dashed", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper541-Figure5-1.png": ["point of view@3rd person", "point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "realism@simplistic", "type@clipart/icon", "UI@rendering", "hue@colors", "element@color highlight", "identifiers@title", "dynamic@Contact shapes", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "purpose@design space", "purpose@interaction sequence", "time@moving", "lines and arrows@direction", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "device based@controllers", "Interaction type@controllers interaction", "audible@sound based", "visual@computer displays", "grouping and linking@text annotation"], "CHI18_paper248-Figure9-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@text", "hue@colors", "color@transparency", "enclosing@exact contour line", "region@color area", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "continuous@writing/drawing", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "grouping and linking@text annotation"], "UIST18_paper595-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "lines and arrows@direction", "effects@waves", "dynamic@Contact shapes", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "activity@fabrication", "device based@mobile devices", "background@removed"], "CHI18_paper236-Figure3-1.png": ["anonymization@blur", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "enclosing@circle/rectangle", "grouping and linking@grouping arrows", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "sound based@speech input", "device based@tangible objects", "audible@speech output", "identifiers@letters"], "CHI18_paper433-Figure1-1.png": ["point of view@overshoulder 3/4", "point of view@1st person", "number of frames@multi frames", "type@photo", "sub-framing@UI embedded", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "body part@full body", "visual@mobile displays", "visual@projected displays"], "CHI18_paper508-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "Interaction type@touch interaction", "discrete@Key press", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "tracking based@outside in"], "UIST18_paper725-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "UI@drawing", "hue@colors", "color@transparency", "element@color highlight", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper89-Figure13-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@data visualization", "hue@colors", "purpose@design space", "time@still", "tracking based@outside in"], "UIST18_paper473-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@UI embedded", "type@photo", "realism@simplistic", "hue@colors", "grouping and linking@grouping arrows", "identifiers@letters", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@tangible interaction", "Interaction type@touch interaction", "tracking based@outside in", "device based@tangible objects", "visual@computer displays"], "CHI18_paper237-Figure12-1.png": ["point of view@UI only", "sub-framing@juxtaposition", "type@data visualization", "UI@drawing", "number of frames@one frame", "type@text", "hue@colors", "grouping and linking@color grouping", "identifiers@letters", "effects@waves", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "grouping and linking@text annotation"], "UIST18_paper99-Figure16-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@transfer", "identifiers@letters", "purpose@interaction sequence", "activity@fabrication", "time@moving", "number@Solo-user", "Specific part@hand", "continuous@deforming", "continuous@translating"], "UIST18_paper5-Figure11-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@mid-air interaction", "device based@VR", "output modality@haptic"], "UIST18_paper247-Figure10-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "output modality@haptic"], "CHI18_paper185-Figure9-1.png": ["point of view@UI only", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing"], "CHI18_paper209-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors", "purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "body part@upper body", "discrete@Pointing", "discrete@Key press", "Interaction type@computer interaction ", "device based@AR", "device based@controllers", "Interaction type@controllers interaction", "device based@desktop devices", "visual@computer displays", "visual@head mounted displays"], "CHI18_paper411-Figure10-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "type@text", "identifiers@title", "grouping and linking@color grouping", "lines and arrows@trajectories", "purpose@design space", "time@moving", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "Ubicomp18_paper200-Figure6-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@desktop devices", "device based@mobile devices", "device based@tangible objects", "visual@computer displays"], "CHI18_paper185-Figure12-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "hue@colors", "element@color highlight", "purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@scaling", "discrete@Pointing", "Interaction type@pen interaction"], "CHI18_paper470-Figure3-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@touch interaction", "discrete@Key press"], "CHI18_paper178-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "point of view@top", "sub-framing@UI embedded", "UI@rendering", "type@photo", "realism@simplistic", "hue@colors", "identifiers@letters", "grouping and linking@color grouping", "lines and arrows@transfer", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "visual@mobile displays", "tracking based@outside in"], "CHI18_paper291-Figure7-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "lines and arrows@trajectories", "effects@stroboscopic", "line style@dashed", "element@color highlight", "grouping and linking@color grouping", "purpose@interaction sequence", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays", "grouping and linking@identifiers grouping"], "UIST18_paper737-Figure7-1.png": ["point of view@3rd person", "type@data visualization", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "lines and arrows@direction", "purpose@interaction sequence", "purpose@interactive system", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "Interaction type@touch interaction", "output modality@haptic", "device based@mobile devices", "identifiers@letters"], "CHI18_paper446-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "line style@dashed", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@mid-air interaction", "device based@AR", "tracking based@outside in", "visual@lights", "visual@head mounted displays", "sub-framing@UI overlay"], "CHI18_paper644-Figure11-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@deforming", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers"], "UIST18_paper595-Figure15-1.png": ["point of view@UI only", "point of view@overshoulder 3/4", "point of view@1st person", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "type@text", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Symbolic gesture", "discrete@Pointing", "Interaction type@touch interaction", "device based@AR", "device based@mobile devices", "activity@fabrication", "visual@head mounted displays", "visual@mobile displays", "grouping and linking@text annotation"], "CHI18_paper249-Figure4-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "element@color highlight", "region@color area", "purpose@design space", "time@still", "activity@data manipulation", "discrete@Pointing", "continuous@scrolling", "discrete@Symbolic gesture", "Interaction type@touch interaction"], "CHI18_paper20-Figure2-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@text", "type@photo", "hue@colors", "hue@grayscale", "line style@dashed", "identifiers@letters", "lines and arrows@direction", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "continuous@rotating", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays", "grouping and linking@text annotation"], "CHI18_paper653-Figure3-1.png": ["point of view@UI only", "point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@title", "identifiers@letters", "type@text", "element@color highlight", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper69-Figure5-1.png": ["point of view@UI only", "sub-framing@UI embedded", "number of frames@one frame", "type@text", "type@clipart/icon", "hue@colors", "lines and arrows@transfer", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "audible@sound based", "visual@mobile displays"], "CHI18_paper541-Figure9-1.png": ["point of view@3rd person", "point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "element@color highlight", "sub-framing@magnification lens", "type@text", "type@clipart/icon", "lines and arrows@transfer", "grouping and linking@identifiers grouping", "dynamic@Contact shapes", "measure@Text indicator", "measure@Arrows", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction", "identifiers@title", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "device based@tangible objects", "output modality@haptic", "visual@computer displays", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper603-Figure1-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "hue@colors", "element@color highlight", "lines and arrows@trajectories", "region@color area", "measure@Text indicator", "measure@Arrows", "identifiers@letters", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "purpose@design space", "time@still"], "CSCW18_paper118-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "type@text", "color@transparency", "hue@colors", "grouping and linking@identifiers grouping", "identifiers@letters", "purpose@design space", "time@still", "activity@production", "number@Multi-users", "body part@full body", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@touch interaction", "device based@mobile devices", "device based@large surfaces", "visual@mobile displays", "visual@large displays", "visual@projected displays"], "UIST18_paper649-Figure6-1.png": ["point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@data visualization", "identifiers@letters", "lines and arrows@trajectories", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction"], "CHI18_paper378-Figure4-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@touch interaction", "Interaction type@on body interaction", "Interaction type@computer interaction ", "device based@desktop devices", "tracking based@inside out"], "CHI18_paper241-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@grayscale", "line style@width", "lines and arrows@direction", "purpose@interactive system", "time@still", "activity@communication", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper423-Figure8-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@distal interaction"], "UIST18_paper649-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@clipart/icon", "hue@colors", "lines and arrows@trajectories", "grouping and linking@color grouping", "element@color highlight", "purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "identifiers@letters"], "CHI18_paper188-Figure8-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@text", "hue@colors", "identifiers@letters", "region@color area", "measure@Text indicator", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "Interaction type@touch interaction"], "CHI18_paper446-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "point of view@1st person", "type@photo", "UI@rendering", "color@transparency", "hue@colors", "effects@waves", "lines and arrows@direction", "grouping and linking@identifiers grouping", "identifiers@letters", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "Interaction type@mid-air interaction", "device based@tangible objects", "tracking based@inside out", "visual@head mounted displays", "output modality@haptic"], "UIST18_paper853-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "color@transparency", "lines and arrows@direction", "line style@width", "element@color highlight", "identifiers@letters", "lines and arrows@projection", "lines and arrows@trajectories", "enclosing@circle/rectangle", "purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper589-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "type@data visualization", "hue@colors", "line style@dashed", "type@text", "lines and arrows@direction", "measure@Text indicator", "effects@stroboscopic", "measure@Arrows", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@rotating", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper477-Figure4-1.png": ["sub-framing@magnification lens", "point of view@top", "type@clipart/icon", "grouping and linking@identifiers grouping", "number of frames@one frame", "hue@colors", "element@color highlight", "region@color area", "sub-framing@UI embedded", "UI@rendering", "realism@simplistic", "type@text", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper53-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "color@transparency", "identifiers@letters", "effects@stroboscopic", "region@color area", "line style@dashed", "purpose@design space", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@mobile devices", "device based@tangible objects", "output modality@haptic", "visual@mobile displays", "lines and arrows@direction"], "CHI18_paper150-Figure2-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "realism@simplistic", "hue@colors", "lines and arrows@trajectories", "identifiers@title", "type@text", "identifiers@letters", "effects@stroboscopic", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays", "device based@large surfaces"], "UIST18_paper141-Figure14-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper128-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@grayscale", "color@transparency", "effects@stroboscopic", "measure@Arrows", "line style@dashed", "identifiers@letters", "lines and arrows@direction", "grouping and linking@identifiers grouping", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@mid-air interaction", "Interaction type@controllers interaction", "device based@controllers", "device based@VR", "visual@head mounted displays"], "CHI18_paper219-Figure8-1.png": ["point of view@top", "number of frames@one frame", "type@text", "hue@colors", "color@transparency", "region@color area", "element@color highlight", "line style@dashed", "lines and arrows@trajectories", "grouping and linking@identifiers grouping", "grouping and linking@color grouping", "effects@stroboscopic", "purpose@interactive system", "time@moving", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@mid-air interaction", "device based@mobile devices"], "CHI18_paper446-Figure12-1.png": ["point of view@3rd person", "sub-framing@UI overlay", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "lines and arrows@trajectories", "line style@dashed", "identifiers@letters", "purpose@interaction sequence", "activity@entertainment", "time@moving", "number@Solo-user", "body part@full body", "continuous@deforming", "discrete@Pointing", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "tracking based@inside out", "device based@AR", "device based@tangible objects", "visual@head mounted displays", "output modality@haptic"], "CHI18_paper82-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "grouping and linking@identifiers grouping", "realism@simplistic", "sub-framing@UI embedded", "UI@drawing", "hue@colors", "color@transparency", "grouping and linking@color grouping", "purpose@interaction sequence", "time@moving", "activity@production", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@touch interaction", "Interaction type@computer interaction ", "device based@mobile devices", "device based@desktop devices", "visual@mobile displays", "visual@computer displays"], "CHI18_paper436-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@monochrome", "lines and arrows@trajectories", "line style@dashed", "identifiers@letters", "purpose@design space", "time@still", "discrete@Pointing"], "CHI18_paper42-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "type@data visualization", "sub-framing@juxtaposition", "hue@colors", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@rotating", "Interaction type@mid-air interaction", "Interaction type@controllers interaction", "tracking based@outside in", "device based@VR", "device based@controllers", "visual@head mounted displays"], "UIST18_paper87-Figure10-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper245-Figure3-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "measure@Arrows", "measure@Text indicator", "grouping and linking@identifiers grouping", "lines and arrows@trajectories", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "body part@full body", "discrete@Pointing", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays"], "UIST18_paper511-Figure12-1.png": ["point of view@1st person", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "line style@dashed", "identifiers@letters", "lines and arrows@trajectories", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@controllers interaction", "Interaction type@distal interaction", "discrete@Pointing", "device based@VR", "device based@controllers", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper460-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "visual@head mounted displays"], "CHI18_paper188-Figure2-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper502-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "Interaction type@tangible interaction", "device based@tangible objects"], "UIST18_paper485-Figure7-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "purpose@design space", "time@still", "activity@entertainment", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "audible@sound based", "visual@mobile displays"], "CHI18_paper320-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "continuous@deforming", "Interaction type@on body interaction", "tracking based@inside out", "visual@computer displays", "identifiers@letters"], "UIST18_paper213-Figure20-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@medical", "number@Solo-user", "body part@upper body", "Specific part@hand", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays", "Interaction type@on body interaction", "Situation@Private indoors"], "CHI18_paper73-Figure4-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "enclosing@circle/rectangle", "purpose@design space", "time@still", "activity@data manipulation", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@scrolling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays", "Situation@Private indoors"], "CHI18_paper629-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@large surfaces", "audible@sound based", "visual@projected displays", "Situation@Desktop"], "CHI18_paper613-Figure8-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "type@data visualization", "hue@colors", "identifiers@letters", "purpose@design space", "purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "Specific part@fingers", "tracking based@inside out", "Interaction type@tangible interaction", "device based@tangible objects", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper162-Figure6-1.png": ["point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper508-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "line style@dashed", "line style@width", "identifiers@title", "sub-framing@juxtaposition", "grouping and linking@text annotation", "grouping and linking@color grouping", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Key press", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper142-Figure1-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "type@text", "hue@colors", "grouping and linking@color grouping", "grouping and linking@text annotation", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Multi-users", "Specific part@hand", "body part@upper body", "discrete@Pointing", "continuous@translating", "continuous@scaling", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays", "Situation@Private indoors"], "CHI18_paper245-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper291-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "UI@drawing", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@direction", "grouping and linking@color grouping", "purpose@interactive system", "time@moving", "activity@data manipulation", "continuous@deforming", "Interaction type@tangible interaction", "device based@large surfaces", "visual@large displays"], "CHI18_paper336-Figure13-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@data visualization", "type@photo", "hue@colors", "identifiers@letters", "purpose@interactive system", "purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@fingers", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper613-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@fingers", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper202-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Private indoors"], "CHI18_paper150-Figure13-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "line style@dashed", "measure@Text indicator", "measure@Arrows", "UI@drawing", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "tracking based@outside in", "visual@computer displays"], "CSCW18_paper159-Figure6-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers", "device based@large surfaces", "visual@large displays", "continuous@scaling", "Situation@Private indoors"], "CHI18_paper150-Figure15-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "measure@Arrows", "measure@Text indicator", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "tracking based@outside in", "visual@computer displays"], "CHI18_paper300-Figure5-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "anonymization@blur", "enclosing@circle/rectangle", "lines and arrows@trajectories", "grouping and linking@color grouping", "grouping and linking@text annotation", "purpose@design space", "time@still", "activity@communication", "activity@production", "number@Multi-users", "body part@full body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Public/private transport"], "CHI18_paper569-Figure15-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "visual@lights", "device based@tangible objects", "Interaction type@tangible interaction"], "CHI18_paper360-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper321-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "effects@waves", "identifiers@letters", "grouping and linking@text annotation", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@mobile devices", "device based@tangible objects", "visual@mobile displays"], "CHI18_paper25-Figure2-1.png": ["point of view@top", "point of view@UI only", "number of frames@multi frames", "UI@drawing", "realism@simplistic", "hue@colors", "line style@dashed", "enclosing@circle/rectangle", "dynamic@Contact shapes", "element@color highlight", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "continuous@scaling", "device based@large surfaces", "visual@large displays"], "UIST18_paper511-Figure2-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors"], "CSCW18_paper192-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@entertainment", "activity@communication", "number@Multi-users", "Specific part@hand", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "Ubicomp18_paper194-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@magnification lens", "line style@dashed", "hue@colors", "realism@simplistic", "realism@realistic", "type@data visualization", "type@text", "grouping and linking@text annotation", "grouping and linking@color grouping", "type@clipart/icon", "lines and arrows@direction", "effects@waves", "measure@Arrows", "measure@Text indicator", "purpose@design space", "time@still", "number@Solo-user", "body part@full body", "Specific part@hand", "Interaction type@distal interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper564-Figure2-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "sub-framing@UI overlay", "type@photo", "UI@rendering", "hue@colors", "element@color highlight", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "tracking based@outside in", "visual@large displays", "visual@mobile displays", "visual@computer displays", "visual@head mounted displays", "color@transparency"], "UIST18_paper853-Figure4-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@magnification lens", "type@data visualization", "realism@simplistic", "hue@colors", "line style@dashed", "effects@stroboscopic", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@rotating"], "CHI18_paper406-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@text", "realism@simplistic", "UI@drawing", "hue@colors", "enclosing@circle/rectangle", "element@color highlight", "effects@stroboscopic", "lines and arrows@direction", "identifiers@title", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@communication", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@mid-air interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper353-Figure4-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "UI@rendering", "hue@colors", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Key press", "Interaction type@touch interaction", "device based@mobile devices", "device based@AR", "visual@mobile displays", "visual@computer displays", "Interaction type@computer interaction ", "device based@laptop devices"], "CHI18_paper661-Figure1-1.png": ["point of view@1st person", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@colors", "number of frames@multi frames", "element@color highlight", "lines and arrows@trajectories", "grouping and linking@grouping arrows", "sub-framing@magnification lens", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "continuous@scrolling", "device based@mobile devices", "visual@large displays", "color@transparency", "effects@stroboscopic"], "CHI18_paper362-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "body part@upper body", "device based@mobile devices", "visual@mobile displays", "Interaction type@touch interaction", "discrete@Key press"], "CHI18_paper529-Figure1-1.png": ["point of view@top", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "color@transparency", "hue@colors", "lines and arrows@transfer", "number of frames@one frame", "sub-framing@juxtaposition", "purpose@interaction sequence", "time@moving", "activity@production", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper220-Figure2-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "hue@monochrome", "type@text", "identifiers@title", "purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "Specific part@hand", "body part@full body", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper335-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@juxtaposition", "type@text", "type@data visualization", "hue@colors", "line style@dashed", "dynamic@Contact shapes", "grouping and linking@color grouping", "lines and arrows@trajectories", "effects@stroboscopic", "grouping and linking@identifiers grouping", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction"], "CSCW18_paper072-Figure5-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "sub-framing@UI embedded"], "CHI18_paper33-Figure5-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "type@data visualization", "type@text", "measure@Text indicator", "measure@Arrows", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@on body interaction", "device based@mobile devices", "hue@colors"], "CHI18_paper425-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded", "purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper76-Figure2-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@mobile devices", "visual@mobile displays", "activity@data manipulation"], "CHI18_paper541-Figure8-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "lines and arrows@transfer", "element@color highlight", "grouping and linking@color grouping", "lines and arrows@trajectories", "UI@drawing", "realism@simplistic", "point of view@UI only", "sub-framing@magnification lens", "sub-framing@UI embedded", "hue@colors", "purpose@design space", "purpose@interaction sequence", "time@still", "type@text", "grouping and linking@text annotation", "identifiers@title", "number@Solo-user", "Specific part@hand", "Specific part@foot", "body part@upper body", "discrete@Key press", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic", "visual@computer displays", "audible@sound based"], "CHI18_paper54-Figure10-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "body part@full body", "device based@VR", "visual@head mounted displays"], "CHI18_paper579-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "point of view@3rd person", "sub-framing@UI overlay", "type@photo", "color@transparency", "UI@rendering", "hue@colors", "grouping and linking@text annotation", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "Specific part@head", "discrete@Pointing", "discrete@Key press", "Interaction type@controllers interaction", "Interaction type@distal interaction", "tracking based@outside in", "device based@VR", "device based@controllers", "visual@head mounted displays", "activity@fabrication"], "UIST18_paper5-Figure7-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "output modality@haptic"], "CHI18_paper287-Figure2-1.png": ["number of frames@multi frames", "type@photo", "point of view@3rd person", "type@data visualization", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "number@Multi-users", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in"], "CHI18_paper354-Figure17-1.png": ["point of view@3rd person", "sub-framing@juxtaposition", "number of frames@one frame", "identifiers@letters", "identifiers@title", "grouping and linking@text annotation", "hue@colors", "sub-framing@UI embedded", "type@photo", "purpose@design space", "time@still", "activity@entertainment", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper737-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "grouping and linking@text annotation", "identifiers@letters", "purpose@interactive system", "purpose@interaction sequence", "time@moving", "lines and arrows@trajectories", "number@Solo-user", "Specific part@fingers", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "output modality@haptic"], "CHI18_paper89-Figure5-1.png": ["point of view@1st person", "point of view@UI only", "number of frames@multi frames", "point of view@3rd person", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@full body", "Interaction type@tangible interaction", "device based@VR", "device based@tangible objects", "visual@head mounted displays"], "UIST18_paper5-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "color@transparency", "region@color area", "element@color highlight", "enclosing@exact contour line", "grouping and linking@color grouping", "lines and arrows@direction", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction"], "UIST18_paper877-Figure3-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "line style@dashed", "line style@width", "lines and arrows@direction", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@translating", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic"], "CHI18_paper401-Figure7-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@driving", "activity@entertainment", "number@Solo-user", "body part@full body", "continuous@rotating", "discrete@Key press", "Interaction type@controllers interaction", "device based@controllers", "visual@large displays", "Situation@Private indoors"], "Ubicomp18_paper176-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@communication", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays", "Situation@Desktop"], "CHI18_paper142-Figure2-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "type@data visualization", "type@text", "element@color highlight", "lines and arrows@direction", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "CHI18_paper547-Figure9-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "line style@dashed", "element@color highlight", "effects@stroboscopic", "lines and arrows@direction", "lines and arrows@trajectories", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@pen interaction", "Interaction type@distal interaction", "device based@AR"], "CHI18_paper61-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@full body", "tracking based@inside out", "visual@lights", "Situation@Public indoors"], "CHI18_paper428-Figure1-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "type@photo", "hue@colors", "element@color highlight", "type@text", "identifiers@title", "grouping and linking@text annotation", "enclosing@exact contour line", "grouping and linking@color grouping", "purpose@interactive system", "purpose@design space", "time@still", "effects@stroboscopic", "number@Solo-user", "body part@full body", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper199-Figure17-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "type@data visualization", "type@text", "hue@colors", "grouping and linking@color grouping", "identifiers@letters", "purpose@interaction sequence", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper69-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "UI@rendering", "type@clipart/icon", "type@text", "hue@colors", "effects@stroboscopic", "color@transparency", "lines and arrows@transfer", "identifiers@letters", "identifiers@title", "purpose@interaction sequence", "purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "audible@speech output", "visual@mobile displays"], "CHI18_paper291-Figure9-1.png": ["point of view@3rd person", "point of view@1st person", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "grouping and linking@grouping arrows", "grouping and linking@text annotation", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic", "tracking based@outside in", "device based@VR", "visual@head mounted displays", "Situation@Desktop"], "CHI18_paper150-Figure8-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "measure@Arrows", "measure@Text indicator", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper547-Figure8-1.png": ["point of view@top", "point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "element@color highlight", "lines and arrows@direction", "dynamic@Contact shapes", "line style@dashed", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@large surfaces"], "CHI18_paper123-Figure7-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "realism@simplistic", "hue@colors", "element@color highlight", "identifiers@letters", "time@still", "purpose@design space", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@computer interaction "], "UIST18_paper53-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "type@text", "hue@colors", "element@color highlight", "region@color area", "lines and arrows@trajectories", "effects@stroboscopic", "identifiers@letters", "effects@waves", "purpose@design space", "time@still", "output modality@haptic", "device based@tangible objects", "Interaction type@tangible interaction", "device based@mobile devices"], "CHI18_paper54-Figure7-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "point of view@UI only", "number of frames@multi frames", "sub-framing@magnification lens", "type@photo", "UI@rendering", "sub-framing@UI embedded", "hue@colors", "enclosing@circle/rectangle", "identifiers@letters", "grouping and linking@text annotation", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "body part@upper body", "discrete@Pointing", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "CHI18_paper593-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays"], "CHI18_paper123-Figure9-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@drawing", "realism@simplistic", "hue@colors", "identifiers@letters", "element@color highlight", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing"], "CHI18_paper443-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "anonymization@blur", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper43-Figure3-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "sub-framing@UI embedded", "sub-framing@inset (PiP)", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@data manipulation", "Interaction type@distal interaction", "tracking based@outside in", "visual@mobile displays"], "Ubicomp18_paper181-Figure1-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "enclosing@circle/rectangle", "purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "visual@mobile displays"], "CHI18_paper54-Figure4-1.png": ["number of frames@one frame", "point of view@3rd person", "sub-framing@UI embedded", "line style@dashed", "type@photo", "hue@colors", "color@transparency", "region@color area", "enclosing@circle/rectangle", "grouping and linking@text annotation", "identifiers@letters", "purpose@design space", "time@still", "activity@communication", "number@Multi-users", "body part@upper body", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@distal interaction", "Interaction type@mid-air interaction", "device based@mobile devices", "visual@head mounted displays", "device based@VR"], "Ubicomp18_paper162-Figure1-1.png": ["point of view@3rd person", "point of view@overshoulder 3/4", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "purpose@design space", "purpose@interaction sequence", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper626-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interaction sequence", "time@moving", "activity@production", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper496-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "type@clipart/icon", "effects@waves", "type@text", "measure@Arrows", "measure@Text indicator", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Interaction type@computer interaction ", "device based@desktop devices"], "UIST18_paper335-Figure1-1.png": ["point of view@top", "point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "realism@simplistic", "hue@colors", "color@transparency", "line style@dashed", "element@color highlight", "region@color area", "effects@stroboscopic", "lines and arrows@trajectories", "lines and arrows@direction", "dynamic@Contact shapes", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "continuous@writing/drawing", "discrete@Symbolic gesture", "device based@mobile devices", "visual@mobile displays"], "Ubicomp18_paper164-Figure2-1.png": ["point of view@1st person", "point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "sub-framing@UI embedded", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays", "tracking based@outside in"], "CHI18_paper189-Figure9-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "sub-framing@magnification lens", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Multi-users", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays"], "CHI18_paper613-Figure10-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "type@text", "hue@colors", "grouping and linking@text annotation", "identifiers@letters", "purpose@design space", "time@still", "activity@medical", "number@Solo-user", "Specific part@hand", "body part@upper body", "Interaction type@tangible interaction", "Interaction type@on body interaction", "tracking based@inside out", "device based@tangible objects", "visual@lights"], "CHI18_paper593-Figure4-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@medical", "number@Solo-user", "body part@full body", "Interaction type@distal interaction", "tracking based@outside in", "visual@large displays", "Situation@Private indoors"], "CSCW18_paper185-Figure5-1.png": ["point of view@top", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays", "Situation@Private indoors"], "UIST18_paper485-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "realism@realistic", "hue@colors", "purpose@design space", "time@still", "activity@communication", "number@Multi-users", "Specific part@fingers", "body part@upper body", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights"], "CHI18_paper61-Figure9-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "Interaction type@on body interaction", "tracking based@inside out", "visual@lights", "Situation@Private indoors"], "Ubicomp18_paper164-Figure8-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "discrete@Pointing"], "CHI18_paper529-Figure14-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper11-Figure1-1.png": ["point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "enclosing@circle/rectangle", "point of view@top", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "Situation@Desktop"], "CHI18_paper362-Figure1-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@controllers interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Outdoors"], "UIST18_paper87-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@mobile devices", "Situation@Private indoors"], "UIST18_paper321-Figure8-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@data visualization", "type@photo", "hue@colors", "color@transparency", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@fingers", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper401-Figure8-1.png": ["point of view@3rd person", "sub-framing@UI embedded", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@mid-air interaction", "discrete@Symbolic gesture", "tracking based@outside in", "visual@projected displays", "Situation@Private indoors"], "CHI18_paper95-Figure4-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@on body interaction", "tracking based@inside out", "visual@computer displays", "Situation@Desktop"], "CHI18_paper5-Figure1-1.png": ["point of view@3rd person", "point of view@top", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "visual@lights"], "CHI18_paper199-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@drawing", "realism@simplistic", "hue@colors", "color@transparency", "line style@dashed", "lines and arrows@trajectories", "identifiers@letters", "element@color highlight", "grouping and linking@color grouping", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "continuous@translating", "Interaction type@tangible interaction", "Interaction type@pen interaction", "device based@tangible objects"], "CHI18_paper45-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "body part@full body", "continuous@scrolling", "device based@desktop devices", "visual@head mounted displays", "Interaction type@controllers interaction", "Situation@Private indoors"], "CHI18_paper218-Figure14-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "activity@production", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@writing/drawing", "Interaction type@mid-air interaction", "Interaction type@pen interaction", "device based@VR", "device based@AR", "visual@head mounted displays", "purpose@design space", "time@still"], "CHI18_paper89-Figure9-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "continuous@rotating", "continuous@translating", "continuous@deforming", "device based@tangible objects", "visual@head mounted displays", "device based@VR", "tracking based@outside in"], "UIST18_paper637-Figure4-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters", "time@still", "purpose@design space", "activity@2D/3D creation", "number@Solo-user", "discrete@Pointing", "continuous@translating", "continuous@scaling", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper150-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "type@photo", "UI@drawing", "hue@colors", "color@transparency", "identifiers@title", "identifiers@letters", "effects@stroboscopic", "lines and arrows@trajectories", "element@color highlight", "purpose@interactive system", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@touch interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@large surfaces", "device based@VR"], "CHI18_paper199-Figure7-1.png": ["point of view@overshoulder 3/4", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper516-Figure1-1.png": ["point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@controllers interaction", "device based@controllers", "audible@sound based", "visual@computer displays"], "CHI18_paper238-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "hue@colors", "element@color highlight", "color@transparency", "effects@stroboscopic", "type@data visualization", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "continuous@translating", "continuous@rotating", "Interaction type@mid-air interaction", "tracking based@outside in"], "UIST18_paper779-Figure3-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@touch interaction", "device based@mobile devices"], "UIST18_paper901-Figure9-1.png": ["point of view@1st person", "point of view@UI only", "sub-framing@inset (PiP)", "number of frames@multi frames", "type@photo", "UI@rendering", "type@text", "hue@colors", "identifiers@title", "element@color highlight", "measure@Arrows", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "visual@head mounted displays"], "CHI18_paper284-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@full body", "device based@tangible objects", "Interaction type@tangible interaction", "Interaction type@mid-air interaction"], "CHI18_paper336-Figure4-1.png": ["point of view@1st person", "number of frames@one frame", "type@text", "type@photo", "hue@colors", "grouping and linking@text annotation", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "activity@fabrication", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper73-Figure5-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@production", "number@Multi-users", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@large displays", "visual@mobile displays"], "CHI18_paper406-Figure3-1.png": ["Interaction type@touch interaction", "continuous@translating", "discrete@Pointing", "visual@mobile displays", "device based@mobile devices", "number@Solo-user", "Specific part@hand", "activity@communication", "purpose@interaction sequence", "time@moving", "lines and arrows@direction", "identifiers@letters", "identifiers@title", "element@color highlight", "UI@drawing", "realism@simplistic", "hue@colors", "number of frames@multi frames", "point of view@top"], "CHI18_paper189-Figure8-1.png": ["point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@juxtaposition", "type@photo", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@fabrication", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper411-Figure6-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "effects@waves", "grouping and linking@text annotation", "purpose@interaction sequence", "time@moving", "activity@entertainment", "activity@fabrication", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "Interaction type@controllers interaction"], "CHI18_paper223-Figure7-1.png": ["number of frames@multi frames", "point of view@UI only", "realism@simplistic", "UI@drawing", "type@text", "hue@colors", "identifiers@title", "lines and arrows@trajectories", "lines and arrows@direction", "element@color highlight", "enclosing@circle/rectangle", "purpose@design space", "time@still", "activity@2D/3D creation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@writing/drawing", "continuous@translating", "Interaction type@touch interaction", "Interaction type@pen interaction"], "UIST18_paper53-Figure7-1.png": ["point of view@3rd person", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "device based@mobile devices", "output modality@haptic", "visual@computer displays", "visual@mobile displays", "Interaction type@computer interaction ", "Interaction type@controllers interaction"], "UIST18_paper745-Figure11-1.png": ["point of view@1st person", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@touch interaction", "Interaction type@on body interaction", "visual@mobile displays"], "UIST18_paper853-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "color@transparency", "hue@colors", "line style@dashed", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters", "grouping and linking@text annotation", "lines and arrows@direction", "type@text", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "discrete@Symbolic gesture"], "CHI18_paper18-Figure2-1.png": ["point of view@3rd person", "number of frames@one frame", "realism@simplistic", "hue@colors", "lines and arrows@projection", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction"], "CHI18_paper219-Figure13-1.png": ["point of view@1st person", "type@photo", "sub-framing@UI overlay", "number of frames@multi frames", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@fabrication", "number@Solo-user", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper426-Figure4-1.png": ["Situation@Desktop", "point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices"], "CHI18_paper248-Figure5-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "type@text", "color@transparency", "hue@colors", "line style@dashed", "grouping and linking@text annotation", "identifiers@letters", "lines and arrows@trajectories", "effects@stroboscopic", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper477-Figure1-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@magnification lens", "UI@drawing", "realism@simplistic", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "grouping and linking@text annotation", "type@text", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction"], "UIST18_paper499-Figure6-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "sub-framing@UI overlay", "UI@drawing", "type@photo", "hue@colors", "color@transparency", "region@color area", "identifiers@letters", "grouping and linking@identifiers grouping", "purpose@design space", "time@still", "device based@VR", "visual@head mounted displays", "Situation@Private indoors"], "CHI18_paper185-Figure6-1.png": ["point of view@overshoulder 3/4", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@design space", "time@still", "activity@production", "number@Solo-user", "body part@upper body", "continuous@writing/drawing", "Interaction type@pen interaction", "Interaction type@mid-air interaction", "device based@controllers", "device based@AR", "device based@mobile devices", "visual@head mounted displays", "visual@mobile displays", "Situation@Desktop"], "UIST18_paper839-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@data visualization", "type@photo", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "continuous@scrolling", "Interaction type@controllers interaction", "device based@controllers"], "CHI18_paper515-Figure8-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "hue@colors", "sub-framing@UI embedded", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@large surfaces", "visual@mobile displays", "Situation@Desktop"], "CHI18_paper160-Figure4-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@production", "number@Multi-users", "body part@full body", "visual@projected displays", "Situation@Private indoors"], "CHI18_paper165-Figure3-1.png": ["point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "visual@head mounted displays", "Situation@Public/private transport"], "CHI18_paper160-Figure6-1.png": ["point of view@3rd person", "number of frames@one frame", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "Situation@Private indoors"], "CHI18_paper426-Figure9-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@juxtaposition", "type@clipart/icon", "type@photo", "hue@colors", "lines and arrows@transfer", "effects@waves", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "Interaction type@touch interaction", "visual@mobile displays", "device based@mobile devices", "Situation@Desktop"], "UIST18_paper335-Figure6-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "continuous@writing/drawing", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@mobile devices", "visual@mobile displays", "Situation@Desktop"], "UIST18_paper313-Figure4-1.png": ["point of view@top", "number of frames@multi frames", "realism@simplistic", "hue@colors", "identifiers@letters", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@fingers", "discrete@Pointing", "continuous@translating"], "CHI18_paper89-Figure7-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "type@photo", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@interactive system", "time@still", "activity@entertainment", "purpose@interaction sequence", "number@Solo-user", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "Situation@Private indoors", "discrete@Key press"], "CHI18_paper531-Figure2-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "purpose@interactive system", "activity@fabrication", "time@still", "number@Solo-user", "body part@upper body"], "CHI18_paper551-Figure3-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@large displays", "device based@large surfaces", "anonymization@blur"], "CHI18_paper284-Figure7-1.png": ["point of view@top", "number of frames@one frame", "type@photo", "sub-framing@UI embedded", "hue@colors", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "visual@lights", "Situation@Desktop"], "CHI18_paper69-Figure4-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@magnification lens", "realism@simplistic", "UI@drawing", "hue@colors", "type@clipart/icon", "type@text", "grouping and linking@text annotation", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "audible@speech output", "visual@mobile displays"], "UIST18_paper499-Figure12-1.png": ["point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "hue@grayscale", "lines and arrows@trajectories", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@full body", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper31-Figure3-1.png": ["point of view@1st person", "point of view@UI only", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "realism@simplistic", "type@photo", "hue@colors", "grouping and linking@color grouping", "identifiers@letters", "purpose@interactive system", "time@still", "activity@data manipulation", "effects@stroboscopic", "color@transparency", "lines and arrows@trajectories", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@mid-air interaction", "tracking based@inside out", "device based@VR", "visual@head mounted displays", "tracking based@outside in"], "UIST18_paper153-Figure3-1.png": ["point of view@UI only", "number of frames@one frame", "sub-framing@juxtaposition", "hue@colors", "type@text", "element@color highlight", "lines and arrows@transfer", "identifiers@title", "grouping and linking@identifiers grouping", "purpose@interactive system", "time@moving", "effects@stroboscopic", "color@transparency", "UI@drawing", "activity@data manipulation", "Interaction type@computer interaction ", "continuous@translating"], "CHI18_paper569-Figure17-1.png": ["point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@controllers", "Interaction type@controllers interaction"], "CHI18_paper446-Figure11-1.png": ["point of view@3rd person", "number of frames@multi frames", "type@photo", "hue@colors", "identifiers@letters", "effects@waves", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@full body", "continuous@translating", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@AR", "output modality@haptic", "visual@head mounted displays"], "CHI18_paper223-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "realism@simplistic", "UI@drawing", "hue@colors", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "continuous@writing/drawing", "Interaction type@pen interaction"], "CHI18_paper163-Figure1-1.png": ["point of view@1st person", "number of frames@one frame", "sub-framing@UI embedded", "type@photo", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "visual@mobile displays", "tracking based@outside in", "Interaction type@distal interaction"], "UIST18_paper499-Figure11-1.png": ["point of view@1st person", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@UI overlay", "point of view@3rd person", "UI@rendering", "type@photo", "hue@colors", "color@transparency", "region@color area", "identifiers@letters", "purpose@interaction sequence", "time@moving", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "device based@AR", "device based@VR", "visual@head mounted displays", "device based@tangible objects", "Interaction type@tangible interaction", "Interaction type@controllers interaction", "device based@controllers"], "Ubicomp18_paper201-Figure3-1.png": ["point of view@top", "point of view@1st person", "number of frames@multi frames", "type@photo", "hue@colors", "type@text", "region@color area", "grouping and linking@text annotation", "purpose@design space", "purpose@interactive system", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@computer interaction ", "device based@desktop devices", "visual@lights"], "UIST18_paper853-Figure3-1.png": ["point of view@1st person", "number of frames@multi frames", "realism@simplistic", "type@text", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "identifiers@title", "grouping and linking@text annotation", "lines and arrows@direction", "lines and arrows@projection", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction"], "CHI18_paper11-Figure6-1.png": ["Specific part@hand", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@rendering", "type@clipart/icon", "type@text", "hue@grayscale", "identifiers@title", "region@color area", "purpose@design space", "time@still", "number@Solo-user", "discrete@Key press", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays"], "CHI18_paper143-Figure7-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@inset (PiP)", "point of view@1st person", "UI@rendering", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@controllers interaction", "Interaction type@computer interaction ", "device based@controllers", "device based@desktop devices", "tracking based@outside in", "device based@VR", "visual@head mounted displays", "visual@computer displays", "Situation@Desktop"], "CHI18_paper150-Figure21-1.png": ["type@photo", "point of view@1st person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "UI@rendering", "hue@colors", "type@clipart/icon", "enclosing@circle/rectangle", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@translating", "Interaction type@mid-air interaction", "tracking based@outside in", "device based@VR", "output modality@haptic"], "CHI18_paper150-Figure22-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "UI@rendering", "hue@colors", "enclosing@circle/rectangle", "grouping and linking@identifiers grouping", "identifiers@letters", "purpose@interaction sequence", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "tracking based@outside in", "device based@tangible objects", "Interaction type@tangible interaction", "output modality@haptic"], "CHI18_paper150-Figure5-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@inset (PiP)", "hue@colors", "enclosing@circle/rectangle", "purpose@interactive system", "time@still", "Specific part@hand", "discrete@Pointing", "Interaction type@computer interaction ", "device based@desktop devices", "tracking based@outside in", "visual@computer displays"], "CHI18_paper150-Figure6-1.png": ["type@text", "type@photo", "point of view@3rd person", "type@data visualization", "number of frames@multi frames", "hue@colors", "identifiers@letters", "measure@Arrows", "measure@Text indicator", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "Interaction type@tangible interaction", "device based@VR", "device based@tangible objects", "visual@head mounted displays"], "CHI18_paper189-Figure1-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "continuous@rotating", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "visual@lights", "visual@computer displays", "activity@entertainment", "Situation@Desktop"], "CHI18_paper189-Figure10-1.png": ["type@text", "type@photo", "point of view@UI only", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "Specific part@hand", "continuous@deforming", "discrete@Key press", "Interaction type@tangible interaction", "Interaction type@computer interaction ", "device based@tangible objects", "device based@desktop devices", "visual@computer displays"], "CHI18_paper19-Figure7-1.png": ["Specific part@hand", "point of view@top", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "type@text", "type@data visualization", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@mobile displays", "visual@large displays"], "CHI18_paper199-Figure16-1.png": ["Specific part@hand", "type@photo", "point of view@top", "number of frames@multi frames", "hue@colors", "lines and arrows@direction", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays"], "CHI18_paper199-Figure5-1.png": ["Specific part@hand", "effects@stroboscopic", "color@transparency", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "element@color highlight", "lines and arrows@direction", "lines and arrows@trajectories", "measure@Arrows", "grouping and linking@color grouping", "identifiers@letters", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "continuous@rotating", "continuous@translating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper210-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "activity@medical", "number@Multi-users", "body part@full body", "Interaction type@controllers interaction", "device based@controllers", "output modality@haptic"], "CHI18_paper210-Figure4-1.png": ["type@text", "type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@UI embedded", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "visual@head mounted displays", "visual@projected displays"], "CHI18_paper218-Figure4-1.png": ["effects@stroboscopic", "point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@grayscale", "line style@dashed", "lines and arrows@trajectories", "lines and arrows@projection", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "continuous@translating", "continuous@scaling", "Interaction type@mid-air interaction", "Interaction type@distal interaction"], "CHI18_paper218-Figure8-1.png": ["type@photo", "point of view@3rd person", "sub-framing@inset (PiP)", "number of frames@multi frames", "UI@rendering", "hue@colors", "effects@motion blur", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays"], "CHI18_paper219-Figure12-1.png": ["type@text", "type@photo", "point of view@1st person", "sub-framing@UI overlay", "number of frames@one frame", "UI@rendering", "hue@colors", "color@transparency", "purpose@interactive system", "time@still", "Interaction type@distal interaction", "device based@AR", "visual@head mounted displays"], "CHI18_paper238-Figure7-1.png": ["Specific part@hand", "point of view@3rd person", "number of frames@multi frames", "point of view@UI only", "UI@rendering", "hue@colors", "line style@dashed", "element@color highlight", "region@color area", "dynamic@Contact shapes", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "discrete@Pointing", "continuous@translating", "continuous@rotating", "Interaction type@mid-air interaction"], "CHI18_paper249-Figure2-1.png": ["point of view@top", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "UI@rendering", "hue@colors", "lines and arrows@trajectories", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "discrete@Symbolic gesture"], "CHI18_paper255-Figure5-1.png": ["hue@monochrome", "Specific part@hand", "point of view@top", "number of frames@multi frames", "realism@simplistic", "purpose@design space", "time@still", "number@Solo-user", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices"], "CHI18_paper291-Figure8-1.png": ["Specific part@hand", "color@transparency", "type@photo", "point of view@3rd person", "sub-framing@UI overlay", "number of frames@multi frames", "hue@colors", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects", "device based@VR", "visual@head mounted displays", "output modality@haptic"], "CHI18_paper299-Figure2-1.png": ["type@text", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "UI@rendering", "realism@simplistic", "hue@colors", "element@color highlight", "identifiers@title", "purpose@design space", "time@still", "activity@data manipulation", "number@Multi-users", "number@Solo-user", "Specific part@hand", "body part@upper body", "continuous@deforming", "discrete@Pointing", "discrete@Symbolic gesture", "Interaction type@on body interaction", "Interaction type@mid-air interaction", "Interaction type@tangible interaction", "device based@tangible objects", "device based@mobile devices", "Interaction type@touch interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper31-Figure1-1.png": ["Specific part@hand", "type@photo", "point of view@1st person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "tracking based@outside in", "device based@mobile devices", "Interaction type@touch interaction"], "CHI18_paper336-Figure15-1.png": ["Specific part@fingers", "activity@fabrication", "type@photo", "point of view@3rd person", "number of frames@multi frames", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "purpose@interactive system", "time@moving", "number@Solo-user", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper359-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "anonymization@blur", "purpose@interactive system", "time@still", "number@Multi-users", "body part@upper body", "body part@full body", "Interaction type@controllers interaction", "device based@controllers", "device based@VR", "visual@head mounted displays"], "CHI18_paper362-Figure5-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "hue@colors", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper362-Figure6-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "hue@colors", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Key press", "device based@mobile devices", "visual@mobile displays", "Interaction type@controllers interaction"], "CHI18_paper407-Figure2-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@multi frames", "hue@colors", "enclosing@exact contour line", "element@color highlight", "purpose@design space", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "visual@projected displays"], "CHI18_paper419-Figure1-1.png": ["Specific part@hand", "point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI embedded", "realism@realistic", "UI@drawing", "hue@grayscale", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@AR", "device based@mobile devices", "visual@mobile displays", "visual@head mounted displays"], "CHI18_paper441-Figure21-1.png": ["point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "type@clipart/icon", "hue@monochrome", "lines and arrows@transfer", "effects@waves", "identifiers@numbers", "purpose@design space", "time@moving", "activity@fabrication"], "CHI18_paper445-Figure4-1.png": ["type@text", "point of view@UI only", "number of frames@one frame", "UI@rendering", "hue@colors", "effects@stroboscopic", "purpose@interactive system", "time@moving", "grouping and linking@text annotation"], "CHI18_paper446-Figure7-1.png": ["color@transparency", "type@photo", "point of view@overshoulder 3/4", "number of frames@multi frames", "sub-framing@UI overlay", "hue@colors", "line style@dashed", "lines and arrows@trajectories", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "body part@upper body", "continuous@rotating", "Interaction type@tangible interaction", "device based@AR", "device based@tangible objects", "tracking based@inside out", "visual@head mounted displays", "identifiers@letters", "UI@drawing", "type@clipart/icon"], "CHI18_paper465-Figure7-1.png": ["type@photo", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "identifiers@letters", "purpose@interaction sequence", "time@still", "activity@medical", "visual@head mounted displays", "device based@AR"], "CHI18_paper530-Figure1-1.png": ["UI@drawing", "type@photo", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "lines and arrows@trajectories", "effects@motion blur", "identifiers@letters", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@controllers interaction", "Interaction type@mid-air interaction", "device based@VR", "device based@controllers", "tracking based@outside in", "visual@head mounted displays"], "CHI18_paper54-Figure6-1.png": ["type@text", "point of view@1st person", "point of view@3rd person", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "type@photo", "hue@colors", "enclosing@circle/rectangle", "purpose@design space", "time@still", "activity@entertainment", "number@Multi-users", "body part@upper body", "Specific part@head", "identifiers@letters", "discrete@Pointing", "Interaction type@touch interaction", "device based@VR", "tracking based@outside in", "visual@head mounted displays", "background@removed", "device based@mobile devices", "line style@dashed", "grouping and linking@text annotation"], "CHI18_paper541-Figure3-1.png": ["type@text", "number of frames@multi frames", "type@data visualization", "hue@colors", "grouping and linking@color grouping", "element@color highlight", "type@clipart/icon", "purpose@interaction sequence", "time@moving", "lines and arrows@transfer", "measure@Arrows", "Interaction type@touch interaction", "dynamic@Contact shapes", "audible@sound based"], "CHI18_paper547-Figure10-1.png": ["Specific part@hand", "point of view@1st person", "number of frames@one frame", "sub-framing@juxtaposition", "realism@simplistic", "hue@colors", "line style@dashed", "color@transparency", "element@color highlight", "lines and arrows@trajectories", "effects@stroboscopic", "purpose@design space", "purpose@interaction sequence", "time@moving", "number@Solo-user", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "CHI18_paper547-Figure5-1.png": ["Specific part@hand", "type@text", "point of view@top", "point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "hue@monochrome", "line style@dashed", "lines and arrows@direction", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "continuous@translating", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@mid-air interaction", "device based@large surfaces"], "CHI18_paper547-Figure7-1.png": ["hue@monochrome", "type@text", "Specific part@hand", "effects@stroboscopic", "point of view@3rd person", "point of view@top", "number of frames@multi frames", "realism@simplistic", "dynamic@Contact shapes", "lines and arrows@direction", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "purpose@interaction sequence", "number@Solo-user", "continuous@rotating", "continuous@translating", "discrete@Pointing", "Interaction type@touch interaction", "Interaction type@mid-air interaction"], "CHI18_paper558-Figure2-1.png": ["hue@monochrome", "point of view@3rd person", "number of frames@one frame", "realism@simplistic", "line style@dashed", "measure@Arrows", "grouping and linking@identifiers grouping", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Key press", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper564-Figure3-1.png": ["type@text", "Specific part@hand", "color@transparency", "type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@inset (PiP)", "hue@colors", "line style@dashed", "enclosing@exact contour line", "grouping and linking@identifiers grouping", "dynamic@Contact shapes", "lines and arrows@trajectories", "purpose@design space", "time@moving", "number@Solo-user", "discrete@Pointing", "continuous@translating", "Interaction type@on body interaction", "tracking based@outside in"], "CHI18_paper579-Figure13-1.png": ["UI@drawing", "type@photo", "point of view@1st person", "number of frames@multi frames", "sub-framing@UI overlay", "hue@colors", "color@transparency", "identifiers@letters", "lines and arrows@projection", "purpose@interaction sequence", "activity@2D/3D creation", "activity@fabrication", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@AR", "visual@head mounted displays"], "CHI18_paper61-Figure10-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "effects@stroboscopic", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "Interaction type@on body interaction", "visual@lights"], "CHI18_paper634-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@full body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper634-Figure8-1.png": ["type@text", "effects@stroboscopic", "type@data visualization", "realism@simplistic", "UI@drawing", "point of view@3rd person", "sub-framing@inset (PiP)", "number of frames@one frame", "hue@colors", "grouping and linking@color grouping", "lines and arrows@direction", "lines and arrows@transfer", "measure@Arrows", "measure@Text indicator", "grouping and linking@identifiers grouping", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@translating", "continuous@rotating", "continuous@scaling", "continuous@scrolling", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper647-Figure2-1.png": ["type@text", "UI@drawing", "point of view@3rd person", "number of frames@one frame", "sub-framing@UI overlay", "realism@realistic", "hue@colors", "element@color highlight", "measure@Text indicator", "measure@Arrows", "dynamic@Contact shapes", "lines and arrows@projection", "grouping and linking@identifiers grouping", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "discrete@Pointing", "discrete@Key press", "Interaction type@distal interaction", "device based@VR", "device based@controllers", "visual@head mounted displays", "Interaction type@controllers interaction"], "CHI18_paper654-Figure4-1.png": ["type@text", "UI@drawing", "point of view@3rd person", "sub-framing@juxtaposition", "number of frames@one frame", "hue@colors", "line style@dashed", "dynamic@Contact shapes", "lines and arrows@direction", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@deforming", "output modality@haptic", "device based@controllers", "Interaction type@controllers interaction"], "CHI18_paper69-Figure3-1.png": ["type@text", "point of view@UI only", "number of frames@multi frames", "type@clipart/icon", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "audible@speech output"], "CHI18_paper73-Figure2-1.png": ["type@text", "point of view@3rd person", "number of frames@multi frames", "realism@simplistic", "UI@rendering", "hue@colors", "element@color highlight", "line style@dashed", "lines and arrows@direction", "grouping and linking@text annotation", "identifiers@letters", "identifiers@title", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "body part@upper body", "continuous@rotating", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "device based@large surfaces", "visual@large displays", "visual@mobile displays"], "CHI18_paper78-Figure2-1.png": ["point of view@UI only", "number of frames@multi frames", "UI@rendering", "hue@colors", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Interaction type@distal interaction", "device based@VR", "visual@head mounted displays"], "CHI18_paper82-Figure7-1.png": ["activity@fabrication", "type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@numbers", "purpose@interaction sequence", "time@still", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "continuous@deforming", "Interaction type@touch interaction", "visual@computer displays"], "CHI18_paper90-Figure7-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "background@removed", "purpose@interactive system", "time@still", "activity@data manipulation", "sub-framing@UI overlay", "color@transparency", "number@Multi-users", "body part@upper body", "Interaction type@mid-air interaction", "Interaction type@touch interaction", "device based@VR", "device based@large surfaces", "visual@large displays", "visual@head mounted displays"], "CHI18_paper95-Figure3-1.png": ["type@text", "type@photo", "point of view@top", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "color@transparency", "region@color area", "identifiers@title", "measure@Arrows", "measure@Text indicator", "time@still", "purpose@interactive system", "number@Solo-user", "Specific part@hand", "Interaction type@on body interaction", "tracking based@inside out"], "CHI18_paper98-Figure2-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "Specific part@hand", "discrete@Pointing", "device based@mobile devices", "visual@mobile displays", "Interaction type@distal interaction"], "CSCW18_paper140-Figure3-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "hue@colors", "enclosing@circle/rectangle", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@distal interaction"], "CSCW18_paper140-Figure5-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "CSCW18_paper185-Figure6-1.png": ["type@photo", "point of view@top", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@large surfaces", "visual@large displays"], "CSCW18_paper192-Figure6-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "activity@entertainment", "number@Multi-users", "Specific part@hand", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper31-Figure4-1.png": ["type@text", "color@transparency", "UI@drawing", "type@photo", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "hue@colors", "region@color area", "element@color highlight", "grouping and linking@color grouping", "identifiers@title", "effects@stroboscopic", "enclosing@exact contour line", "purpose@interaction sequence", "time@moving", "number@Solo-user", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper321-Figure15-1.png": ["Specific part@fingers", "type@photo", "point of view@1st person", "number of frames@multi frames", "hue@colors", "lines and arrows@direction", "identifiers@letters", "effects@waves", "purpose@design space", "time@still", "activity@data manipulation", "activity@entertainment", "number@Solo-user", "continuous@rotating", "discrete@Symbolic gesture", "continuous@translating", "continuous@deforming", "Interaction type@tangible interaction", "device based@mobile devices", "device based@tangible objects", "visual@mobile displays"], "UIST18_paper335-Figure8-1.png": ["Specific part@hand", "type@photo", "point of view@top", "number of frames@multi frames", "sub-framing@juxtaposition", "hue@colors", "type@text", "identifiers@letters", "identifiers@title", "lines and arrows@direction", "purpose@interaction sequence", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "discrete@Pointing", "discrete@Symbolic gesture", "continuous@translating", "Interaction type@touch interaction", "device based@mobile devices", "tracking based@inside out", "visual@mobile displays"], "UIST18_paper347-Figure1-1.png": ["type@text", "UI@drawing", "point of view@top", "point of view@UI only", "sub-framing@juxtaposition", "number of frames@multi frames", "realism@simplistic", "hue@colors", "region@color area", "effects@waves", "lines and arrows@direction", "purpose@design space", "time@moving", "number@Solo-user", "Specific part@hand", "discrete@Key press", "Interaction type@mid-air interaction", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays", "discrete@Symbolic gesture", "identifiers@title", "identifiers@letters"], "UIST18_paper485-Figure3-1.png": ["Specific part@hand", "UI@drawing", "point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "type@data visualization", "hue@colors", "grouping and linking@color grouping", "grouping and linking@identifiers grouping", "line style@dashed", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "activity@fabrication", "number@Solo-user", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "lines and arrows@direction", "realism@realistic"], "UIST18_paper499-Figure7-1.png": ["color@transparency", "type@photo", "point of view@3rd person", "number of frames@multi frames", "sub-framing@UI overlay", "UI@drawing", "hue@colors", "region@color area", "identifiers@letters", "purpose@design space", "time@still", "number@Multi-users", "body part@full body", "Interaction type@mid-air interaction", "Interaction type@controllers interaction", "device based@VR", "visual@head mounted displays", "Situation@Private indoors"], "UIST18_paper581-Figure10-1.png": ["type@photo", "background@grayed out/blurred", "point of view@3rd person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "activity@communication", "number@Solo-user", "body part@upper body", "discrete@Pointing", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays", "Situation@Public/private transport"], "UIST18_paper745-Figure13-1.png": ["Specific part@fingers", "type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "number@Solo-user", "discrete@Pointing", "device based@tangible objects", "Interaction type@tangible interaction", "visual@mobile displays"], "UIST18_paper779-Figure7-1.png": ["effects@stroboscopic", "point of view@UI only", "sub-framing@juxtaposition", "number of frames@multi frames", "point of view@top", "point of view@3rd person", "realism@simplistic", "UI@rendering", "hue@colors", "element@color highlight", "lines and arrows@direction", "lines and arrows@transfer", "identifiers@letters", "purpose@design space", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "discrete@Symbolic gesture", "Interaction type@touch interaction", "device based@mobile devices", "visual@mobile displays"], "UIST18_paper839-Figure7-1.png": ["type@photo", "point of view@1st person", "number of frames@multi frames", "point of view@UI only", "UI@rendering", "hue@colors", "color@transparency", "effects@stroboscopic", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper853-Figure7-1.png": ["UI@drawing", "point of view@3rd person", "number of frames@multi frames", "color@transparency", "hue@colors", "grouping and linking@color grouping", "lines and arrows@projection", "identifiers@letters", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Multi-users", "body part@upper body", "discrete@Pointing", "continuous@translating", "Interaction type@distal interaction", "device based@VR"], "UIST18_paper867-Figure3-1.png": ["point of view@UI only", "number of frames@one frame", "type@clipart/icon", "UI@rendering", "hue@colors", "lines and arrows@direction", "dynamic@Contact shapes", "purpose@interactive system", "time@still", "activity@data manipulation", "number@Solo-user", "Specific part@hand", "continuous@scaling", "continuous@scrolling", "continuous@translating", "Interaction type@mid-air interaction", "device based@VR", "visual@head mounted displays"], "UIST18_paper87-Figure11-1.png": ["type@photo", "point of view@overshoulder 3/4", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "activity@driving", "number@Solo-user", "body part@upper body", "Interaction type@controllers interaction", "Interaction type@distal interaction", "device based@controllers", "visual@lights", "Situation@Public indoors"], "UIST18_paper927-Figure6-1.png": ["type@photo", "point of view@3rd person", "point of view@UI only", "number of frames@multi frames", "sub-framing@inset (PiP)", "sub-framing@juxtaposition", "UI@rendering", "hue@colors", "lines and arrows@transfer", "lines and arrows@direction", "identifiers@letters", "purpose@design space", "purpose@interaction sequence", "time@moving", "activity@entertainment", "number@Solo-user", "body part@upper body", "discrete@Symbolic gesture", "continuous@rotating", "Interaction type@mid-air interaction", "device based@VR", "tracking based@inside out", "visual@head mounted displays"], "UIST18_paper99-Figure15-1.png": ["type@photo", "point of view@3rd person", "number of frames@one frame", "hue@colors", "purpose@interactive system", "time@still", "activity@fabrication", "number@Solo-user", "Specific part@hand", "continuous@deforming", "Interaction type@tangible interaction", "device based@tangible objects", "output modality@haptic"], "Ubicomp18_paper161-Figure2-1.png": ["type@text", "UI@drawing", "type@photo", "point of view@3rd person", "number of frames@one frame", "sub-framing@juxtaposition", "hue@colors", "element@color highlight", "grouping and linking@color grouping", "identifiers@title", "purpose@design space", "time@still", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects", "background@removed"], "Ubicomp18_paper198-Figure3-1.png": ["type@text", "Specific part@head", "point of view@3rd person", "line style@dashed", "line style@width", "hue@monochrome", "type@clipart/icon", "identifiers@title", "lines and arrows@direction", "purpose@design space", "time@still", "number@Solo-user", "continuous@translating", "continuous@rotating", "Interaction type@distal interaction", "realism@simplistic", "number of frames@multi frames"], "Ubicomp18_paper200-Figure7-1.png": ["type@photo", "point of view@3rd person", "number of frames@multi frames", "hue@colors", "identifiers@letters", "purpose@design space", "time@still", "activity@entertainment", "number@Solo-user", "Specific part@hand", "Interaction type@tangible interaction", "device based@tangible objects", "visual@computer displays"], "CHI18_paper107-Figure5-1.png": ["discrete@Symbolic gesture", "Situation@Desktop", "purpose@interaction sequence", "Interaction type@touch interaction", "Specific part@hand", "point of view@3rd person", "type@photo", "discrete@Pointing", "hue@colors", "number of frames@multi frames", "point of view@UI only", "identifiers@letters", "purpose@interactive system", "time@still", "activity@production"], "CHI18_paper11-Figure7-1.png": ["discrete@Key press", "identifiers@title", "Situation@Desktop", "purpose@interaction sequence", "point of view@1st person", "line style@width", "audible@speech output", "Specific part@hand", "type@text", "hue@grayscale", "region@color area", "sub-framing@UI embedded", "number of frames@multi frames", "realism@simplistic", "UI@rendering", "type@clipart/icon", "time@moving", "activity@data manipulation", "number@Solo-user", "Interaction type@computer interaction ", "device based@laptop devices", "visual@computer displays"], "CHI18_paper131-Figure1-1.png": ["type@photo", "point of view@1st person", "hue@colors", "Interaction type@touch interaction", "continuous@writing/drawing", "Specific part@hand", "number of frames@one frame", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@pen interaction"], "CHI18_paper150-Figure16-1.png": ["Interaction type@mid-air interaction", "continuous@translating", "point of view@1st person", "hue@colors", "device based@VR", "Specific part@hand", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "lines and arrows@direction", "measure@Text indicator", "purpose@design space", "time@still", "number@Solo-user", "visual@head mounted displays"], "CHI18_paper165-Figure1-1.png": ["point of view@3rd person", "activity@driving", "type@photo", "hue@colors", "body part@upper body", "device based@VR", "number of frames@one frame", "purpose@interactive system", "time@still", "number@Solo-user", "continuous@rotating", "Interaction type@controllers interaction", "device based@controllers", "Situation@Public/private transport", "visual@head mounted displays"], "CHI18_paper173-Figure1-1.png": ["Interaction type@mid-air interaction", "purpose@interaction sequence", "effects@stroboscopic", "line style@width", "body part@upper body", "point of view@3rd person", "Interaction type@distal interaction", "Specific part@hand", "purpose@design space", "hue@grayscale", "discrete@Pointing", "continuous@translating", "hue@colors", "measure@Arrows", "continuous@rotating", "number of frames@multi frames", "realism@simplistic", "type@text", "line style@dashed", "lines and arrows@direction", "lines and arrows@projection", "color@transparency", "identifiers@letters", "time@moving", "activity@data manipulation", "number@Solo-user", "tracking based@inside out", "device based@mobile devices", "Situation@Private indoors"], "CHI18_paper179-Figure6-1.png": ["point of view@3rd person", "purpose@design space", "type@photo", "hue@colors", "Specific part@hand", "number of frames@multi frames", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects", "Interaction type@mid-air interaction", "visual@lights", "visual@mobile displays"], "CHI18_paper18-Figure6-1.png": ["purpose@design space", "discrete@Pointing", "continuous@translating", "point of view@1st person", "hue@colors", "Specific part@hand", "number of frames@multi frames", "realism@simplistic", "time@still", "lines and arrows@projection", "activity@data manipulation", "number@Solo-user", "Interaction type@touch interaction", "Interaction type@distal interaction"], "CHI18_paper188-Figure4-1.png": ["point of view@3rd person", "activity@fabrication", "type@photo", "purpose@interaction sequence", "point of view@1st person", "hue@colors", "Specific part@hand", "number of frames@multi frames", "identifiers@letters", "time@moving"], "CHI18_paper19-Figure8-1.png": ["activity@data manipulation", "point of view@1st person", "Interaction type@touch interaction", "Specific part@hand", "purpose@design space", "discrete@Pointing", "continuous@translating", "UI@drawing", "realism@realistic", "hue@colors", "continuous@rotating", "number of frames@multi frames", "sub-framing@UI embedded", "element@color highlight", "lines and arrows@trajectories", "lines and arrows@direction", "identifiers@letters", "time@still", "number@Solo-user", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper19-Figure9-1.png": ["point of view@1st person", "Interaction type@touch interaction", "Specific part@hand", "purpose@design space", "type@photo", "continuous@translating", "hue@colors", "continuous@rotating", "sub-framing@juxtaposition", "number of frames@multi frames", "lines and arrows@direction", "identifiers@letters", "time@still", "activity@data manipulation", "number@Solo-user", "device based@mobile devices", "visual@mobile displays", "visual@large displays", "Interaction type@distal interaction"], "CHI18_paper199-Figure11-1.png": ["point of view@3rd person", "point of view@top", "type@photo", "continuous@translating", "hue@colors", "Specific part@hand", "point of view@UI only", "number of frames@multi frames", "sub-framing@UI overlay", "UI@rendering", "color@transparency", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper199-Figure12-1.png": ["Interaction type@distal interaction", "point of view@top", "type@photo", "purpose@interaction sequence", "hue@colors", "Specific part@hand", "number of frames@multi frames", "time@moving", "activity@2D/3D creation", "number@Solo-user", "discrete@Pointing", "continuous@writing/drawing", "device based@tangible objects", "Interaction type@tangible interaction", "device based@controllers", "Interaction type@controllers interaction", "visual@projected displays"], "CHI18_paper199-Figure14-1.png": ["point of view@top", "type@photo", "continuous@translating", "hue@colors", "Specific part@hand", "number of frames@multi frames", "identifiers@letters", "purpose@design space", "time@still", "activity@data manipulation", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "Situation@Desktop"], "CHI18_paper199-Figure15-1.png": ["point of view@top", "type@photo", "purpose@interaction sequence", "hue@colors", "Specific part@hand", "number of frames@multi frames", "identifiers@letters", "time@moving", "activity@data manipulation", "number@Solo-user", "continuous@scaling", "Interaction type@tangible interaction", "device based@tangible objects", "visual@projected displays", "Situation@Desktop"], "CHI18_paper199-Figure6-1.png": ["purpose@interaction sequence", "effects@stroboscopic", "Specific part@hand", "point of view@3rd person", "continuous@translating", "UI@drawing", "hue@colors", "measure@Arrows", "continuous@rotating", "number of frames@multi frames", "realism@simplistic", "element@color highlight", "line style@dashed", "lines and arrows@direction", "grouping and linking@color grouping", "identifiers@letters", "time@moving", "activity@data manipulation", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper21-Figure1-1.png": ["discrete@Key press", "point of view@top", "Interaction type@mid-air interaction", "Situation@Desktop", "type@photo", "hue@colors", "continuous@rotating", "Specific part@hand", "number of frames@one frame", "lines and arrows@direction", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@computer interaction ", "visual@computer displays", "device based@laptop devices"], "CHI18_paper218-Figure3-1.png": ["Interaction type@mid-air interaction", "discrete@Symbolic gesture", "purpose@interaction sequence", "continuous@scaling", "Specific part@hand", "point of view@3rd person", "Interaction type@distal interaction", "body part@full body", "UI@drawing", "hue@colors", "point of view@1st person", "number of frames@multi frames", "sub-framing@inset (PiP)", "realism@simplistic", "hue@grayscale", "element@color highlight", "grouping and linking@color grouping", "line style@dashed", "lines and arrows@direction", "identifiers@letters", "lines and arrows@projection", "time@moving", "activity@data manipulation", "number@Solo-user"], "CHI18_paper218-Figure5-1.png": ["point of view@top", "effects@stroboscopic", "body part@upper body", "Interaction type@distal interaction", "type@text", "hue@grayscale", "continuous@translating", "measure@Arrows", "number of frames@multi frames", "realism@simplistic", "line style@dashed", "measure@Text indicator", "lines and arrows@projection", "purpose@interactive system", "purpose@interaction sequence", "time@moving", "activity@data manipulation", "number@Multi-users"], "CHI18_paper218-Figure6-1.png": ["Interaction type@mid-air interaction", "Specific part@head", "point of view@1st person", "device based@VR", "Specific part@hand", "point of view@3rd person", "type@photo", "type@text", "hue@colors", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "grouping and linking@text annotation", "identifiers@letters", "purpose@interactive system", "time@still", "number@Solo-user", "visual@head mounted displays"], "CHI18_paper223-Figure3-1.png": ["identifiers@title", "point of view@top", "purpose@interaction sequence", "Interaction type@touch interaction", "continuous@writing/drawing", "Specific part@hand", "purpose@design space", "type@text", "discrete@Pointing", "continuous@translating", "UI@drawing", "hue@colors", "Interaction type@pen interaction", "number of frames@multi frames", "sub-framing@UI embedded", "realism@simplistic", "time@still", "activity@production", "activity@2D/3D creation", "number@Solo-user"], "CHI18_paper223-Figure5-1.png": ["Interaction type@touch interaction", "continuous@writing/drawing", "Specific part@hand", "discrete@Pointing", "continuous@translating", "UI@drawing", "hue@colors", "Interaction type@pen interaction", "point of view@UI only", "number of frames@one frame", "line style@dashed", "lines and arrows@trajectories", "purpose@interaction sequence", "time@moving", "activity@2D/3D creation", "number@Solo-user", "continuous@rotating", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper236-Figure2-1.png": ["point of view@3rd person", "type@photo", "discrete@Pointing", "Interaction type@on body interaction", "body part@full body", "hue@colors", "Specific part@hand", "point of view@overshoulder 3/4", "number of frames@one frame", "sub-framing@juxtaposition", "purpose@interactive system", "time@still", "activity@communication", "number@Solo-user", "sound based@speech input", "audible@speech output"], "CHI18_paper246-Figure3-1.png": ["point of view@3rd person", "activity@driving", "type@photo", "point of view@1st person", "hue@colors", "body part@upper body", "continuous@rotating", "Specific part@hand", "number of frames@multi frames", "purpose@interactive system", "time@still", "number@Solo-user", "Interaction type@controllers interaction", "device based@controllers", "visual@large displays"], "CHI18_paper248-Figure7-1.png": ["Specific part@fingers", "continuous@scaling", "effects@stroboscopic", "continuous@scrolling", "Interaction type@touch interaction", "purpose@design space", "type@text", "discrete@Pointing", "color@transparency", "continuous@translating", "hue@colors", "point of view@UI only", "number of frames@multi frames", "UI@rendering", "identifiers@letters", "lines and arrows@direction", "time@still", "number@Solo-user", "activity@data manipulation", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper258-Figure1-1.png": ["identifiers@title", "Interaction type@mid-air interaction", "effects@stroboscopic", "point of view@1st person", "body part@upper body", "point of view@3rd person", "Specific part@hand", "Interaction type@distal interaction", "purpose@design space", "type@text", "Interaction type@on body interaction", "color@transparency", "continuous@translating", "sub-framing@UI embedded", "realism@realistic", "hue@colors", "measure@Arrows", "effects@waves", "number of frames@multi frames", "grouping and linking@color grouping", "lines and arrows@trajectories", "time@moving", "discrete@Pointing", "device based@mobile devices", "tracking based@inside out", "visual@mobile displays", "output modality@haptic"], "CHI18_paper258-Figure2-1.png": ["purpose@design space", "Interaction type@on body interaction", "color@transparency", "region@color area", "UI@drawing", "realism@realistic", "point of view@1st person", "sub-framing@UI overlay", "hue@colors", "line style@width", "Interaction type@touch interaction", "Specific part@hand", "point of view@top", "number of frames@multi frames", "purpose@interactive system", "time@still", "number@Solo-user", "device based@mobile devices", "output modality@haptic"], "CHI18_paper281-Figure1-1.png": ["identifiers@title", "body part@upper body", "point of view@3rd person", "Interaction type@distal interaction", "point of view@overshoulder 3/4", "type@text", "hue@colors", "number of frames@multi frames", "realism@simplistic", "element@color highlight", "grouping and linking@color grouping", "purpose@design space", "time@still", "number@Solo-user", "tracking based@outside in", "device based@laptop devices", "visual@computer displays"], "CHI18_paper297-Figure2-1.png": ["point of view@top", "type@photo", "hue@colors", "Specific part@hand", "number of frames@one frame", "purpose@interactive system", "time@still", "number@Solo-user", "continuous@rotating", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper300-Figure4-1.png": ["point of view@3rd person", "identifiers@title", "type@photo", "type@text", "discrete@Pointing", "Interaction type@gaze interaction", "hue@colors", "body part@upper body", "number of frames@multi frames", "element@color highlight", "lines and arrows@projection", "purpose@design space", "time@still", "activity@communication", "number@Multi-users", "anonymization@blur", "Situation@Private indoors"], "CHI18_paper336-Figure5-1.png": ["point of view@3rd person", "Specific part@fingers", "activity@fabrication", "type@photo", "purpose@interaction sequence", "Interaction type@on body interaction", "hue@colors", "Specific part@foot", "number of frames@multi frames", "lines and arrows@direction", "identifiers@letters", "time@moving", "continuous@deforming", "device based@tangible objects", "Interaction type@tangible interaction"], "CHI18_paper349-Figure3-1.png": ["point of view@3rd person", "discrete@Key press", "hue@monochrome", "Situation@Desktop", "type@text", "discrete@Pointing", "Interaction type@gaze interaction", "hue@colors", "body part@upper body", "grouping and linking@grouping arrows", "lines and arrows@projection", "number of frames@one frame", "sub-framing@UI embedded", "realism@simplistic", "UI@drawing", "element@color highlight", "purpose@interactive system", "time@still", "number@Multi-users", "Interaction type@computer interaction ", "device based@desktop devices", "visual@computer displays"], "CHI18_paper354-Figure16-1.png": ["point of view@1st person", "Interaction type@touch interaction", "Specific part@hand", "type@photo", "type@text", "discrete@Pointing", "hue@colors", "background@removed", "grouping and linking@identifiers grouping", "number of frames@one frame", "purpose@interactive system", "time@still", "activity@production", "number@Solo-user", "device based@mobile devices", "visual@mobile displays", "grouping and linking@text annotation"], "CHI18_paper367-Figure1-1.png": ["point of view@3rd person", "identifiers@title", "purpose@design space", "type@text", "continuous@translating", "hue@colors", "continuous@rotating", "Specific part@hand", "number of frames@multi frames", "realism@simplistic", "lines and arrows@direction", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper367-Figure7-1.png": ["point of view@3rd person", "purpose@design space", "Situation@Desktop", "type@photo", "discrete@Pointing", "point of view@1st person", "hue@colors", "Interaction type@touch interaction", "continuous@rotating", "Specific part@hand", "number of frames@multi frames", "sub-framing@juxtaposition", "time@still", "activity@data manipulation", "number@Solo-user", "Interaction type@controllers interaction", "device based@desktop devices", "device based@large surfaces", "device based@controllers", "visual@computer displays"], "CHI18_paper374-Figure5-1.png": ["hue@monochrome", "purpose@design space", "continuous@deforming", "activity@fabrication", "type@photo", "point of view@1st person", "Specific part@hand", "number of frames@multi frames", "time@still", "number@Solo-user", "Interaction type@tangible interaction", "device based@tangible objects"], "CHI18_paper397-Figure5-1.png": ["point of view@3rd person", "Interaction type@distal interaction", "type@photo", "discrete@Pointing", "hue@colors", "body part@upper body", "number of frames@one frame", "purpose@interactive system", "time@still", "activity@entertainment", "number@Solo-user", "tracking based@outside in", "visual@mobile displays", "anonymization@blur"], "CHI18_paper4-Figure3-1.png": ["type@photo", "discrete@Pointing", "point of view@1st person", "hue@colors", "Interaction type@touch interaction", "Specific part@hand", "number of frames@one frame", "purpose@interactive system", "time@still", "number@Solo-user", "device based@mobile devices", "visual@mobile displays"], "CHI18_paper407-Figure1-1.png": ["point of view@top", "purpose@interaction sequence", "sub-framing@magnification lens", "point of view@1st person", "body part@upper body", "Interaction type@touch interaction", "Specific part@hand", "point of view@3rd person", "purpose@design space", "discrete@Pointing", "region@color area", "hue@colors", "number of frames@multi frames", "realism@simplistic", "element@color highlight", "grouping and linking@color grouping", "UI@rendering", "identifiers@numbers", "identifiers@letters", "lines and arrows@transfer", "time@moving", "activity@data manipulation", "number@Multi-users", "device based@mobile devices", "visual@mobile displays", "device based@tangible objects", "Interaction type@controllers interaction"], "CHI18_paper46-Figure3-1.png": ["device based@VR", "time@still", "visual@head mounted displays", "hue@colors", "number of frames@multi frames", "type@photo", "device based@AR", "tracking based@outside in", "purpose@design space", "color@transparency", "point of view@3rd person", "Interaction type@distal interaction", "point of view@1st person", "Situation@Private indoors", "identifiers@letters", "number@Multi-users", "UI@rendering", "sub-framing@UI overlay", "body part@upper body", "body part@full body"], "CHI18_paper86-Figure9-1.png": ["device based@VR", "Specific part@hand", "time@still", "visual@head mounted displays", "point of view@1st person", "hue@colors", "lines and arrows@trajectories", "number of frames@multi frames", "Interaction type@touch interaction", "number@Solo-user", "dynamic@Contact shapes", "continuous@translating", "purpose@design space", "UI@rendering", "discrete@Pointing"], "CHI18_paper638-Figure3-1.png": ["hue@colors", "visual@computer displays", "device based@desktop devices", "continuous@translating", "Interaction type@computer interaction ", "line style@dashed", "type@text", "lines and arrows@trajectories", "activity@data manipulation", "identifiers@numbers", "point of view@UI only", "time@moving", "UI@rendering", "number of frames@one frame", "effects@waves", "purpose@interaction sequence", "discrete@Pointing", "type@clipart/icon", "effects@stroboscopic", "grouping and linking@text annotation", "number@Solo-user"], "CSCW18_paper140-Figure2-1.png": ["Specific part@hand", "time@still", "visual@mobile displays", "discrete@Pointing", "hue@colors", "purpose@interactive system", "type@photo", "activity@data manipulation", "Interaction type@touch interaction", "continuous@scrolling", "number@Multi-users", "device based@mobile devices", "number of frames@one frame", "point of view@3rd person"], "CHI18_paper89-Figure6-1.png": ["hue@monochrome", "type@text", "lines and arrows@trajectories", "purpose@interactive system", "number@Solo-user", "realism@simplistic", "point of view@top", "number of frames@one frame", "line style@dashed", "line style@width", "grouping and linking@text annotation", "time@still", "body part@upper body"], "CHI18_paper46-Figure2-1.png": ["Specific part@hand", "visual@head mounted displays", "hue@colors", "number of frames@multi frames", "type@photo", "device based@AR", "color@transparency", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "point of view@1st person", "Interaction type@gaze interaction", "Situation@Private indoors", "identifiers@letters", "UI@rendering", "discrete@Pointing", "sub-framing@UI overlay", "purpose@interaction sequence", "time@moving", "activity@communication", "number@Solo-user"], "CHI18_paper579-Figure4-1.png": ["Specific part@hand", "visual@head mounted displays", "hue@colors", "activity@2D/3D creation", "number of frames@multi frames", "type@photo", "device based@AR", "tracking based@outside in", "body part@upper body", "continuous@translating", "device based@controllers", "color@transparency", "Interaction type@mid-air interaction", "point of view@3rd person", "Interaction type@distal interaction", "point of view@1st person", "number@Solo-user", "Interaction type@controllers interaction", "continuous@rotating", "identifiers@letters", "time@moving", "UI@rendering", "sub-framing@juxtaposition", "purpose@interaction sequence", "discrete@Pointing", "sub-framing@UI overlay", "lines and arrows@projection", "activity@fabrication"], "CHI18_paper99-Figure3-1.png": ["device based@VR", "sub-framing@inset (PiP)", "time@still", "visual@head mounted displays", "hue@colors", "purpose@interactive system", "number of frames@multi frames", "type@photo", "body part@full body", "tracking based@outside in", "point of view@1st person", "activity@data manipulation", "number@Solo-user", "discrete@Symbolic gesture", "point of view@3rd person", "UI@rendering", "point of view@UI only", "Interaction type@mid-air interaction", "Interaction type@distal interaction", "Interaction type@controllers interaction", "device based@controllers"], "UIST18_paper913-Figure8-1.png": ["time@still", "Interaction type@on body interaction", "hue@colors", "purpose@interactive system", "point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "number@Solo-user", "Interaction type@touch interaction", "device based@controllers", "identifiers@letters", "body part@upper body", "output modality@haptic", "sub-framing@juxtaposition", "point of view@top"], "UIST18_paper473-Figure3-1.png": ["sub-framing@inset (PiP)", "time@still", "Specific part@hand", "hue@colors", "number of frames@multi frames", "type@photo", "device based@tangible objects", "purpose@design space", "point of view@3rd person", "type@text", "Interaction type@tangible interaction", "identifiers@letters", "activity@communication", "number@Multi-users", "tracking based@inside out", "audible@speech output", "discrete@Pointing"], "CHI18_paper465-Figure5-1.png": ["Specific part@hand", "time@still", "measure@Text indicator", "hue@colors", "purpose@interactive system", "number of frames@multi frames", "type@photo", "device based@tangible objects", "type@text", "number@Solo-user", "identifiers@title", "identifiers@letters", "measure@Arrows", "Interaction type@tangible interaction", "line style@width", "activity@medical", "point of view@3rd person", "point of view@1st person", "grouping and linking@text annotation", "activity@fabrication", "tracking based@outside in"], "CHI18_paper43-Figure1-1.png": ["effects@stroboscopic", "sub-framing@inset (PiP)", "hue@monochrome", "time@still", "visual@mobile displays", "hue@colors", "number of frames@multi frames", "type@photo", "Interaction type@touch interaction", "discrete@Symbolic gesture", "device based@mobile devices", "realism@realistic", "point of view@3rd person", "point of view@1st person", "purpose@design space", "activity@data manipulation", "number@Solo-user", "body part@upper body", "Specific part@hand", "Interaction type@mid-air interaction"], "CHI18_paper446-Figure18-1.png": ["Specific part@hand", "time@still", "visual@mobile displays", "visual@head mounted displays", "hue@colors", "purpose@interactive system", "number of frames@multi frames", "type@photo", "device based@tangible objects", "body part@upper body", "output modality@haptic", "number@Solo-user", "activity@entertainment", "identifiers@letters", "UI@rendering", "Interaction type@tangible interaction", "tracking based@inside out", "Situation@Public indoors", "sub-framing@UI overlay", "device based@AR", "color@transparency", "point of view@3rd person", "point of view@1st person", "Interaction type@on body interaction", "continuous@rotating", "Interaction type@mid-air interaction"], "UIST18_paper335-Figure10-1.png": ["Specific part@hand", "visual@mobile displays", "hue@colors", "number of frames@multi frames", "type@photo", "continuous@translating", "activity@data manipulation", "number@Solo-user", "identifiers@letters", "continuous@scaling", "tracking based@inside out", "purpose@interaction sequence", "discrete@Pointing", "Interaction type@touch interaction", "continuous@rotating", "point of view@top", "time@moving", "device based@mobile devices", "discrete@Symbolic gesture", "Interaction type@mid-air interaction"], "UIST18_paper261-Figure4-1.png": ["time@still", "hue@colors", "activity@data manipulation", "number of frames@multi frames", "type@photo", "device based@tangible objects", "Specific part@fingers", "point of view@top", "purpose@design space", "number@Multi-users", "Interaction type@tangible interaction", "discrete@Pointing"], "UIST18_paper853-Figure9-1.png": ["device based@VR", "visual@head mounted displays", "hue@colors", "number of frames@multi frames", "body part@upper body", "color@transparency", "point of view@3rd person", "line style@dashed", "Interaction type@distal interaction", "lines and arrows@projection", "element@color highlight", "realism@simplistic", "grouping and linking@color grouping", "discrete@Pointing", "sub-framing@juxtaposition", "type@text", "identifiers@title", "purpose@design space", "activity@data manipulation", "time@still", "number@Multi-users"], "UIST18_paper737-Figure8-1.png": ["time@still", "hue@colors", "activity@data manipulation", "number of frames@multi frames", "type@photo", "number@Solo-user", "device based@tangible objects", "Specific part@fingers", "continuous@rotating", "identifiers@letters", "purpose@design space", "continuous@translating", "Interaction type@tangible interaction", "point of view@3rd person", "discrete@Key press", "output modality@haptic"], "Ubicomp18_paper201-Figure1-1.png": ["line style@width", "hue@colors", "visual@computer displays", "device based@controllers", "Interaction type@distal interaction", "point of view@1st person", "type@text", "identifiers@title", "number@Solo-user", "Interaction type@controllers interaction", "realism@simplistic", "time@moving", "activity@communication", "sub-framing@juxtaposition", "purpose@interaction sequence", "type@clipart/icon", "type@data visualization", "number of frames@one frame", "line style@dashed", "grouping and linking@color grouping", "element@color highlight", "Specific part@hand"], "UIST18_paper745-Figure4-1.png": ["time@still", "hue@colors", "purpose@interactive system", "type@photo", "Interaction type@touch interaction", "Specific part@fingers", "number@Solo-user", "point of view@top", "number of frames@one frame", "discrete@Pointing"], "CHI18_paper446-Figure8-1.png": ["visual@head mounted displays", "hue@colors", "point of view@overshoulder 3/4", "number of frames@multi frames", "type@photo", "device based@tangible objects", "body part@upper body", "line style@dashed", "lines and arrows@direction", "activity@data manipulation", "number@Solo-user", "Situation@Private indoors", "identifiers@letters", "purpose@interaction sequence", "tracking based@inside out", "Interaction type@tangible interaction", "line style@width", "sub-framing@UI overlay", "device based@AR", "continuous@rotating", "time@moving", "UI@drawing"], "CHI18_paper65-Figure2-1.png": ["effects@stroboscopic", "Specific part@hand", "measure@Text indicator", "hue@colors", "number of frames@multi frames", "type@photo", "tracking based@outside in", "continuous@translating", "device based@controllers", "point of view@3rd person", "lines and arrows@direction", "lines and arrows@projection", "UI@drawing", "number@Solo-user", "Interaction type@controllers interaction", "continuous@rotating", "identifiers@letters", "measure@Arrows", "time@moving", "purpose@interaction sequence", "discrete@Pointing", "sub-framing@UI overlay", "purpose@design space", "activity@data manipulation", "Interaction type@distal interaction", "output modality@haptic", "visual@head mounted displays", "device based@VR"], "CHI18_paper446-Figure2-1.png": ["visual@head mounted displays", "hue@colors", "number of frames@multi frames", "type@photo", "point of view@overshoulder 3/4", "device based@tangible objects", "body part@upper body", "output modality@haptic", "line style@dashed", "lines and arrows@direction", "type@text", "activity@data manipulation", "number@Solo-user", "realism@simplistic", "Situation@Private indoors", "identifiers@letters", "Interaction type@tangible interaction", "purpose@interaction sequence", "tracking based@inside out", "measure@Arrows", "line style@width", "sub-framing@UI overlay", "type@clipart/icon", "device based@AR", "color@transparency", "UI@drawing", "lines and arrows@trajectories", "continuous@rotating", "visual@lights", "time@moving", "grouping and linking@text annotation"], "CHI18_paper655-Figure2-1.png": ["sub-framing@inset (PiP)", "time@still", "Specific part@hand", "hue@colors", "purpose@interactive system", "type@photo", "device based@tangible objects", "point of view@3rd person", "point of view@1st person", "number@Solo-user", "continuous@rotating", "number of frames@one frame", "Interaction type@tangible interaction"], "UIST18_paper141-Figure10-1.png": ["Specific part@hand", "time@still", "continuous@deforming", "hue@colors", "number of frames@multi frames", "type@photo", "device based@tangible objects", "background@removed", "purpose@design space", "point of view@3rd person", "type@text", "activity@fabrication", "number@Solo-user", "identifiers@title", "continuous@rotating", "identifiers@letters", "Interaction type@tangible interaction"], "CHI18_paper603-Figure6-1.png": ["Specific part@hand", "measure@Text indicator", "hue@colors", "purpose@interactive system", "number of frames@multi frames", "type@photo", "point of view@3rd person", "type@text", "visual@large displays", "number@Solo-user", "Interaction type@pen interaction", "identifiers@letters", "measure@Arrows", "discrete@Pointing", "type@data visualization", "grouping and linking@text annotation", "lines and arrows@trajectories", "time@still", "device based@large surfaces", "device based@controllers"], "UIST18_paper485-Figure5-1.png": ["Specific part@hand", "time@still", "continuous@deforming", "hue@colors", "device based@tangible objects", "purpose@design space", "realism@realistic", "point of view@3rd person", "type@text", "identifiers@title", "number@Solo-user", "discrete@Symbolic gesture", "measure@Arrows", "Interaction type@tangible interaction", "number of frames@multi frames", "type@data visualization", "Interaction type@mid-air interaction"], "UIST18_paper867-Figure4-1.png": ["device based@VR", "sub-framing@inset (PiP)", "time@still", "Specific part@hand", "visual@head mounted displays", "enclosing@exact contour line", "hue@colors", "purpose@interactive system", "type@photo", "activity@medical", "Interaction type@mid-air interaction", "type@text", "number@Solo-user", "point of view@UI only", "discrete@Symbolic gesture", "UI@rendering", "number of frames@one frame", "realism@simplistic", "lines and arrows@direction"], "CHI18_paper76-Figure1-1.png": ["time@still", "visual@mobile displays", "hue@colors", "purpose@interactive system", "type@photo", "Interaction type@touch interaction", "point of view@overshoulder 3/4", "body part@upper body", "number@Solo-user", "device based@mobile devices", "number of frames@one frame", "Situation@Desktop"], "Ubicomp18_paper201-Figure9-1.png": ["Specific part@hand", "time@still", "hue@colors", "activity@production", "number of frames@multi frames", "type@photo", "device based@tangible objects", "line style@dashed", "visual@large displays", "number@Solo-user", "Situation@Private indoors", "identifiers@letters", "Interaction type@tangible interaction", "discrete@Pointing", "sub-framing@inset (PiP)", "enclosing@circle/rectangle", "sub-framing@magnification lens", "Interaction type@mid-air interaction", "point of view@3rd person", "point of view@1st person", "visual@lights", "Interaction type@pen interaction", "discrete@Symbolic gesture", "device based@mobile devices", "purpose@design space", "visual@mobile displays"], "UIST18_paper5-Figure2-1.png": ["Specific part@hand", "time@still", "continuous@deforming", "hue@colors", "number of frames@multi frames", "device based@tangible objects", "purpose@design space", "continuous@translating", "point of view@3rd person", "line style@dashed", "lines and arrows@direction", "type@text", "activity@data manipulation", "identifiers@title", "number@Solo-user", "element@color highlight", "realism@simplistic", "Interaction type@tangible interaction", "grouping and linking@color grouping"], "UIST18_paper19-Figure2-1.png": ["effects@stroboscopic", "visual@mobile displays", "measure@Text indicator", "hue@colors", "purpose@interactive system", "device based@AR", "body part@upper body", "continuous@translating", "grouping and linking@identifiers grouping", "line style@dashed", "Interaction type@distal interaction", "lines and arrows@trajectories", "realism@simplistic", "point of view@top", "number@Multi-users", "time@moving", "device based@mobile devices", "number of frames@one frame", "measure@Arrows", "grouping and linking@color grouping", "grouping and linking@text annotation"], "CHI18_paper42-Figure9-1.png": ["device based@VR", "time@still", "visual@head mounted displays", "hue@colors", "purpose@interactive system", "number@Solo-user", "body part@full body", "element@color highlight", "point of view@UI only", "UI@rendering", "number of frames@one frame", "Interaction type@mid-air interaction", "discrete@Pointing"], "UIST18_paper19-Figure9-1.png": ["time@still", "visual@mobile displays", "visual@head mounted displays", "hue@colors", "purpose@interactive system", "type@photo", "body part@full body", "number of frames@multi frames", "line style@dashed", "lines and arrows@direction", "type@text", "number@Solo-user", "Situation@Private indoors", "UI@rendering", "number of frames@one frame", "discrete@Pointing", "enclosing@circle/rectangle", "sub-framing@UI overlay", "device based@AR", "sub-framing@magnification lens", "color@transparency", "Interaction type@mid-air interaction", "point of view@1st person", "element@color highlight", "continuous@rotating", "point of view@top", "device based@mobile devices", "grouping and linking@color grouping", "sub-framing@inset (PiP)"], "CHI18_paper599-Figure8-1.png": ["time@still", "visual@head mounted displays", "hue@colors", "number of frames@multi frames", "type@photo", "body part@full body", "tracking based@outside in", "device based@controllers", "visual@large displays", "activity@entertainment", "identifiers@letters", "number@Multi-users", "sub-framing@juxtaposition", "discrete@Pointing", "device based@VR", "purpose@design space", "Interaction type@mid-air interaction", "point of view@3rd person", "device based@large surfaces", "Interaction type@distal interaction", "Interaction type@controllers interaction", "discrete@Symbolic gesture"], "UIST18_paper19-Figure12-1.png": ["visual@mobile displays", "hue@colors", "number of frames@multi frames", "body part@full body", "type@photo", "activity@entertainment", "identifiers@letters", "number@Multi-users", "UI@rendering", "purpose@interaction sequence", "discrete@Pointing", "Situation@Public indoors", "sub-framing@UI overlay", "Interaction type@touch interaction", "device based@AR", "point of view@UI only", "time@moving", "device based@mobile devices", "Interaction type@distal interaction"], "UIST18_paper757-Figure1-1.png": ["Specific part@hand", "line style@width", "time@still", "continuous@deforming", "hue@colors", "purpose@interactive system", "type@photo", "device based@tangible objects", "color@transparency", "line style@dashed", "point of view@1st person", "type@text", "number@Solo-user", "realism@simplistic", "number of frames@one frame", "Interaction type@tangible interaction", "grouping and linking@color grouping", "element@color highlight", "lines and arrows@direction", "grouping and linking@text annotation"], "CHI18_paper89-Figure3-1.png": ["Specific part@hand", "time@still", "visual@head mounted displays", "hue@colors", "purpose@interactive system", "number of frames@multi frames", "type@photo", "body part@full body", "device based@tangible objects", "number@Solo-user", "Situation@Private indoors", "activity@entertainment", "identifiers@letters", "UI@rendering", "tracking based@inside out", "Interaction type@tangible interaction", "device based@VR", "purpose@design space", "Interaction type@mid-air interaction", "point of view@3rd person", "point of view@1st person", "Interaction type@controllers interaction", "point of view@UI only", "discrete@Symbolic gesture", "tracking based@outside in"], "UIST18_paper637-Figure6-1.png": ["time@still", "hue@colors", "purpose@interactive system", "number of frames@multi frames", "activity@production", "activity@2D/3D creation", "visual@computer displays", "device based@desktop devices", "continuous@translating", "Interaction type@computer interaction ", "continuous@rotating", "point of view@UI only", "UI@rendering", "discrete@Pointing", "type@clipart/icon", "identifiers@letters"]}, "images_by_code_dict": {"point of view@UI only": ["CHI18_paper107-Figure2-1.png", "CHI18_paper184-Figure10-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper653-Figure5-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper185-Figure11-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper340-Figure3-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper529-Figure11-1.png", "CHI18_paper219-Figure14-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper237-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper5-Figure11-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper185-Figure12-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper436-Figure2-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper123-Figure9-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper218-Figure14-1.png", "UIST18_paper637-Figure4-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper223-Figure2-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper78-Figure2-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure12-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "point of view@3rd person": ["CHI18_paper107-Figure2-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper18-Figure4-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper711-Figure2-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "CHI18_paper446-Figure10-1.png", "Ubicomp18_paper170-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper241-Figure2-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper529-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper853-Figure5-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper380-Figure2-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper339-Figure5-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper622-Figure4-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper5-Figure7-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper589-Figure1-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper499-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper374-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper160-Figure3-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper508-Figure7-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper644-Figure11-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper541-Figure9-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper423-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper188-Figure2-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "UIST18_paper321-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper220-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper547-Figure8-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "CHI18_paper54-Figure4-1.png", "Ubicomp18_paper162-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper593-Figure4-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper61-Figure9-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper516-Figure1-1.png", "UIST18_paper779-Figure3-1.png", "CHI18_paper284-Figure6-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper426-Figure4-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper160-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "number of frames@multi frames": ["CHI18_paper107-Figure2-1.png", "UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper335-Figure7-1.png", "UIST18_paper711-Figure2-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper513-Figure9-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "CHI18_paper436-Figure7-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper529-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper853-Figure5-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper436-Figure6-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper339-Figure5-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper499-Figure5-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper654-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper446-Figure6-1.png", "UIST18_paper335-Figure9-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "CHI18_paper508-Figure7-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper473-Figure4-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper178-Figure1-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper436-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper123-Figure9-1.png", "Ubicomp18_paper181-Figure1-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "UIST18_paper485-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper779-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper248-Figure5-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper223-Figure2-1.png", "UIST18_paper499-Figure11-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "Ubicomp18_paper198-Figure3-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "UI@rendering": ["CHI18_paper107-Figure2-1.png", "UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper411-Figure3-1.png", "CHI18_paper234-Figure1-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper446-Figure10-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper185-Figure11-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper340-Figure3-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper529-Figure11-1.png", "CHI18_paper219-Figure14-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper638-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper82-Figure9-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper248-Figure3-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper5-Figure11-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper178-Figure1-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper653-Figure3-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper436-Figure2-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper150-Figure15-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper54-Figure7-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper218-Figure14-1.png", "UIST18_paper637-Figure4-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper19-Figure12-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "type@photo": ["CHI18_paper107-Figure2-1.png", "UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper558-Figure6-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper539-Figure1-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "Ubicomp18_paper170-Figure5-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper513-Figure9-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "UIST18_paper365-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "UIST18_paper321-Figure12-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper529-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper98-Figure4-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "UIST18_paper5-Figure8-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper436-Figure6-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper339-Figure5-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper622-Figure4-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper298-Figure3-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "UIST18_paper499-Figure5-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper336-Figure14-1.png", "UIST18_paper913-Figure11-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper446-Figure6-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper160-Figure3-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper162-Figure11-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "CHI18_paper508-Figure7-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper473-Figure4-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper20-Figure2-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper423-Figure8-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper511-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper362-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper401-Figure7-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper54-Figure4-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper61-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper516-Figure1-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper248-Figure5-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper160-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper163-Figure1-1.png", "UIST18_paper499-Figure11-1.png", "Ubicomp18_paper201-Figure3-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper927-Figure6-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper46-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper737-Figure8-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png"], "hue@colors": ["CHI18_paper107-Figure2-1.png", "UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper411-Figure3-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "Ubicomp18_paper170-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper529-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper401-Figure3-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper98-Figure4-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "UIST18_paper499-Figure10-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper339-Figure5-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper18-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper499-Figure5-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper201-Figure5-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper401-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper446-Figure6-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper160-Figure3-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper423-Figure8-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper529-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper401-Figure7-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper54-Figure4-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper61-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper160-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper223-Figure2-1.png", "CHI18_paper163-Figure1-1.png", "UIST18_paper499-Figure11-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper927-Figure6-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper638-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper737-Figure8-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "lines and arrows@trajectories": ["CHI18_paper107-Figure2-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper634-Figure12-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper185-Figure11-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper89-Figure15-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper380-Figure2-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper411-Figure10-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper436-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper661-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper547-Figure9-1.png", "UIST18_paper53-Figure2-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper248-Figure5-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper19-Figure2-1.png"], "purpose@interaction sequence": ["CHI18_paper107-Figure2-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper477-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper89-Figure15-1.png", "UIST18_paper853-Figure5-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper529-Figure11-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper150-Figure3-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper82-Figure9-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper99-Figure16-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper188-Figure2-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper238-Figure3-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper499-Figure12-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper223-Figure2-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper82-Figure7-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper335-Figure10-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper19-Figure12-1.png"], "purpose@interactive system": ["CHI18_paper107-Figure2-1.png", "UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper745-Figure10-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "Ubicomp18_paper170-Figure5-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper185-Figure11-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper288-Figure1-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper98-Figure4-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper362-Figure3-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper18-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "UIST18_paper499-Figure5-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "UIST18_paper499-Figure1-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper201-Figure5-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper401-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper160-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper423-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper336-Figure13-1.png", "CSCW18_paper159-Figure6-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper362-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper287-Figure2-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "CHI18_paper61-Figure9-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper73-Figure5-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper160-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper163-Figure1-1.png", "Ubicomp18_paper201-Figure3-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper99-Figure15-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper655-Figure2-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "UIST18_paper19-Figure2-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "time@moving": ["CHI18_paper107-Figure2-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper477-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper89-Figure15-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper219-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper150-Figure3-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper99-Figure16-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper411-Figure10-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper188-Figure2-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper626-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper238-Figure3-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper313-Figure4-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper223-Figure2-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper61-Figure10-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper335-Figure10-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper19-Figure12-1.png"], "number@Solo-user": ["CHI18_paper107-Figure2-1.png", "UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "Ubicomp18_paper170-Figure5-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper513-Figure9-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper675-Figure4-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper436-Figure6-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper634-Figure3-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper339-Figure5-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper589-Figure1-1.png", "UIST18_paper499-Figure5-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper401-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper446-Figure6-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper160-Figure3-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper725-Figure1-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper423-Figure8-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper188-Figure2-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper443-Figure2-1.png", "Ubicomp18_paper181-Figure1-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "CHI18_paper496-Figure5-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper593-Figure4-1.png", "CHI18_paper61-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper779-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper163-Figure1-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper927-Figure6-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper737-Figure8-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png"], "Specific part@fingers": ["CHI18_paper107-Figure2-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper745-Figure10-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper251-Figure1-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper76-Figure3-1.png", "UIST18_paper675-Figure2-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper177-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper675-Figure4-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper248-Figure6-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper336-Figure14-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper33-Figure5-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper626-Figure1-1.png", "UIST18_paper485-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper336-Figure15-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper745-Figure13-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper336-Figure5-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper737-Figure8-1.png", "UIST18_paper745-Figure4-1.png"], "discrete@Symbolic gesture": ["CHI18_paper107-Figure2-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper634-Figure12-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper825-Figure1-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper5-Figure3-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper65-Figure4-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper281-Figure8-1.png", "UIST18_paper595-Figure5-1.png", "UIST18_paper247-Figure10-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper626-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure8-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper547-Figure10-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper43-Figure1-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "Ubicomp18_paper201-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "Interaction type@touch interaction": ["CHI18_paper107-Figure2-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "UIST18_paper675-Figure2-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper529-Figure3-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper164-Figure1-1.png", "CSCW18_paper192-Figure4-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper19-Figure10-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper725-Figure1-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper737-Figure7-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper69-Figure5-1.png", "CSCW18_paper118-Figure6-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "CHI18_paper25-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper626-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "CSCW18_paper185-Figure5-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper150-Figure4-1.png", "UIST18_paper779-Figure3-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper90-Figure7-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper779-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper86-Figure9-1.png", "CSCW18_paper140-Figure2-1.png", "UIST18_paper913-Figure8-1.png", "CHI18_paper43-Figure1-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "UIST18_paper19-Figure12-1.png"], "device based@large surfaces": ["CHI18_paper107-Figure2-1.png", "CSCW18_paper128-Figure2-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper185-Figure11-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper436-Figure7-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper291-Figure7-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper291-Figure5-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "CSCW18_paper185-Figure5-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "CSCW18_paper185-Figure6-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper603-Figure6-1.png", "CHI18_paper599-Figure8-1.png"], "visual@large displays": ["CHI18_paper107-Figure2-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper653-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper185-Figure11-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper436-Figure7-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper564-Figure1-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper372-Figure1-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper291-Figure7-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper291-Figure5-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "CSCW18_paper185-Figure6-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper603-Figure6-1.png", "Ubicomp18_paper201-Figure9-1.png", "CHI18_paper599-Figure8-1.png"], "point of view@overshoulder 3/4": ["UIST18_paper45-Figure4-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper653-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper513-Figure9-1.png", "UIST18_paper365-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper45-Figure1-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper433-Figure1-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper446-Figure5-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper245-Figure6-1.png", "CSCW18_paper159-Figure6-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "Ubicomp18_paper162-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper446-Figure7-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "UIST18_paper87-Figure11-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper281-Figure1-1.png", "UIST18_paper913-Figure8-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper76-Figure1-1.png"], "point of view@1st person": ["UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper19-Figure2-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper450-Figure10-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "UIST18_paper867-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper521-Figure1-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper362-Figure3-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper439-Figure6-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "UIST18_paper927-Figure7-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "UIST18_paper5-Figure11-1.png", "CHI18_paper185-Figure9-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper185-Figure12-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper141-Figure14-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper360-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper626-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper362-Figure1-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper336-Figure4-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper426-Figure9-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper163-Figure1-1.png", "UIST18_paper499-Figure11-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper579-Figure13-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper839-Figure7-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper655-Figure2-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png"], "time@still": ["UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper411-Figure3-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "Ubicomp18_paper170-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper241-Figure2-1.png", "Ubicomp18_paper162-Figure8-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper529-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper98-Figure4-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper436-Figure6-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "UIST18_paper499-Figure10-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "CHI18_paper18-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper589-Figure1-1.png", "UIST18_paper499-Figure5-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper201-Figure5-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper401-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper160-Figure3-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper237-Figure1-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper209-Figure2-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper603-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper423-Figure8-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper589-Figure5-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper436-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure10-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper401-Figure7-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper54-Figure4-1.png", "Ubicomp18_paper162-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper61-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper516-Figure1-1.png", "UIST18_paper779-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper160-Figure6-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper163-Figure1-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper737-Figure8-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "body part@full body": ["UIST18_paper45-Figure4-1.png", "CHI18_paper558-Figure6-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper429-Figure1-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper241-Figure8-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper593-Figure5-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper499-Figure9-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper372-Figure1-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper201-Figure5-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper433-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper423-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper300-Figure5-1.png", "UIST18_paper511-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper593-Figure4-1.png", "CHI18_paper61-Figure9-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper499-Figure12-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper634-Figure2-1.png", "UIST18_paper499-Figure7-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "CHI18_paper89-Figure3-1.png"], "discrete@Pointing": ["UIST18_paper45-Figure4-1.png", "CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper18-Figure4-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper234-Figure1-1.png", "UIST18_paper745-Figure10-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper539-Figure1-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper497-Figure6-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper98-Figure4-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper18-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "CSCW18_paper192-Figure4-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper654-Figure5-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper237-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper595-Figure5-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper5-Figure11-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper603-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "UIST18_paper649-Figure5-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper436-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper25-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper5-Figure9-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper123-Figure7-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper626-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CSCW18_paper185-Figure5-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper218-Figure14-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper853-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper638-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper473-Figure3-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper603-Figure6-1.png", "Ubicomp18_paper201-Figure9-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "UIST18_paper637-Figure6-1.png"], "continuous@translating": ["UIST18_paper45-Figure4-1.png", "CHI18_paper218-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper365-Figure5-1.png", "UIST18_paper737-Figure1-1.png", "UIST18_paper321-Figure12-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper569-Figure27-1.png", "UIST18_paper853-Figure5-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper189-Figure11-1.png", "UIST18_paper499-Figure5-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "UIST18_paper725-Figure1-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper150-Figure13-1.png", "UIST18_paper321-Figure2-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper529-Figure1-1.png", "UIST18_paper737-Figure6-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper189-Figure9-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper238-Figure3-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper515-Figure8-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper637-Figure6-1.png"], "Interaction type@distal interaction": ["UIST18_paper45-Figure4-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "UIST18_paper853-Figure5-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper446-Figure6-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper160-Figure3-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper423-Figure8-1.png", "UIST18_paper853-Figure8-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper245-Figure6-1.png", "CSCW18_paper159-Figure6-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "UIST18_paper5-Figure9-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper593-Figure4-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "UIST18_paper499-Figure12-1.png", "CHI18_paper163-Figure1-1.png", "UIST18_paper499-Figure11-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper87-Figure11-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper65-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png"], "tracking based@inside out": ["UIST18_paper45-Figure4-1.png", "UIST18_paper335-Figure7-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper446-Figure10-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper288-Figure1-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper362-Figure3-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper78-Figure6-1.png", "UIST18_paper65-Figure2-1.png", "Ubicomp18_paper185-Figure3-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper362-Figure4-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper61-Figure9-1.png", "CHI18_paper95-Figure4-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper258-Figure1-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper89-Figure3-1.png"], "device based@AR": ["UIST18_paper45-Figure4-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper219-Figure14-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper446-Figure5-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper446-Figure18-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper19-Figure12-1.png"], "visual@head mounted displays": ["UIST18_paper45-Figure4-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper711-Figure2-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper54-Figure5-1.png", "UIST18_paper867-Figure2-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper219-Figure14-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper81-Figure10-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper446-Figure5-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper219-Figure13-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper853-Figure9-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "identifiers@letters": ["UIST18_paper45-Figure4-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper19-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper360-Figure5-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper853-Figure5-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper54-Figure5-1.png", "UIST18_paper5-Figure8-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "UIST18_paper499-Figure5-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper654-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "UIST18_paper927-Figure7-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper446-Figure6-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper237-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper185-Figure9-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper178-Figure1-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper603-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper436-Figure2-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper529-Figure14-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper779-Figure3-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper189-Figure8-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper248-Figure5-1.png", "UIST18_paper499-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "Ubicomp18_paper201-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "CHI18_paper89-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "sub-framing@UI overlay": ["UIST18_paper45-Figure4-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper219-Figure14-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper219-Figure13-1.png", "UIST18_paper499-Figure6-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper499-Figure7-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper446-Figure18-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper19-Figure12-1.png"], "Situation@Private indoors": ["UIST18_paper45-Figure4-1.png", "CHI18_paper558-Figure6-1.png", "UIST18_paper87-Figure1-1.png", "CHI18_paper446-Figure6-1.png", "UIST18_paper511-Figure12-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper202-Figure5-1.png", "CSCW18_paper159-Figure6-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "CHI18_paper61-Figure9-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper219-Figure13-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper160-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper499-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper89-Figure3-1.png"], "activity@data manipulation": ["CHI18_paper184-Figure10-1.png", "CHI18_paper218-Figure2-1.png", "CHI18_paper477-Figure2-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper745-Figure10-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper150-Figure23-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "UIST18_paper53-Figure6-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper629-Figure1-1.png", "UIST18_paper853-Figure5-1.png", "Ubicomp18_paper174-Figure1-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "Ubicomp18_paper200-Figure6-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper460-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper123-Figure7-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper43-Figure3-1.png", "CHI18_paper496-Figure5-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper11-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "UIST18_paper901-Figure9-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper515-Figure8-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper223-Figure2-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper638-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper43-Figure1-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper5-Figure2-1.png"], "continuous@writing/drawing": ["CHI18_paper184-Figure10-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper185-Figure7-1.png", "UIST18_paper335-Figure3-1.png", "Ubicomp18_paper162-Figure8-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper380-Figure2-1.png", "CHI18_paper82-Figure9-1.png", "CHI18_paper87-Figure1-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper205-Figure14-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper248-Figure9-1.png", "CHI18_paper185-Figure9-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper82-Figure2-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper223-Figure2-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png"], "Interaction type@computer interaction ": ["CHI18_paper184-Figure10-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper353-Figure4-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper123-Figure7-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper11-Figure1-1.png", "UIST18_paper637-Figure4-1.png", "UIST18_paper53-Figure7-1.png", "CHI18_paper426-Figure4-1.png", "UIST18_paper153-Figure3-1.png", "Ubicomp18_paper201-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper558-Figure2-1.png", "UIST18_paper347-Figure1-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper638-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "Interaction type@pen interaction": ["CHI18_paper184-Figure10-1.png", "UIST18_paper825-Figure2-1.png", "CSCW18_paper128-Figure2-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper185-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper185-Figure7-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper185-Figure12-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper223-Figure2-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper603-Figure6-1.png", "Ubicomp18_paper201-Figure9-1.png"], "device based@tangible objects": ["CHI18_paper184-Figure10-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper515-Figure7-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper499-Figure5-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper150-Figure9-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper236-Figure3-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper53-Figure5-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper569-Figure15-1.png", "UIST18_paper321-Figure2-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper199-Figure17-1.png", "UIST18_paper53-Figure2-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "UIST18_paper485-Figure9-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper446-Figure7-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "UIST18_paper485-Figure5-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png"], "device based@desktop devices": ["CHI18_paper184-Figure10-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper638-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper237-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper209-Figure2-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper603-Figure1-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper45-Figure3-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper426-Figure4-1.png", "Ubicomp18_paper201-Figure3-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper558-Figure2-1.png", "UIST18_paper347-Figure1-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper638-Figure3-1.png", "UIST18_paper637-Figure6-1.png"], "visual@computer displays": ["CHI18_paper184-Figure10-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "UIST18_paper335-Figure3-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper237-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper209-Figure2-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper320-Figure1-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper541-Figure8-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper95-Figure4-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper516-Figure1-1.png", "UIST18_paper53-Figure7-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper82-Figure7-1.png", "UIST18_paper347-Figure1-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper638-Figure3-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper637-Figure6-1.png"], "device based@mobile devices": ["CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "CHI18_paper173-Figure3-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper164-Figure1-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper19-Figure10-1.png", "UIST18_paper335-Figure9-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper595-Figure5-1.png", "UIST18_paper725-Figure1-1.png", "Ubicomp18_paper200-Figure6-1.png", "UIST18_paper737-Figure7-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper69-Figure5-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper82-Figure2-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper626-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "UIST18_paper779-Figure3-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "UIST18_paper53-Figure7-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper779-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper43-Figure1-1.png", "UIST18_paper335-Figure10-1.png", "CHI18_paper76-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper19-Figure12-1.png"], "visual@mobile displays": ["CHI18_paper184-Figure10-1.png", "Ubicomp18_paper164-Figure1-1.png", "CHI18_paper477-Figure2-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "UIST18_paper675-Figure2-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper164-Figure1-1.png", "CSCW18_paper192-Figure4-1.png", "CHI18_paper372-Figure1-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper19-Figure10-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper248-Figure9-1.png", "CHI18_paper433-Figure1-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper178-Figure1-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper69-Figure5-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper82-Figure2-1.png", "UIST18_paper485-Figure7-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper354-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper362-Figure1-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper406-Figure3-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "CHI18_paper163-Figure1-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper779-Figure7-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "CHI18_paper76-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper19-Figure12-1.png"], "sub-framing@inset (PiP)": ["Ubicomp18_paper164-Figure1-1.png", "CHI18_paper558-Figure6-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper347-Figure3-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper473-Figure4-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper541-Figure9-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper43-Figure3-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper11-Figure1-1.png", "UIST18_paper901-Figure9-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper867-Figure4-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure9-1.png"], "dynamic@Contact shapes": ["Ubicomp18_paper164-Figure1-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper634-Figure12-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper251-Figure2-1.png", "CHI18_paper529-Figure11-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper564-Figure1-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "UIST18_paper867-Figure3-1.png", "CHI18_paper86-Figure9-1.png"], "purpose@design space": ["Ubicomp18_paper164-Figure1-1.png", "CHI18_paper558-Figure6-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper411-Figure3-1.png", "UIST18_paper335-Figure7-1.png", "Ubicomp18_paper162-Figure9-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper362-Figure7-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "UIST18_paper557-Figure1-1.png", "UIST18_paper321-Figure12-1.png", "CHI18_paper89-Figure10-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper241-Figure8-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper634-Figure3-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper622-Figure4-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper589-Figure1-1.png", "UIST18_paper499-Figure5-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper654-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper913-Figure11-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper5-Figure3-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "UIST18_paper335-Figure9-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper188-Figure8-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper436-Figure2-1.png", "CHI18_paper460-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper661-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure10-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper54-Figure4-1.png", "Ubicomp18_paper162-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper613-Figure10-1.png", "UIST18_paper485-Figure9-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper362-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "UIST18_paper637-Figure4-1.png", "UIST18_paper779-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper499-Figure12-1.png", "CHI18_paper569-Figure17-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper43-Figure1-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "UIST18_paper485-Figure5-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "realism@simplistic": ["CHI18_paper218-Figure2-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper251-Figure1-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper89-Figure15-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper380-Figure2-1.png", "CHI18_paper363-Figure2-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "UIST18_paper19-Figure15-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper291-Figure6-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper589-Figure1-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper201-Figure5-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper547-Figure1-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper725-Figure1-1.png", "UIST18_paper473-Figure4-1.png", "UIST18_paper99-Figure16-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper241-Figure6-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper508-Figure3-1.png", "UIST18_paper321-Figure2-1.png", "CHI18_paper25-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper123-Figure9-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper779-Figure3-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper223-Figure2-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper73-Figure2-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper779-Figure7-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper89-Figure6-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper867-Figure4-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper757-Figure1-1.png"], "line style@width": ["CHI18_paper218-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper363-Figure2-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper589-Figure1-1.png", "UIST18_paper499-Figure1-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper241-Figure6-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper508-Figure3-1.png", "UIST18_paper877-Figure3-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper465-Figure5-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper757-Figure1-1.png"], "element@color highlight": ["CHI18_paper218-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper429-Figure1-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper380-Figure2-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper529-Figure11-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper374-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper725-Figure1-1.png", "UIST18_paper99-Figure16-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "UIST18_paper649-Figure5-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper541-Figure8-1.png", "UIST18_paper5-Figure9-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper123-Figure9-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper153-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper73-Figure2-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper779-Figure7-1.png", "Ubicomp18_paper161-Figure2-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper5-Figure2-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png"], "grouping and linking@color grouping": ["CHI18_paper218-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper429-Figure1-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper241-Figure7-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "UIST18_paper65-Figure2-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper411-Figure10-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper300-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "UIST18_paper5-Figure9-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper199-Figure3-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper853-Figure7-1.png", "Ubicomp18_paper161-Figure2-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper407-Figure1-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png"], "Specific part@hand": ["CHI18_paper218-Figure2-1.png", "CHI18_paper477-Figure2-1.png", "CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CHI18_paper18-Figure4-1.png", "UIST18_paper335-Figure7-1.png", "Ubicomp18_paper162-Figure9-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "UIST18_paper53-Figure6-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper185-Figure11-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "CHI18_paper497-Figure6-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "UIST18_paper321-Figure12-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper339-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper18-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper164-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper589-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper205-Figure14-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "UIST18_paper335-Figure9-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper433-Figure1-1.png", "UIST18_paper725-Figure1-1.png", "UIST18_paper473-Figure4-1.png", "UIST18_paper99-Figure16-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper378-Figure4-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper460-Figure1-1.png", "CHI18_paper188-Figure2-1.png", "UIST18_paper485-Figure7-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper202-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper360-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "CHI18_paper25-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper564-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper220-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure7-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper54-Figure7-1.png", "Ubicomp18_paper162-Figure1-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "CSCW18_paper185-Figure5-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper362-Figure1-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper779-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper248-Figure5-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper515-Figure8-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper569-Figure17-1.png", "CHI18_paper163-Figure1-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper86-Figure9-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png"], "Interaction type@mid-air interaction": ["CHI18_paper218-Figure2-1.png", "CHI18_paper558-Figure6-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper19-Figure2-1.png", "UIST18_paper5-Figure10-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "Ubicomp18_paper174-Figure1-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper589-Figure1-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper42-Figure2-1.png", "CHI18_paper460-Figure1-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper54-Figure4-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper284-Figure6-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper90-Figure7-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "Ubicomp18_paper201-Figure9-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "tracking based@outside in": ["CHI18_paper558-Figure6-1.png", "UIST18_paper87-Figure1-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper310-Figure1-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper558-Figure4-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper89-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "UIST18_paper499-Figure9-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper245-Figure12-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper499-Figure5-1.png", "UIST18_paper499-Figure15-1.png", "UIST18_paper499-Figure1-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper508-Figure7-1.png", "CHI18_paper89-Figure13-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper42-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper593-Figure4-1.png", "UIST18_paper87-Figure8-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper31-Figure3-1.png", "CHI18_paper163-Figure1-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "point of view@top": ["CHI18_paper477-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper251-Figure1-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper347-Figure3-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper547-Figure1-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper411-Figure10-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper529-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper33-Figure5-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "UIST18_paper335-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper189-Figure9-1.png", "CSCW18_paper185-Figure5-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper515-Figure8-1.png", "UIST18_paper335-Figure6-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "Ubicomp18_paper201-Figure3-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "CSCW18_paper185-Figure6-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper779-Figure7-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper89-Figure6-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper745-Figure4-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper19-Figure9-1.png"], "number of frames@one frame": ["CHI18_paper477-Figure2-1.png", "CHI18_paper18-Figure4-1.png", "CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper745-Figure10-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper251-Figure1-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper539-Figure1-1.png", "Ubicomp18_paper170-Figure5-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper42-Figure5-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper310-Figure1-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper89-Figure15-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper593-Figure5-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper18-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper209-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper589-Figure1-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "UIST18_paper499-Figure1-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper477-Figure3-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper401-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper160-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper89-Figure4-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper423-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper291-Figure5-1.png", "CSCW18_paper159-Figure6-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper529-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper354-Figure17-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "CHI18_paper61-Figure9-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper18-Figure2-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper160-Figure6-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper163-Figure1-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper31-Figure1-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper634-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper98-Figure2-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "CHI18_paper131-Figure1-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper4-Figure3-1.png", "CHI18_paper638-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "CHI18_paper89-Figure6-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper745-Figure4-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper76-Figure1-1.png", "UIST18_paper19-Figure2-1.png", "CHI18_paper42-Figure9-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png"], "sub-framing@magnification lens": ["CHI18_paper477-Figure2-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper251-Figure1-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper564-Figure1-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper477-Figure4-1.png", "Ubicomp18_paper194-Figure1-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper69-Figure4-1.png", "CHI18_paper407-Figure1-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure9-1.png"], "sub-framing@UI embedded": ["CHI18_paper477-Figure2-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper529-Figure6-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper529-Figure12-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper251-Figure1-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper634-Figure9-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper675-Figure2-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper288-Figure1-1.png", "UIST18_paper335-Figure3-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper436-Figure7-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure11-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper401-Figure3-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper173-Figure3-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper529-Figure13-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper567-Figure1-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper42-Figure1-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper189-Figure11-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper372-Figure1-1.png", "UIST18_paper765-Figure1-1.png", "UIST18_paper913-Figure11-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper19-Figure10-1.png", "UIST18_paper335-Figure9-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper433-Figure1-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper209-Figure2-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper178-Figure1-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper245-Figure6-1.png", "CHI18_paper291-Figure5-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper76-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper354-Figure17-1.png", "CHI18_paper401-Figure7-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper593-Figure2-1.png", "CHI18_paper43-Figure3-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper626-Figure1-1.png", "Ubicomp18_paper164-Figure2-1.png", "CHI18_paper593-Figure4-1.png", "CSCW18_paper185-Figure5-1.png", "Ubicomp18_paper164-Figure8-1.png", "CHI18_paper529-Figure14-1.png", "CHI18_paper362-Figure1-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper411-Figure6-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper515-Figure8-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper163-Figure1-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper249-Figure2-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper349-Figure3-1.png"], "type@clipart/icon": ["CHI18_paper477-Figure2-1.png", "CHI18_paper18-Figure4-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper426-Figure9-1.png", "CHI18_paper69-Figure4-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper69-Figure3-1.png", "UIST18_paper867-Figure3-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper638-Figure3-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper637-Figure6-1.png"], "region@color area": ["CHI18_paper477-Figure2-1.png", "UIST18_paper19-Figure10-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper634-Figure10-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper385-Figure1-1.png", "UIST18_paper499-Figure5-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper245-Figure2-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper477-Figure3-1.png", "UIST18_paper825-Figure9-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper603-Figure17-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper248-Figure9-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper603-Figure1-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper477-Figure4-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper219-Figure8-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper54-Figure4-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper499-Figure6-1.png", "UIST18_paper499-Figure11-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper499-Figure7-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper407-Figure1-1.png"], "lines and arrows@transfer": ["CHI18_paper477-Figure2-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper132-Figure1-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper603-Figure17-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper99-Figure16-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper407-Figure1-1.png"], "continuous@deforming": ["CHI18_paper477-Figure2-1.png", "Ubicomp18_paper162-Figure9-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper579-Figure9-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper697-Figure3-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper380-Figure5-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper629-Figure7-1.png", "UIST18_paper99-Figure16-1.png", "CHI18_paper411-Figure10-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper446-Figure12-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper320-Figure1-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper82-Figure7-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper99-Figure15-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper374-Figure5-1.png", "UIST18_paper141-Figure10-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper757-Figure1-1.png"], "grouping and linking@text annotation": ["CHI18_paper477-Figure2-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper429-Figure1-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper218-Figure7-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "CHI18_paper363-Figure2-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper199-Figure4-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper547-Figure1-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper248-Figure9-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper300-Figure5-1.png", "UIST18_paper321-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper411-Figure6-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper69-Figure4-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper757-Figure1-1.png"], "grouping and linking@identifiers grouping": ["CHI18_paper477-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper558-Figure4-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper477-Figure3-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper541-Figure9-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "UIST18_paper335-Figure2-1.png", "UIST18_paper499-Figure6-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "UIST18_paper485-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "UIST18_paper19-Figure2-1.png"], "hue@monochrome": ["CHI18_paper202-Figure1-1.png", "CHI18_paper437-Figure3-1.png", "CSCW18_paper151-Figure10-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper378-Figure3-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper89-Figure15-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper589-Figure1-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper436-Figure2-1.png", "CHI18_paper220-Figure2-1.png", "UIST18_paper779-Figure3-1.png", "CHI18_paper255-Figure5-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper558-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper43-Figure1-1.png"], "identifiers@title": ["CHI18_paper202-Figure1-1.png", "Ubicomp18_paper162-Figure9-1.png", "CHI18_paper210-Figure1-1.png", "UIST18_paper825-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper497-Figure6-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper638-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper508-Figure7-1.png", "CHI18_paper411-Figure10-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper150-Figure4-1.png", "UIST18_paper779-Figure3-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper153-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper95-Figure3-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper465-Figure5-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "UIST18_paper141-Figure10-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper5-Figure2-1.png"], "identifiers@numbers": ["CHI18_paper437-Figure3-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper82-Figure9-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper82-Figure7-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper638-Figure3-1.png"], "lines and arrows@direction": ["CHI18_paper411-Figure3-1.png", "UIST18_paper335-Figure7-1.png", "CHI18_paper19-Figure2-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper150-Figure23-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper634-Figure9-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper53-Figure6-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper89-Figure14-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper569-Figure27-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper251-Figure2-1.png", "UIST18_paper321-Figure1-1.png", "UIST18_paper697-Figure3-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper589-Figure1-1.png", "CHI18_paper336-Figure6-1.png", "CHI18_paper245-Figure2-1.png", "UIST18_paper867-Figure5-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper374-Figure4-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper541-Figure5-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "CHI18_paper291-Figure5-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper406-Figure4-1.png", "UIST18_paper5-Figure9-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper547-Figure8-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "UIST18_paper313-Figure4-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper73-Figure2-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper927-Figure6-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper867-Figure4-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png"], "activity@2D/3D creation": ["CHI18_paper411-Figure3-1.png", "CHI18_paper185-Figure7-1.png", "UIST18_paper335-Figure3-1.png", "CHI18_paper579-Figure9-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper335-Figure9-1.png", "UIST18_paper5-Figure11-1.png", "CHI18_paper185-Figure9-1.png", "CHI18_paper185-Figure12-1.png", "UIST18_paper649-Figure6-1.png", "UIST18_paper649-Figure5-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper579-Figure7-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper529-Figure14-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper238-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper637-Figure6-1.png"], "lines and arrows@projection": ["CHI18_paper18-Figure4-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper436-Figure7-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper380-Figure2-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper654-Figure5-1.png", "CHI18_paper547-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper18-Figure2-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper647-Figure2-1.png", "UIST18_paper853-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper18-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "CHI18_paper65-Figure2-1.png"], "color@transparency": ["UIST18_paper19-Figure10-1.png", "CHI18_paper529-Figure6-1.png", "CHI18_paper248-Figure4-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper529-Figure8-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper219-Figure14-1.png", "CHI18_paper610-Figure1-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper150-Figure3-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper81-Figure10-1.png", "UIST18_paper499-Figure5-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper224-Figure4-1.png", "CHI18_paper547-Figure1-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper529-Figure4-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper725-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper853-Figure8-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper5-Figure9-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper54-Figure4-1.png", "UIST18_paper335-Figure1-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper248-Figure5-1.png", "UIST18_paper499-Figure6-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper90-Figure7-1.png", "CHI18_paper95-Figure3-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper46-Figure2-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper853-Figure9-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png"], "line style@dashed": ["UIST18_paper19-Figure10-1.png", "CHI18_paper234-Figure1-1.png", "CHI18_paper634-Figure12-1.png", "CHI18_paper446-Figure10-1.png", "UIST18_paper825-Figure1-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper89-Figure15-1.png", "UIST18_paper853-Figure5-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper436-Figure6-1.png", "UIST18_paper19-Figure15-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper589-Figure5-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper436-Figure2-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper25-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "UIST18_paper853-Figure4-1.png", "UIST18_paper335-Figure2-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper547-Figure8-1.png", "CHI18_paper54-Figure4-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper199-Figure3-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper73-Figure2-1.png", "UIST18_paper485-Figure3-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper89-Figure6-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png"], "effects@stroboscopic": ["UIST18_paper19-Figure10-1.png", "CHI18_paper150-Figure23-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper569-Figure27-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper150-Figure3-1.png", "UIST18_paper275-Figure4-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper547-Figure1-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper291-Figure7-1.png", "CHI18_paper589-Figure5-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "UIST18_paper853-Figure4-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper69-Figure2-1.png", "UIST18_paper53-Figure2-1.png", "UIST18_paper335-Figure1-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper238-Figure3-1.png", "CHI18_paper248-Figure5-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper65-Figure2-1.png", "UIST18_paper19-Figure2-1.png"], "Situation@Public indoors": ["UIST18_paper19-Figure10-1.png", "CHI18_paper61-Figure1-1.png", "UIST18_paper87-Figure11-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper19-Figure12-1.png"], "continuous@scaling": ["UIST18_paper335-Figure7-1.png", "CHI18_paper76-Figure3-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper142-Figure1-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper25-Figure2-1.png", "UIST18_paper637-Figure4-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper867-Figure3-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "UIST18_paper335-Figure10-1.png"], "Interaction type@on body interaction": ["UIST18_paper335-Figure7-1.png", "UIST18_paper711-Figure2-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper320-Figure11-1.png", "UIST18_paper365-Figure5-1.png", "UIST18_paper697-Figure3-1.png", "CHI18_paper564-Figure1-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper765-Figure1-1.png", "UIST18_paper65-Figure2-1.png", "Ubicomp18_paper185-Figure3-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper564-Figure2-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper61-Figure9-1.png", "CHI18_paper95-Figure4-1.png", "UIST18_paper745-Figure11-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper362-Figure5-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper336-Figure5-1.png", "UIST18_paper913-Figure8-1.png", "CHI18_paper446-Figure18-1.png"], "Situation@Desktop": ["UIST18_paper335-Figure7-1.png", "CHI18_paper132-Figure4-1.png", "CHI18_paper629-Figure6-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper95-Figure4-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper515-Figure8-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper76-Figure1-1.png"], "Interaction type@tangible interaction": ["CHI18_paper198-Figure9-1.png", "UIST18_paper87-Figure1-1.png", "UIST18_paper745-Figure10-1.png", "Ubicomp18_paper162-Figure9-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper291-Figure2-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper46-Figure4-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper579-Figure9-1.png", "UIST18_paper321-Figure12-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper613-Figure1-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper89-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper291-Figure6-1.png", "CHI18_paper298-Figure3-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper499-Figure5-1.png", "UIST18_paper913-Figure5-1.png", "CHI18_paper336-Figure6-1.png", "UIST18_paper745-Figure8-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper150-Figure9-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "UIST18_paper913-Figure11-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper502-Figure7-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper162-Figure11-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper508-Figure7-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper411-Figure10-1.png", "Ubicomp18_paper200-Figure6-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper53-Figure5-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper502-Figure1-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "CHI18_paper569-Figure15-1.png", "UIST18_paper321-Figure2-1.png", "UIST18_paper5-Figure7-1.png", "UIST18_paper737-Figure6-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper199-Figure17-1.png", "UIST18_paper53-Figure2-1.png", "Ubicomp18_paper162-Figure1-1.png", "CHI18_paper189-Figure9-1.png", "CHI18_paper613-Figure10-1.png", "UIST18_paper485-Figure9-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper199-Figure7-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper446-Figure11-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper446-Figure7-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper745-Figure13-1.png", "UIST18_paper99-Figure15-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper199-Figure11-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper374-Figure5-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "UIST18_paper485-Figure5-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper757-Figure1-1.png", "CHI18_paper89-Figure3-1.png"], "body part@upper body": ["UIST18_paper87-Figure1-1.png", "CHI18_paper132-Figure4-1.png", "UIST18_paper913-Figure12-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper446-Figure10-1.png", "Ubicomp18_paper170-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper202-Figure4-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper46-Figure4-1.png", "Ubicomp18_paper184-Figure4-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper529-Figure3-1.png", "UIST18_paper853-Figure5-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper567-Figure1-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper622-Figure4-1.png", "CHI18_paper291-Figure6-1.png", "CSCW18_paper192-Figure4-1.png", "UIST18_paper913-Figure5-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper78-Figure6-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper547-Figure1-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper160-Figure3-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper644-Figure11-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper541-Figure9-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper320-Figure1-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper202-Figure5-1.png", "CSCW18_paper159-Figure6-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper220-Figure2-1.png", "CSCW18_paper072-Figure5-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper443-Figure2-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper613-Figure10-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper165-Figure3-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "CSCW18_paper140-Figure3-1.png", "CSCW18_paper140-Figure5-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper581-Figure10-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper87-Figure11-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper89-Figure6-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper913-Figure8-1.png", "CHI18_paper43-Figure1-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper853-Figure9-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper76-Figure1-1.png", "UIST18_paper19-Figure2-1.png"], "type@text": ["UIST18_paper711-Figure2-1.png", "CHI18_paper132-Figure4-1.png", "Ubicomp18_paper162-Figure9-1.png", "CHI18_paper210-Figure1-1.png", "CHI18_paper360-Figure5-1.png", "CHI18_paper251-Figure1-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper653-Figure5-1.png", "CHI18_paper81-Figure1-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper27-Figure2-1.png", "CHI18_paper497-Figure6-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper251-Figure2-1.png", "CHI18_paper363-Figure2-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper380-Figure5-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper18-Figure5-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper248-Figure6-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "CHI18_paper589-Figure1-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper245-Figure2-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper477-Figure3-1.png", "CHI18_paper201-Figure5-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper65-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper547-Figure1-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper210-Figure3-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper248-Figure9-1.png", "CHI18_paper508-Figure7-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper411-Figure10-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper69-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper477-Figure4-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper219-Figure8-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper142-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper220-Figure2-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper199-Figure17-1.png", "CHI18_paper69-Figure2-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper613-Figure10-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper223-Figure7-1.png", "UIST18_paper853-Figure6-1.png", "CHI18_paper248-Figure5-1.png", "CHI18_paper477-Figure1-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper153-Figure3-1.png", "Ubicomp18_paper201-Figure3-1.png", "UIST18_paper853-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper219-Figure12-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper445-Figure4-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper547-Figure5-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper564-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper73-Figure2-1.png", "CHI18_paper95-Figure3-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "Ubicomp18_paper161-Figure2-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper281-Figure1-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper638-Figure3-1.png", "CHI18_paper89-Figure6-1.png", "UIST18_paper473-Figure3-1.png", "CHI18_paper465-Figure5-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper446-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper867-Figure4-1.png", "UIST18_paper5-Figure2-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper757-Figure1-1.png"], "measure@Arrows": ["UIST18_paper711-Figure2-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper496-Figure5-1.png", "UIST18_paper901-Figure9-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png", "UIST18_paper19-Figure2-1.png"], "measure@Text indicator": ["UIST18_paper711-Figure2-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper42-Figure1-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper245-Figure3-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper150-Figure15-1.png", "Ubicomp18_paper194-Figure1-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper95-Figure3-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper465-Figure5-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper19-Figure2-1.png"], "Specific part@head": ["UIST18_paper711-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper54-Figure6-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper218-Figure6-1.png"], "device based@VR": ["UIST18_paper711-Figure2-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper653-Figure5-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper54-Figure5-1.png", "UIST18_paper867-Figure2-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper289-Figure1-1.png", "UIST18_paper839-Figure8-1.png", "UIST18_paper499-Figure5-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper89-Figure16-1.png", "CHI18_paper150-Figure9-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper654-Figure5-1.png", "UIST18_paper867-Figure5-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper78-Figure6-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper89-Figure11-1.png", "UIST18_paper5-Figure11-1.png", "CHI18_paper653-Figure3-1.png", "CHI18_paper241-Figure6-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper150-Figure2-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "UIST18_paper511-Figure2-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper150-Figure4-1.png", "UIST18_paper901-Figure9-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper89-Figure7-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper218-Figure8-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper78-Figure2-1.png", "CHI18_paper90-Figure7-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper839-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "UIST18_paper867-Figure3-1.png", "UIST18_paper927-Figure6-1.png", "CHI18_paper150-Figure16-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper218-Figure6-1.png", "CHI18_paper46-Figure3-1.png", "CHI18_paper86-Figure9-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper853-Figure9-1.png", "CHI18_paper65-Figure2-1.png", "UIST18_paper867-Figure4-1.png", "CHI18_paper42-Figure9-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "visual@lights": ["UIST18_paper745-Figure10-1.png", "UIST18_paper485-Figure6-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper513-Figure9-1.png", "CHI18_paper569-Figure27-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper5-Figure7-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper569-Figure15-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper613-Figure10-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper61-Figure9-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper284-Figure7-1.png", "Ubicomp18_paper201-Figure3-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper61-Figure10-1.png", "UIST18_paper87-Figure11-1.png", "CHI18_paper179-Figure6-1.png", "CHI18_paper446-Figure2-1.png", "Ubicomp18_paper201-Figure9-1.png"], "activity@fabrication": ["Ubicomp18_paper162-Figure9-1.png", "CHI18_paper188-Figure5-1.png", "CHI18_paper177-Figure2-1.png", "CHI18_paper579-Figure9-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper380-Figure2-1.png", "UIST18_paper5-Figure8-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper579-Figure8-1.png", "UIST18_paper127-Figure16-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper745-Figure8-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper5-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper5-Figure9-1.png", "CHI18_paper374-Figure4-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper502-Figure7-1.png", "UIST18_paper595-Figure5-1.png", "UIST18_paper99-Figure16-1.png", "CHI18_paper470-Figure3-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper188-Figure8-1.png", "UIST18_paper53-Figure5-1.png", "UIST18_paper141-Figure14-1.png", "CHI18_paper188-Figure2-1.png", "CHI18_paper162-Figure6-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper5-Figure1-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper219-Figure13-1.png", "CHI18_paper531-Figure2-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper441-Figure21-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper82-Figure7-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper99-Figure15-1.png", "CHI18_paper188-Figure4-1.png", "CHI18_paper336-Figure5-1.png", "CHI18_paper374-Figure5-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper465-Figure5-1.png", "UIST18_paper141-Figure10-1.png"], "output modality@haptic": ["UIST18_paper913-Figure12-1.png", "CHI18_paper429-Figure1-1.png", "CHI18_paper150-Figure23-1.png", "UIST18_paper5-Figure10-1.png", "CHI18_paper446-Figure10-1.png", "UIST18_paper839-Figure3-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper450-Figure10-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "UIST18_paper737-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "UIST18_paper913-Figure10-1.png", "CHI18_paper150-Figure3-1.png", "UIST18_paper927-Figure5-1.png", "CHI18_paper291-Figure6-1.png", "UIST18_paper913-Figure5-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper336-Figure14-1.png", "UIST18_paper913-Figure11-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper150-Figure20-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper5-Figure11-1.png", "UIST18_paper247-Figure10-1.png", "UIST18_paper737-Figure7-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper53-Figure5-1.png", "CHI18_paper446-Figure12-1.png", "CHI18_paper541-Figure8-1.png", "UIST18_paper5-Figure7-1.png", "UIST18_paper737-Figure6-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper291-Figure9-1.png", "UIST18_paper53-Figure2-1.png", "UIST18_paper53-Figure7-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper291-Figure8-1.png", "CHI18_paper654-Figure4-1.png", "UIST18_paper99-Figure15-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "UIST18_paper913-Figure8-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png"], "continuous@rotating": ["CHI18_paper19-Figure2-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper291-Figure2-1.png", "CHI18_paper42-Figure5-1.png", "CHI18_paper21-Figure2-1.png", "CHI18_paper547-Figure2-1.png", "UIST18_paper321-Figure12-1.png", "UIST18_paper321-Figure1-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper164-Figure1-1.png", "UIST18_paper877-Figure9-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper589-Figure1-1.png", "UIST18_paper927-Figure7-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper19-Figure10-1.png", "CHI18_paper446-Figure6-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "CHI18_paper613-Figure7-1.png", "UIST18_paper321-Figure2-1.png", "UIST18_paper853-Figure4-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper89-Figure9-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper238-Figure3-1.png", "CHI18_paper336-Figure4-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper238-Figure7-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper547-Figure7-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper73-Figure2-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper927-Figure6-1.png", "Ubicomp18_paper198-Figure3-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper297-Figure2-1.png", "CHI18_paper367-Figure1-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper446-Figure18-1.png", "UIST18_paper335-Figure10-1.png", "UIST18_paper737-Figure8-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png", "CHI18_paper655-Figure2-1.png", "UIST18_paper141-Figure10-1.png", "UIST18_paper19-Figure9-1.png", "UIST18_paper637-Figure6-1.png"], "sub-framing@juxtaposition": ["CSCW18_paper151-Figure10-1.png", "UIST18_paper825-Figure2-1.png", "CHI18_paper529-Figure8-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper188-Figure5-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper340-Figure3-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper634-Figure3-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper334-Figure5-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper19-Figure3-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper185-Figure12-1.png", "CHI18_paper178-Figure1-1.png", "CHI18_paper188-Figure8-1.png", "CHI18_paper42-Figure2-1.png", "CHI18_paper508-Figure3-1.png", "CHI18_paper336-Figure13-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper529-Figure1-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper54-Figure10-1.png", "CHI18_paper354-Figure17-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper547-Figure9-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper547-Figure8-1.png", "UIST18_paper53-Figure2-1.png", "Ubicomp18_paper162-Figure1-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper189-Figure8-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper31-Figure3-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper336-Figure15-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper547-Figure10-1.png", "CHI18_paper654-Figure4-1.png", "UIST18_paper335-Figure8-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper779-Figure7-1.png", "UIST18_paper927-Figure6-1.png", "Ubicomp18_paper161-Figure2-1.png", "CHI18_paper19-Figure9-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper579-Figure4-1.png", "UIST18_paper913-Figure8-1.png", "UIST18_paper853-Figure9-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper599-Figure8-1.png"], "realism@realistic": ["CHI18_paper210-Figure1-1.png", "CHI18_paper634-Figure11-1.png", "CHI18_paper634-Figure10-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper82-Figure9-1.png", "Ubicomp18_paper194-Figure1-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper647-Figure2-1.png", "UIST18_paper485-Figure3-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper43-Figure1-1.png", "UIST18_paper485-Figure5-1.png"], "number@Multi-users": ["CHI18_paper210-Figure1-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper160-Figure2-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper853-Figure5-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper593-Figure5-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper622-Figure4-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper289-Figure1-1.png", "CSCW18_paper192-Figure4-1.png", "CHI18_paper372-Figure1-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper266-Figure7-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper241-Figure6-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper142-Figure1-1.png", "CHI18_paper300-Figure5-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper287-Figure2-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper189-Figure9-1.png", "CSCW18_paper185-Figure5-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper73-Figure5-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper551-Figure3-1.png", "UIST18_paper499-Figure12-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper299-Figure2-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper90-Figure7-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "CHI18_paper218-Figure5-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper46-Figure3-1.png", "CSCW18_paper140-Figure2-1.png", "UIST18_paper473-Figure3-1.png", "UIST18_paper261-Figure4-1.png", "UIST18_paper853-Figure9-1.png", "UIST18_paper19-Figure2-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png"], "discrete@Key press": ["CHI18_paper210-Figure1-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper378-Figure3-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "CHI18_paper558-Figure4-1.png", "CHI18_paper151-Figure4-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper11-Figure5-1.png", "UIST18_paper347-Figure3-1.png", "CHI18_paper82-Figure9-1.png", "UIST18_paper913-Figure5-1.png", "UIST18_paper765-Figure1-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper654-Figure5-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper446-Figure3-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper963-Figure4-1.png", "CHI18_paper248-Figure9-1.png", "CHI18_paper508-Figure7-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper470-Figure3-1.png", "CHI18_paper446-Figure5-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper378-Figure4-1.png", "CHI18_paper508-Figure3-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper353-Figure4-1.png", "CHI18_paper362-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper579-Figure7-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper362-Figure1-1.png", "CHI18_paper426-Figure4-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper558-Figure2-1.png", "CHI18_paper647-Figure2-1.png", "UIST18_paper347-Figure1-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper349-Figure3-1.png", "UIST18_paper737-Figure8-1.png"], "UI@drawing": ["CHI18_paper634-Figure12-1.png", "CHI18_paper291-Figure2-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper81-Figure1-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper241-Figure8-1.png", "CHI18_paper219-Figure2-1.png", "CHI18_paper199-Figure4-1.png", "CHI18_paper610-Figure1-1.png", "CHI18_paper291-Figure3-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper529-Figure7-1.png", "CHI18_paper291-Figure4-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "UIST18_paper725-Figure1-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper603-Figure1-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper291-Figure5-1.png", "CHI18_paper150-Figure13-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper220-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper123-Figure7-1.png", "CHI18_paper123-Figure9-1.png", "CHI18_paper199-Figure3-1.png", "CHI18_paper150-Figure4-1.png", "CHI18_paper406-Figure3-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper477-Figure1-1.png", "UIST18_paper499-Figure6-1.png", "CHI18_paper69-Figure4-1.png", "UIST18_paper153-Figure3-1.png", "CHI18_paper223-Figure2-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper199-Figure5-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper446-Figure7-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper634-Figure8-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper347-Figure1-1.png", "UIST18_paper485-Figure3-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper853-Figure7-1.png", "Ubicomp18_paper161-Figure2-1.png", "CHI18_paper19-Figure8-1.png", "CHI18_paper199-Figure6-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper223-Figure5-1.png", "CHI18_paper258-Figure2-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper446-Figure8-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper446-Figure2-1.png"], "grouping and linking@grouping arrows": ["CHI18_paper429-Figure1-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper236-Figure3-1.png", "UIST18_paper473-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper349-Figure3-1.png"], "activity@communication": ["CHI18_paper429-Figure1-1.png", "CSCW18_paper128-Figure2-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper98-Figure4-1.png", "CHI18_paper380-Figure5-1.png", "UIST18_paper19-Figure6-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper300-Figure5-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper406-Figure4-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper54-Figure4-1.png", "UIST18_paper485-Figure9-1.png", "CHI18_paper406-Figure3-1.png", "UIST18_paper581-Figure10-1.png", "CHI18_paper236-Figure2-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper46-Figure2-1.png", "UIST18_paper473-Figure3-1.png", "Ubicomp18_paper201-Figure1-1.png"], "activity@medical": ["CHI18_paper362-Figure7-1.png", "CHI18_paper613-Figure3-1.png", "CHI18_paper98-Figure4-1.png", "UIST18_paper867-Figure2-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper362-Figure3-1.png", "CHI18_paper558-Figure1-1.png", "CHI18_paper362-Figure4-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper209-Figure2-1.png", "UIST18_paper213-Figure20-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper613-Figure7-1.png", "CHI18_paper425-Figure1-1.png", "CHI18_paper613-Figure10-1.png", "CHI18_paper593-Figure4-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper465-Figure7-1.png", "CHI18_paper465-Figure5-1.png", "UIST18_paper867-Figure4-1.png"], "activity@entertainment": ["UIST18_paper5-Figure10-1.png", "CHI18_paper515-Figure7-1.png", "CHI18_paper539-Figure1-1.png", "CHI18_paper446-Figure10-1.png", "CHI18_paper160-Figure2-1.png", "UIST18_paper53-Figure6-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper513-Figure9-1.png", "UIST18_paper557-Figure1-1.png", "CHI18_paper436-Figure7-1.png", "Ubicomp18_paper162-Figure8-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper241-Figure8-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper89-Figure8-1.png", "CHI18_paper539-Figure9-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper927-Figure5-1.png", "UIST18_paper499-Figure9-1.png", "CSCW18_paper185-Figure4-1.png", "UIST18_paper839-Figure8-1.png", "CHI18_paper298-Figure3-1.png", "CHI18_paper189-Figure11-1.png", "CHI18_paper372-Figure1-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper644-Figure7-1.png", "CHI18_paper654-Figure5-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper298-Figure1-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper160-Figure3-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper89-Figure11-1.png", "CHI18_paper266-Figure7-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper89-Figure4-1.png", "CHI18_paper236-Figure3-1.png", "UIST18_paper595-Figure15-1.png", "CHI18_paper446-Figure1-1.png", "CHI18_paper446-Figure12-1.png", "UIST18_paper87-Figure10-1.png", "UIST18_paper511-Figure12-1.png", "UIST18_paper485-Figure7-1.png", "CSCW18_paper159-Figure6-1.png", "UIST18_paper511-Figure2-1.png", "CSCW18_paper192-Figure5-1.png", "CHI18_paper354-Figure17-1.png", "CHI18_paper89-Figure5-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper61-Figure1-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper54-Figure7-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper189-Figure9-1.png", "CSCW18_paper185-Figure5-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper284-Figure6-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "CHI18_paper89-Figure7-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper189-Figure1-1.png", "CHI18_paper189-Figure10-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper61-Figure10-1.png", "CHI18_paper78-Figure2-1.png", "CSCW18_paper185-Figure6-1.png", "CSCW18_paper192-Figure6-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper927-Figure6-1.png", "Ubicomp18_paper200-Figure7-1.png", "CHI18_paper397-Figure5-1.png", "CHI18_paper446-Figure18-1.png", "CHI18_paper599-Figure8-1.png", "UIST18_paper19-Figure12-1.png", "CHI18_paper89-Figure3-1.png"], "effects@waves": ["CHI18_paper446-Figure10-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper368-Figure7-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper547-Figure2-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper89-Figure15-1.png", "CHI18_paper385-Figure1-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper281-Figure8-1.png", "CHI18_paper638-Figure2-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper237-Figure12-1.png", "CHI18_paper446-Figure1-1.png", "UIST18_paper321-Figure2-1.png", "Ubicomp18_paper194-Figure1-1.png", "UIST18_paper53-Figure2-1.png", "CHI18_paper496-Figure5-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper426-Figure9-1.png", "CHI18_paper446-Figure11-1.png", "CHI18_paper441-Figure21-1.png", "UIST18_paper321-Figure15-1.png", "UIST18_paper347-Figure1-1.png", "CHI18_paper258-Figure1-1.png", "CHI18_paper638-Figure3-1.png"], "activity@production": ["Ubicomp18_paper170-Figure5-1.png", "CSCW18_paper128-Figure2-1.png", "CHI18_paper185-Figure7-1.png", "UIST18_paper675-Figure4-1.png", "CHI18_paper439-Figure6-1.png", "CHI18_paper339-Figure5-1.png", "CHI18_paper45-Figure1-1.png", "CHI18_paper622-Figure4-1.png", "CHI18_paper224-Figure4-1.png", "UIST18_paper825-Figure9-1.png", "CHI18_paper205-Figure14-1.png", "CHI18_paper297-Figure3-1.png", "CHI18_paper117-Figure13-1.png", "CHI18_paper411-Figure10-1.png", "CHI18_paper178-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper82-Figure2-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper529-Figure1-1.png", "CHI18_paper626-Figure1-1.png", "CHI18_paper218-Figure14-1.png", "CHI18_paper73-Figure5-1.png", "UIST18_paper53-Figure7-1.png", "CHI18_paper185-Figure6-1.png", "CHI18_paper160-Figure4-1.png", "UIST18_paper335-Figure6-1.png", "CHI18_paper284-Figure7-1.png", "CHI18_paper98-Figure2-1.png", "CHI18_paper107-Figure5-1.png", "CHI18_paper223-Figure3-1.png", "CHI18_paper354-Figure16-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper637-Figure6-1.png"], "sound based@speech input": ["Ubicomp18_paper170-Figure5-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper160-Figure3-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper236-Figure2-1.png"], "visual@projected displays": ["CHI18_paper160-Figure2-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper401-Figure3-1.png", "CHI18_paper173-Figure3-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper199-Figure13-1.png", "CHI18_paper411-Figure4-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper629-Figure7-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper433-Figure1-1.png", "CSCW18_paper118-Figure6-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper401-Figure8-1.png", "CHI18_paper189-Figure8-1.png", "CHI18_paper411-Figure6-1.png", "CHI18_paper160-Figure4-1.png", "CHI18_paper199-Figure16-1.png", "CHI18_paper210-Figure4-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper199-Figure14-1.png", "CHI18_paper199-Figure15-1.png"], "type@data visualization": ["CHI18_paper81-Figure1-1.png", "CHI18_paper368-Figure7-1.png", "CHI18_paper132-Figure1-1.png", "CHI18_paper378-Figure3-1.png", "CHI18_paper188-Figure5-1.png", "UIST18_paper745-Figure2-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper628-Figure6-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper123-Figure8-1.png", "CHI18_paper336-Figure14-1.png", "CHI18_paper65-Figure4-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper237-Figure1-1.png", "CHI18_paper89-Figure13-1.png", "CHI18_paper237-Figure12-1.png", "UIST18_paper737-Figure7-1.png", "UIST18_paper649-Figure6-1.png", "CHI18_paper589-Figure5-1.png", "CHI18_paper42-Figure2-1.png", "CHI18_paper613-Figure8-1.png", "CHI18_paper336-Figure13-1.png", "Ubicomp18_paper194-Figure1-1.png", "UIST18_paper853-Figure4-1.png", "UIST18_paper335-Figure2-1.png", "CHI18_paper33-Figure5-1.png", "CHI18_paper287-Figure2-1.png", "CHI18_paper142-Figure2-1.png", "CHI18_paper199-Figure17-1.png", "UIST18_paper321-Figure8-1.png", "CHI18_paper238-Figure3-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper150-Figure6-1.png", "CHI18_paper19-Figure7-1.png", "CHI18_paper541-Figure3-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper485-Figure3-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper603-Figure6-1.png", "UIST18_paper485-Figure5-1.png"], "enclosing@circle/rectangle": ["CHI18_paper81-Figure1-1.png", "UIST18_paper825-Figure1-1.png", "CHI18_paper202-Figure4-1.png", "UIST18_paper365-Figure5-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper219-Figure14-1.png", "UIST18_paper499-Figure10-1.png", "CHI18_paper564-Figure1-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper638-Figure1-1.png", "CHI18_paper87-Figure1-1.png", "CHI18_paper19-Figure3-1.png", "CHI18_paper150-Figure20-1.png", "CHI18_paper354-Figure18-1.png", "CHI18_paper19-Figure10-1.png", "Ubicomp18_paper185-Figure3-1.png", "CHI18_paper564-Figure6-1.png", "CHI18_paper248-Figure3-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "UIST18_paper853-Figure8-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper25-Figure2-1.png", "CHI18_paper406-Figure4-1.png", "CHI18_paper54-Figure7-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper54-Figure4-1.png", "CHI18_paper11-Figure1-1.png", "CHI18_paper223-Figure7-1.png", "CHI18_paper150-Figure21-1.png", "CHI18_paper150-Figure22-1.png", "CHI18_paper150-Figure5-1.png", "CHI18_paper54-Figure6-1.png", "CSCW18_paper140-Figure3-1.png", "Ubicomp18_paper201-Figure9-1.png", "UIST18_paper19-Figure9-1.png"], "Interaction type@controllers interaction": ["CHI18_paper81-Figure1-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper86-Figure12-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper241-Figure7-1.png", "CHI18_paper644-Figure7-1.png", "UIST18_paper499-Figure1-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper644-Figure11-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper362-Figure1-1.png", "CHI18_paper45-Figure3-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper411-Figure6-1.png", "UIST18_paper53-Figure7-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper569-Figure17-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper362-Figure6-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper579-Figure13-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "UIST18_paper499-Figure7-1.png", "UIST18_paper87-Figure11-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper407-Figure1-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper599-Figure8-1.png", "CHI18_paper89-Figure3-1.png"], "device based@controllers": ["CHI18_paper81-Figure1-1.png", "CHI18_paper320-Figure11-1.png", "CHI18_paper86-Figure6-1.png", "CHI18_paper218-Figure7-1.png", "CHI18_paper310-Figure1-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper340-Figure3-1.png", "CHI18_paper241-Figure8-1.png", "UIST18_paper521-Figure1-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper613-Figure1-1.png", "CHI18_paper579-Figure8-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper42-Figure1-1.png", "CHI18_paper81-Figure10-1.png", "UIST18_paper499-Figure15-1.png", "CHI18_paper201-Figure5-1.png", "CHI18_paper86-Figure13-1.png", "CHI18_paper401-Figure4-1.png", "CHI18_paper423-Figure10-1.png", "CHI18_paper86-Figure8-1.png", "CHI18_paper644-Figure9-1.png", "CHI18_paper411-Figure5-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper209-Figure2-1.png", "CHI18_paper644-Figure11-1.png", "CHI18_paper541-Figure9-1.png", "CHI18_paper128-Figure2-1.png", "CHI18_paper42-Figure2-1.png", "UIST18_paper87-Figure10-1.png", "UIST18_paper511-Figure12-1.png", "CHI18_paper460-Figure1-1.png", "CSCW18_paper159-Figure6-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper579-Figure7-1.png", "UIST18_paper877-Figure3-1.png", "CHI18_paper401-Figure7-1.png", "CHI18_paper291-Figure9-1.png", "CHI18_paper150-Figure8-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper185-Figure6-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper569-Figure17-1.png", "UIST18_paper499-Figure11-1.png", "CHI18_paper143-Figure7-1.png", "CHI18_paper210-Figure2-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper530-Figure1-1.png", "CHI18_paper647-Figure2-1.png", "CHI18_paper654-Figure4-1.png", "UIST18_paper87-Figure11-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper199-Figure12-1.png", "CHI18_paper246-Figure3-1.png", "CHI18_paper367-Figure7-1.png", "CHI18_paper579-Figure4-1.png", "CHI18_paper99-Figure3-1.png", "UIST18_paper913-Figure8-1.png", "Ubicomp18_paper201-Figure1-1.png", "CHI18_paper65-Figure2-1.png", "CHI18_paper603-Figure6-1.png", "CHI18_paper599-Figure8-1.png"], "anonymization@blur": ["CHI18_paper76-Figure3-1.png", "CHI18_paper289-Figure1-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper443-Figure2-1.png", "CHI18_paper551-Figure3-1.png", "CHI18_paper359-Figure2-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper397-Figure5-1.png"], "Interaction type@gaze interaction": ["CHI18_paper368-Figure7-1.png", "CHI18_paper46-Figure4-1.png", "CHI18_paper288-Figure1-1.png", "CHI18_paper310-Figure1-1.png", "CHI18_paper436-Figure7-1.png", "Ubicomp18_paper174-Figure1-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper81-Figure10-1.png", "CHI18_paper300-Figure4-1.png", "CHI18_paper349-Figure3-1.png", "CHI18_paper46-Figure2-1.png"], "enclosing@exact contour line": ["CHI18_paper218-Figure7-1.png", "CHI18_paper363-Figure2-1.png", "CHI18_paper380-Figure5-1.png", "CHI18_paper241-Figure4-1.png", "UIST18_paper499-Figure9-1.png", "CHI18_paper638-Figure5-1.png", "CHI18_paper334-Figure5-1.png", "CHI18_paper638-Figure2-1.png", "CHI18_paper248-Figure9-1.png", "UIST18_paper5-Figure9-1.png", "CHI18_paper428-Figure1-1.png", "CHI18_paper407-Figure2-1.png", "CHI18_paper564-Figure3-1.png", "UIST18_paper31-Figure4-1.png", "UIST18_paper867-Figure4-1.png"], "activity@driving": ["Ubicomp18_paper184-Figure4-1.png", "CHI18_paper401-Figure7-1.png", "Ubicomp18_paper181-Figure1-1.png", "CHI18_paper165-Figure3-1.png", "UIST18_paper87-Figure11-1.png", "CHI18_paper165-Figure1-1.png", "CHI18_paper246-Figure3-1.png"], "audible@sound based": ["Ubicomp18_paper184-Figure4-1.png", "CHI18_paper378-Figure3-1.png", "CSCW18_paper192-Figure8-1.png", "CHI18_paper629-Figure1-1.png", "CHI18_paper541-Figure1-1.png", "CHI18_paper541-Figure5-1.png", "CHI18_paper69-Figure5-1.png", "UIST18_paper485-Figure7-1.png", "CHI18_paper629-Figure6-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper516-Figure1-1.png", "CHI18_paper541-Figure3-1.png"], "Situation@Public/private transport": ["Ubicomp18_paper184-Figure4-1.png", "CHI18_paper436-Figure7-1.png", "CHI18_paper300-Figure5-1.png", "CHI18_paper165-Figure3-1.png", "UIST18_paper581-Figure10-1.png", "CHI18_paper165-Figure1-1.png"], "continuous@scrolling": ["CHI18_paper132-Figure1-1.png", "Ubicomp18_paper162-Figure8-1.png", "CHI18_paper634-Figure3-1.png", "CHI18_paper339-Figure5-1.png", "CSCW18_paper185-Figure4-1.png", "CHI18_paper245-Figure12-1.png", "CHI18_paper123-Figure8-1.png", "UIST18_paper867-Figure5-1.png", "CHI18_paper249-Figure4-1.png", "CHI18_paper73-Figure4-1.png", "CHI18_paper661-Figure1-1.png", "CHI18_paper45-Figure3-1.png", "UIST18_paper839-Figure2-1.png", "CHI18_paper634-Figure8-1.png", "UIST18_paper867-Figure3-1.png", "CHI18_paper248-Figure7-1.png", "CSCW18_paper140-Figure2-1.png"], "Specific part@foot": ["CHI18_paper378-Figure3-1.png", "CHI18_paper334-Figure5-1.png", "UIST18_paper65-Figure2-1.png", "CHI18_paper541-Figure8-1.png", "CHI18_paper336-Figure5-1.png"], "background@removed": ["CHI18_paper310-Figure1-1.png", "CHI18_paper54-Figure5-1.png", "CHI18_paper90-Figure6-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper150-Figure3-1.png", "CHI18_paper354-Figure18-1.png", "UIST18_paper595-Figure5-1.png", "CHI18_paper54-Figure6-1.png", "CHI18_paper90-Figure7-1.png", "Ubicomp18_paper161-Figure2-1.png", "CHI18_paper354-Figure16-1.png", "UIST18_paper141-Figure10-1.png"], "device based@laptop devices": ["UIST18_paper335-Figure3-1.png", "CHI18_paper622-Figure4-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper353-Figure4-1.png", "Ubicomp18_paper176-Figure3-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper21-Figure1-1.png", "CHI18_paper281-Figure1-1.png"], "audible@speech output": ["UIST18_paper557-Figure1-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper236-Figure3-1.png", "CHI18_paper69-Figure2-1.png", "CHI18_paper69-Figure4-1.png", "CHI18_paper69-Figure3-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper236-Figure2-1.png", "UIST18_paper473-Figure3-1.png"], "hue@grayscale": ["CHI18_paper436-Figure7-1.png", "CHI18_paper89-Figure10-1.png", "CHI18_paper241-Figure2-1.png", "CHI18_paper436-Figure6-1.png", "CHI18_paper11-Figure5-1.png", "CHI18_paper241-Figure4-1.png", "CHI18_paper374-Figure2-1.png", "CHI18_paper547-Figure1-1.png", "Ubicomp18_paper161-Figure9-1.png", "CHI18_paper20-Figure2-1.png", "CHI18_paper241-Figure6-1.png", "CHI18_paper128-Figure2-1.png", "UIST18_paper499-Figure12-1.png", "CHI18_paper11-Figure6-1.png", "CHI18_paper218-Figure4-1.png", "CHI18_paper419-Figure1-1.png", "CHI18_paper11-Figure7-1.png", "CHI18_paper173-Figure1-1.png", "CHI18_paper218-Figure3-1.png", "CHI18_paper218-Figure5-1.png"], "body part@lower body": ["UIST18_paper499-Figure10-1.png", "UIST18_paper499-Figure5-1.png"], "background@grayed out/blurred": ["UIST18_paper499-Figure9-1.png", "UIST18_paper581-Figure10-1.png"], "Situation@Outdoors": ["CHI18_paper362-Figure1-1.png"], "effects@motion blur": ["CHI18_paper218-Figure8-1.png", "CHI18_paper530-Figure1-1.png"]}, "dois_dict": {"Ubicomp18_paper180.pdf": "https://doi.org/10.1145/3287058", "CHI18_paper398.pdf": "https://doi.org/10.1145/3173574.3173972", "Ubicomp18_paper194.pdf": "https://doi.org/10.1145/3287072", "CHI18_paper429.pdf": "https://doi.org/10.1145/3173574.3174003", "Ubicomp18_paper157.pdf": "https://doi.org/10.1145/3287035", "CHI18_paper401.pdf": "https://doi.org/10.1145/3173574.3173975", "CHI18_paper367.pdf": "https://doi.org/10.1145/3173574.3173941", "CHI18_paper373.pdf": "https://doi.org/10.1145/3173574.3173947", "CHI18_paper415.pdf": "https://doi.org/10.1145/3173574.3173989", "CHI18_paper159.pdf": "https://doi.org/10.1145/3173574.3173733", "CHI18_paper165.pdf": "https://doi.org/10.1145/3173574.3173739", "CHI18_paper603.pdf": "https://doi.org/10.1145/3173574.3174177", "CHI18_paper617.pdf": "https://doi.org/10.1145/3173574.3174191", "CHI18_paper171.pdf": "https://doi.org/10.1145/3173574.3173745", "CSCW18_paper58.pdf": "https://doi.org/10.1145/3274327", "CSCW18_paper64.pdf": "https://doi.org/10.1145/3274333", "UIST18_paper247.pdf": "https://doi.org/10.1145/3242587.3242590", "CSCW18_paper70.pdf": "https://doi.org/10.1145/3274339", "UIST18_paper521.pdf": "https://doi.org/10.1145/3242587.3242601", "UIST18_paper737.pdf": "https://doi.org/10.1145/3242587.3242615", "CSCW18_paper124.pdf": "https://doi.org/10.1145/3274393", "CHI18_paper71.pdf": "https://doi.org/10.1145/3173574.3173645", "CHI18_paper549.pdf": "https://doi.org/10.1145/10.1145/3173574.3174123", "CSCW18_paper130.pdf": "https://doi.org/10.1145/3274399", "CHI18_paper65.pdf": "https://doi.org/10.1145/3173574.3173639", "CHI18_paper575.pdf": "https://doi.org/10.1145/3173574.3174149", "CSCW18_paper118.pdf": "https://doi.org/10.1145/3274387", "CHI18_paper213.pdf": "https://doi.org/10.1145/3173574.3173787", "CHI18_paper207.pdf": "https://doi.org/10.1145/3173574.3173781", "CHI18_paper59.pdf": "https://doi.org/10.1145/3173574.3173633", "CHI18_paper561.pdf": "https://doi.org/10.1145/3173574.3174135", "CHI18_paper206.pdf": "https://doi.org/10.1145/3173574.3173780", "CHI18_paper560.pdf": "https://doi.org/10.1145/3173574.3174134", "CHI18_paper58.pdf": "https://doi.org/10.1145/3173574.3173632", "CSCW18_paper119.pdf": "https://doi.org/10.1145/3274388", "CHI18_paper574.pdf": "https://doi.org/10.1145/3173574.3174148", "CHI18_paper212.pdf": "https://doi.org/10.1145/3173574.3173786", "CHI18_paper64.pdf": "https://doi.org/10.1145/3173574.3173638", "CSCW18_paper131.pdf": "https://doi.org/10.1145/", "CHI18_paper548.pdf": "https://doi.org/10.1145/3173574.3174122", "CHI18_paper70.pdf": "https://doi.org/10.1145/3173574.3173644", "CSCW18_paper125.pdf": "https://doi.org/10.1145/3274394", "UIST18_paper99.pdf": "https://doi.org/10.1145/3242587.3242659", "CSCW18_paper71.pdf": "https://doi.org/10.1145/3274340", "CSCW18_paper65.pdf": "https://doi.org/10.1145/3274334", "CSCW18_paper59.pdf": "https://doi.org/10.1145/3274328", "CHI18_paper616.pdf": "https://doi.org/10.1145/3173574.3174190", "CHI18_paper170.pdf": "https://doi.org/10.1145/3173574.3173744", "CHI18_paper164.pdf": "https://doi.org/10.1145/3173574.3173738", "CHI18_paper602.pdf": "https://doi.org/10.1145/3173574.3174176", "CHI18_paper158.pdf": "https://doi.org/10.1145/3173574.3173732", "CHI18_paper372.pdf": "https://doi.org/10.1145/3173574.3173946", "CHI18_paper400.pdf": "https://doi.org/10.1145/3173574.3173974 ", "CHI18_paper366.pdf": "https://doi.org/10.1145/3173574.3173940", "Ubicomp18_paper156.pdf": "https://doi.org/10.1145/3287034", "CHI18_paper428.pdf": "https://doi.org/10.1145/3173574.3174002", "Ubicomp18_paper195.pdf": "https://doi.org/10.1145/3287073", "Ubicomp18_paper181.pdf": "https://doi.org/10.1145/3287059", "CHI18_paper399.pdf": "https://doi.org/10.1145/3173574.3173973", "Ubicomp18_paper197.pdf": "https://doi.org/10.1145/3287075", "Ubicomp18_paper183.pdf": "https://doi.org/10.1145/3287061", "Ubicomp18_paper154.pdf": "https://doi.org/10.1145/3287032", "CHI18_paper8.pdf": "https://doi.org/10.1145/3173574.3173582", "CHI18_paper358.pdf": "https://doi.org/10.1145/3173574.3173932", "UIST18_paper867.pdf": "https://doi.org/10.1145/3242587.3242636", "CHI18_paper416.pdf": "https://doi.org/10.1145/3173574.3173990", "Ubicomp18_paper168.pdf": "https://doi.org/10.1145/", "CHI18_paper370.pdf": "https://doi.org/10.1145/3173574.3173944.", "UIST18_paper697.pdf": "https://doi.org/10.1145/3242587.3242629", "UIST18_paper683.pdf": "https://doi.org/10.1145/3242587.3242622", "CHI18_paper364.pdf": "https://doi.org/10.1145/3173574.3173938", "CHI18_paper402.pdf": "https://doi.org/10.1145/3173574.3173976", "CHI18_paper199.pdf": "https://doi.org/10.1145/3173574.3173773", "CHI18_paper628.pdf": "https://doi.org/10.1145/3173574.317420", "CHI18_paper172.pdf": "https://doi.org/10.1145/3173574.3173746", "CHI18_paper614.pdf": "https://doi.org/10.1145/3173574.3174188", "CHI18_paper600.pdf": "https://doi.org/10.1145/3173574.3174174", "CHI18_paper166.pdf": "https://doi.org/10.1145/3173574.3173740", "CSCW18_paper73.pdf": "https://doi.org/10.1145/3274342", "CSCW18_paper67.pdf": "https://doi.org/10.1145/3274336", "CSCW18_paper98.pdf": "https://doi.org/10.1145/3274367", "UIST18_paper287.pdf": "https://doi.org/10.1145/3242587.3242606", "CHI18_paper589.pdf": "https://doi.org/10.1145/3173574.3174163", "CHI18_paper99.pdf": "https://doi.org/10.1145/3173574.3173673", "CSCW18_paper133.pdf": "https://doi.org/10.1145/3274402", "CHI18_paper66.pdf": "https://doi.org/10.1145/3173574.3173640", "CHI18_paper238.pdf": "https://doi.org/10.1145/3173574.3173812", "UIST18_paper913.pdf": "https://doi.org/10.1145/3242587.3242667", "CSCW18_paper127.pdf": "https://doi.org/10.1145/3274396", "CHI18_paper72.pdf": "https://doi.org/10.1145/3173574.3173646", "CHI18_paper562.pdf": "https://doi.org/10.1145/3173574.3174136", "CHI18_paper204.pdf": "https://doi.org/10.1145/3173574.3173778", "CHI18_paper210.pdf": "https://doi.org/10.1145/3173574.3173784", "CHI18_paper576.pdf": "https://doi.org/10.1145/3173574.3174150", "CHI18_paper211.pdf": "https://doi.org/10.1145/3173574.3173785", "CHI18_paper577.pdf": "https://doi.org/10.1145/3173574.3174151", "CHI18_paper563.pdf": "https://doi.org/10.1145/3173574.3174137", "CHI18_paper205.pdf": "https://doi.org/10.1145/3173574.3173779", "CHI18_paper73.pdf": "https://doi.org/10.1145/3173574.3173647", "CSCW18_paper126.pdf": "https://doi.org/10.1145/3274395", "CHI18_paper67.pdf": "https://doi.org/10.1145/3173574.3173641", "CSCW18_paper132.pdf": "https://doi.org/10.1145/3274401", "CHI18_paper239.pdf": "https://doi.org/10.1145/3173574.3173813", "CHI18_paper98.pdf": "https://doi.org/10.1145/3173574.3173672", "CHI18_paper588.pdf": "https://doi.org/10.1145/3173574.3174162", "CSCW18_paper99.pdf": "https://doi.org/10.1145/3274368", "CSCW18_paper66.pdf": "https://doi.org/10.1145/3274335", "UIST18_paper65.pdf": "https://doi.org/10.1145/3242587.3242665", "CSCW18_paper72.pdf": "https://doi.org/10.1145/3274341 ", "CHI18_paper601.pdf": "https://doi.org/10.1145/3173574.3174175", "CHI18_paper167.pdf": "https://doi.org/10.1145/3173574.3173741 ", "CHI18_paper173.pdf": "https://doi.org/10.1145/3173574.3173747", "CHI18_paper615.pdf": "https://doi.org/10.1145/3173574.3174189", "CHI18_paper629.pdf": "https://doi.org/10.1145/3173574.3174203", "UIST18_paper457.pdf": "https://doi.org/10.1145/3242587.3242655", "CHI18_paper198.pdf": "https://doi.org/10.1145/3173574.3173772", "CHI18_paper365.pdf": "https://doi.org/10.1145/3173574.3173939", "CHI18_paper403.pdf": "https://doi.org/10.1145/3173574.3173977 ", "Ubicomp18_paper169.pdf": "https://doi.org/10.1145/3287047", "CHI18_paper417.pdf": "https://doi.org/10.1145/3173574.3173991", "CHI18_paper371.pdf": "https://doi.org/10.1145/3173574.3173945 ", "CHI18_paper359.pdf": "https://doi.org/10.1145/3173574.3173933 ", "CHI18_paper9.pdf": "https://doi.org/10.1145/3173574.3173583", "Ubicomp18_paper155.pdf": "https://doi.org/10.1145/3287033", "UIST18_paper127.pdf": "https://doi.org/10.1145/3242587.3242619", "Ubicomp18_paper182.pdf": "https://doi.org/10.1145/3287060", "Ubicomp18_paper196.pdf": "https://doi.org/10.1145/3287074", "UIST18_paper889.pdf": "https://doi.org/10.1145/3242587.3242643", "Ubicomp18_paper192.pdf": "https://doi.org/10.1145/3287070", "Ubicomp18_paper186.pdf": "https://doi.org/10.1145/3287064", "CHI18_paper375.pdf": "https://doi.org/10.1145/3173574.3173949", "CHI18_paper413.pdf": "https://doi.org/10.1145/3173574.3173987", "CHI18_paper407.pdf": "https://doi.org/10.1145/3173574.3173981", "Ubicomp18_paper179.pdf": "https://doi.org/10.1145/3287057", "CHI18_paper361.pdf": "https://doi.org/10.1145/3173574.3173935", "CHI18_paper349.pdf": "https://doi.org/10.1145/3173574.3173923", "UIST18_paper335.pdf": "https://doi.org/10.1145/3242587.3242651", "UIST18_paper447.pdf": "https://doi.org/10.1145/3242587.3242668", "UIST18_paper321.pdf": "https://doi.org/10.1145/3242587.3242662", "CHI18_paper188.pdf": "https://doi.org/10.1145/3173574.3173762", "CHI18_paper611.pdf": "https://doi.org/10.1145/3173574.3174185", "CHI18_paper177.pdf": "https://doi.org/10.1145/3173574.3173751", "CHI18_paper163.pdf": "https://doi.org/10.1145/3173574.3173737", "CHI18_paper605.pdf": "https://doi.org/10.1145/3173574.3174179", "CHI18_paper639.pdf": "https://doi.org/10.1145/3173574.3174213", "UIST18_paper75.pdf": "https://doi.org/10.1145/3242587.3242634", "CSCW18_paper62.pdf": "https://doi.org/10.1145/3274331", "UIST18_paper725.pdf": "https://doi.org/10.1145/3242587.3242637", "CHI18_paper88.pdf": "https://doi.org/10.1145/3173574.3173662", "CHI18_paper598.pdf": "https://doi.org/10.1145/3173574.3174172", "CHI18_paper201.pdf": "https://doi.org/10.1145/3173574.3173775 ", "CHI18_paper567.pdf": "https://doi.org/10.1145/3173574.3174141", "CHI18_paper573.pdf": "https://doi.org/10.1145/3173574.3174147", "CHI18_paper215.pdf": "https://doi.org/10.1145/3173574.3173789", "CSCW18_paper136.pdf": "https://doi.org/10.1145/3274405", "CHI18_paper63.pdf": "https://doi.org/10.1145/3173574.3173637", "CSCW18_paper122.pdf": "https://doi.org/10.1145/3274391", "CHI18_paper77.pdf": "https://doi.org/10.1145/3173574.3173651", "CHI18_paper229.pdf": "https://doi.org/10.1145/3173574.3173803", "CHI18_paper76.pdf": "https://doi.org/10.1145/3173574.3173650", "CSCW18_paper123.pdf": "https://doi.org/10.1145/3274392", "CHI18_paper228.pdf": "https://doi.org/10.1145/3173574.3173802", "CHI18_paper62.pdf": "https://doi.org/10.1145/3173574.3173636", "CSCW18_paper137.pdf": "https://doi.org/10.1145/3274406", "CHI18_paper572.pdf": "https://doi.org/10.1145/3173574.3174146", "CHI18_paper214.pdf": "https://doi.org/10.1145/3173574.3173788", "CHI18_paper200.pdf": "https://doi.org/10.1145/3173574.3173774", "CHI18_paper566.pdf": "https://doi.org/10.1145/3173574.3174140", "CHI18_paper599.pdf": "https://doi.org/10.1145/3173574.3174173", "CHI18_paper89.pdf": "https://doi.org/10.1145/3173574.3173663", "CSCW18_paper63.pdf": "https://doi.org/10.1145/3274332", "CHI18_paper638.pdf": "https://doi.org/10.1145/3173574.3174212", "UIST18_paper485.pdf": "https://doi.org/10.1145/3242587.3242638", "CHI18_paper162.pdf": "https://doi.org/10.1145/3173574.3173736", "CHI18_paper604.pdf": "https://doi.org/10.1145/3173574.3174178", "CHI18_paper610.pdf": "https://doi.org/10.1145/3173574.3174184", "CHI18_paper176.pdf": "https://doi.org/10.1145/3173574.3173750", "CHI18_paper189.pdf": "https://doi.org/10.1145/3173574.3173763", "CHI18_paper348.pdf": "https://doi.org/10.1145/3173574.3173922", "UIST18_paper877.pdf": "https://doi.org/10.1145/3242587.3242604", "Ubicomp18_paper178.pdf": "https://doi.org/10.1145/3287056", "CHI18_paper406.pdf": "https://doi.org/10.1145/3173574.3173980", "CHI18_paper360.pdf": "https://doi.org/10.1145/3173574.3173934", "CHI18_paper374.pdf": "https://doi.org/10.1145/3173574.3173948", "CHI18_paper412.pdf": "https://doi.org/10.1145/3173574.3173986", "Ubicomp18_paper187.pdf": "https://doi.org/10.1145/3287065", "Ubicomp18_paper193.pdf": "https://doi.org/10.1145/3287071", "Ubicomp18_paper185.pdf": "https://doi.org/10.1145/3287063", "Ubicomp18_paper191.pdf": "https://doi.org/10.1145/3287069", "CHI18_paper389.pdf": "https://doi.org/10.1145/3173574.3173963", "CHI18_paper362.pdf": "https://doi.org/10.1145/3173574.3173936", "CHI18_paper404.pdf": "https://doi.org/10.1145/3173574.3173978", "CHI18_paper410.pdf": "https://doi.org/10.1145/3173574.3173984", "CHI18_paper376.pdf": "https://doi.org/10.1145/3173574.3173950", "CHI18_paper438.pdf": "https://doi.org/10.1145/3173574.3174012", "CHI18_paper606.pdf": "https://doi.org/10.1145/3173574.3174180", "CHI18_paper160.pdf": "https://doi.org/10.1145/3173574.3173734", "CHI18_paper174.pdf": "https://doi.org/10.1145/3173574.3173748", "CHI18_paper612.pdf": "https://doi.org/10.1145/3173574.3174186", "CHI18_paper148.pdf": "https://doi.org/10.1145/3173574.3173722", "CSCW18_paper75.pdf": "https://doi.org/10.1145/3274344", "CSCW18_paper49.pdf": "https://doi.org/10.1145/3274318", "CHI18_paper216.pdf": "https://doi.org/10.1145/3173574.3173790", "CHI18_paper570.pdf": "", "CHI18_paper48.pdf": "https://doi.org/10.1145/3173574.3173622", "CSCW18_paper109.pdf": "https://doi.org/10.1145/3274378", "CHI18_paper564.pdf": "https://doi.org/10.1145/3173574.3174138", "CHI18_paper202.pdf": "https://doi.org/10.1145/3173574.3173776", "CSCW18_paper121.pdf": "https://doi.org/10.1145/", "CHI18_paper74.pdf": "https://doi.org/10.1145/3173574.3173648", "CHI18_paper558.pdf": "https://doi.org/10.1145/3173574.3174132", "CSCW18_paper135.pdf": "https://doi.org/10.5281/zenodo.1405894]. Data and", "CHI18_paper60.pdf": "https://doi.org/10.1145/3173574.3173634", "UIST18_paper901.pdf": "https://doi.org/10.1145/3242587.3242657", "CHI18_paper61.pdf": "https://doi.org/10.1145/3173574.3173635", "CSCW18_paper134.pdf": "https://doi.org/10.1145/3274403", "CHI18_paper559.pdf": "https://doi.org/10.1145/3173574.3174133", "CHI18_paper75.pdf": "https://doi.org/10.1145/3173574.3173649", "CSCW18_paper120.pdf": "https://doi.org/10.1145/3274389", "CHI18_paper565.pdf": "https://doi.org/10.1145/3173574.3174139", "CSCW18_paper108.pdf": "https://doi.org/10.1145/3274377.", "CHI18_paper203.pdf": "https://doi.org/10.1145/3173574.3173777 ", "CHI18_paper217.pdf": "https://doi.org/10.1145/3173574.3173791 ", "CHI18_paper49.pdf": "https://doi.org/10.1145/3173574.3173623", "CHI18_paper571.pdf": "https://doi.org/10.1145/3173574.3174145", "CSCW18_paper48.pdf": "https://doi.org/10.1145/3274317", "CSCW18_paper74.pdf": "https://doi.org/10.1145/3274343", "UIST18_paper531.pdf": "https://doi.org/10.1145/3242587.3242593", "CHI18_paper149.pdf": "https://doi.org/10.1145/3173574.3173723", "CHI18_paper613.pdf": "https://doi.org/10.1145/3173574.3174187", "CHI18_paper607.pdf": "https://doi.org/10.1145/3173574.3174181", "CHI18_paper161.pdf": "https://doi.org/10.1145/3173574.3173735", "Ubicomp18_paper153.pdf": "https://doi.org/10.1145/3287031", "CHI18_paper439.pdf": "https://doi.org/10.1145/3173574.3174013", "CHI18_paper411.pdf": "https://doi.org/10.1145/3173574.3173985", "CHI18_paper377.pdf": "https://doi.org/10.1145/3173574.3173951", "CHI18_paper363.pdf": "", "CHI18_paper405.pdf": "https://doi.org/10.1145/3173574.3173979 ", "Ubicomp18_paper190.pdf": "https://doi.org/10.1145/3287068", "CHI18_paper388.pdf": "https://doi.org/10.1145/3173574.3173962", "Ubicomp18_paper184.pdf": "https://doi.org/10.1145/3287062", "CHI18_paper489.pdf": "https://doi.org/10.1145/3173574.3174063", "CHI18_paper338.pdf": "https://doi.org/10.1145/3173574.3173912", "CHI18_paper304.pdf": "https://doi.org/10.1145/3173574.3173878", "CHI18_paper462.pdf": "https://doi.org/10.1145/3173574.3174036", "CHI18_paper476.pdf": "https://doi.org/10.1145/3173574.3174050", "CHI18_paper310.pdf": "https://doi.org/10.1145/3173574.3173884", "CHI18_paper648.pdf": "https://doi.org/10.1145/3173574.3174222", "CHI18_paper660.pdf": "https://doi.org/10.1145/3173574.3174234 ", "CHI18_paper106.pdf": "https://doi.org/10.1145/3173574.3173680", "CHI18_paper112.pdf": "https://doi.org/10.1145/3173574.3173686", "UIST18_paper595.pdf": "https://doi.org/10.1145/3242587.3242635", "UIST18_paper581.pdf": "https://doi.org/10.1145/3242587.3242599", "CSCW18_paper184.pdf": "https://doi.org/10.1145/3274453", "CSCW18_paper190.pdf": "https://doi.org/10.1145.3274459", "CSCW18_paper147.pdf": "https://doi.org/10.1145/3274416", "CHI18_paper12.pdf": "https://doi.org/10.1145/3173574.3173586", "CSCW18_paper153.pdf": "https://doi.org/10.1145/3274422", "CHI18_paper258.pdf": "https://doi.org/10.1145/3173574.3173832", "CHI18_paper270.pdf": "https://doi.org/10.1145/3173574.3173844", "CHI18_paper516.pdf": "https://doi.org/10.1145/3173574.3174090", "CHI18_paper502.pdf": "https://doi.org/10.1145/3173574.3174076", "CHI18_paper264.pdf": "https://doi.org/10.1145/3173574.3173838", "CHI18_paper503.pdf": "https://doi.org/10.1145/3173574.3174077", "CHI18_paper265.pdf": "https://doi.org/10.1145/3173574.3173839", "CHI18_paper271.pdf": "https://doi.org/10.1145/3173574.3173845", "CHI18_paper517.pdf": "https://doi.org/10.1145/3173574.3174091", "CSCW18_paper152.pdf": "https://doi.org/10.1145/3274421", "CHI18_paper259.pdf": "https://doi.org/10.1145/3173574.3173833", "CHI18_paper13.pdf": "https://doi.org/10.1145/3173574.3173587", "CSCW18_paper146.pdf": "https://doi.org/10.1145/3274415", "CSCW18_paper191.pdf": "https://doi.org/10.1145/3274460", "CSCW18_paper185.pdf": "https://doi.org/10.1145/3274454", "UIST18_paper543.pdf": "https://doi.org/10.1145/3242587.3242610", "UIST18_paper225.pdf": "https://doi.org/10.1145/3242587.3242608", "UIST18_paper557.pdf": "https://doi.org/10.1145/3242587.3242614", "CHI18_paper113.pdf": "https://doi.org/10.1145/3173574.3173687", "CHI18_paper661.pdf": "https://doi.org/10.1145/3173574.3174235", "CHI18_paper107.pdf": "https://doi.org/10.1145/3173574.3173681", "CHI18_paper649.pdf": "https://doi.org/10.1145/3173574.3174223", "UIST18_paper423.pdf": "https://doi.org/10.1145/3242587.3242617", "UIST18_paper379.pdf": "https://doi.org/10.1145/3242587.3242663", "CHI18_paper477.pdf": "https://doi.org/10.1145/3173574.3174051", "CHI18_paper311.pdf": "https://doi.org/10.1145/3173574.3173885", "CHI18_paper305.pdf": "https://doi.org/10.1145/3173574.3173879", "CHI18_paper463.pdf": "https://doi.org/10.1145/3173574.3174037", "CHI18_paper339.pdf": "https://doi.org/10.1145/3173574.3173913", "UIST18_paper153.pdf": "https://doi.org/10.1145/3242587.3242596", "CHI18_paper488.pdf": "https://doi.org/10.1145/3173574.3174062", "UIST18_paper609.pdf": "https://doi.org/10.1145/3242587.3242616", "UIST18_paper637.pdf": "https://doi.org/10.1145/3242587.3242602", "UIST18_paper623.pdf": "https://doi.org/10.1145/3242587.3242625", "CHI18_paper449.pdf": "https://doi.org/10.1145/3173574.3174023", "CHI18_paper313.pdf": "https://doi.org/10.1145/3173574.3173887", "CHI18_paper475.pdf": "https://doi.org/10.1145/3173574.3174049", "CHI18_paper461.pdf": "https://doi.org/10.1145/3173574.3174035", "CHI18_paper307.pdf": "https://doi.org/10.1145/3173574.3173881", "UIST18_paper435.pdf": "https://doi.org/10.1145/3242587.3242591", "UIST18_paper347.pdf": "https://doi.org/10.1145/3242587.3242589", "CHI18_paper139.pdf": "https://doi.org/10.1145/3173574.3173713", "CHI18_paper111.pdf": "https://doi.org/10.1145/3173574.3173685", "CHI18_paper105.pdf": "https://doi.org/10.1145/3173574.3173679", "CHI18_paper663.pdf": "https://doi.org/10.1145/3173574.3174237", "UIST18_paper569.pdf": "https://doi.org/10.1145/3242587.3242650", "CSCW18_paper38.pdf": "https://doi.org/10.1145/3274307", "CHI18_paper298.pdf": "https://doi.org/10.1145/3173574.3173872", "CSCW18_paper193.pdf": "https://doi.org/10.1145/3274462", "CSCW18_paper187.pdf": "https://doi.org/10.1145/3274456", "UIST18_paper757.pdf": "https://doi.org/10.1145/3242587.3242656", "CSCW18_paper150.pdf": "https://doi.org/10.1145/3274419", "CHI18_paper529.pdf": "https://doi.org/10.1145/3173574.3174103", "CSCW18_paper144.pdf": "https://doi.org/10.1145/3274413", "CHI18_paper11.pdf": "https://doi.org/10.1145/3173574.3173585", "CHI18_paper267.pdf": "https://doi.org/10.1145/3173574.3173841 ", "CHI18_paper501.pdf": "https://doi.org/10.1145/3173574.3174075", "CHI18_paper39.pdf": "https://doi.org/10.1145/3173574.3173613 ", "CSCW18_paper178.pdf": "https://doi.org/10.1145/3274447", "CHI18_paper515.pdf": "https://doi.org/10.1145/3173574.3174089", "CHI18_paper273.pdf": "https://doi.org/10.1145/3173574.3173847", "CHI18_paper514.pdf": "https://doi.org/10.1145/3173574.3174088.", "CSCW18_paper179.pdf": "https://doi.org/10.1145/", "CHI18_paper272.pdf": "https://doi.org/10.1145/3173574.3173846", "CHI18_paper266.pdf": "https://doi.org/10.1145/3173574.3173840.", "CHI18_paper38.pdf": "https://doi.org/10.1145/3173574.3173612", "CHI18_paper500.pdf": "https://doi.org/10.1145/3173574.3174074", "CHI18_paper10.pdf": "https://doi.org/10.1145/ 173574.3173584", "CSCW18_paper145.pdf": "https://doi.org/10.1145/3274414", "CHI18_paper528.pdf": "https://doi.org/10.1145/3173574.3174102", "CSCW18_paper151.pdf": "https://doi.org/10.1145/3274420", "CSCW18_paper186.pdf": "https://doi.org/10.1145/3274455", "CHI18_paper299.pdf": "https://doi.org/10.1145/3173574.3173873", "CSCW18_paper192.pdf": "https://doi.org/10.1145/3274461", "CSCW18_paper39.pdf": "https://doi.org/10.1145/3274308", "CHI18_paper104.pdf": "https://doi.org/10.1145/3173574.3173678", "CHI18_paper662.pdf": "https://doi.org/10.1145/3173574.3174236", "CHI18_paper110.pdf": "https://doi.org/10.1145/3173574.3173684", "CHI18_paper138.pdf": "https://doi.org/10.1145/3173574.3173712", "CHI18_paper460.pdf": "https://doi.org/10.1145/3173574.3174034", "UIST18_paper839.pdf": "https://doi.org/10.1145/3242587.3242627", "CHI18_paper306.pdf": "https://doi.org/10.1145/3173574.3173880 ", "CHI18_paper312.pdf": "https://doi.org/10.1145/3173574.3173886", "CHI18_paper474.pdf": "https://doi.org/10.1145/3173574.3174048", "CHI18_paper448.pdf": "https://doi.org/10.1145/3173574.3174022", "CHI18_paper470.pdf": "https://doi.org/10.1145/3173574.3174044 ", "CHI18_paper316.pdf": "https://doi.org/10.1145/3173574.3173890", "CHI18_paper302.pdf": "https://doi.org/10.1145/3173574.3173876 ", "CHI18_paper464.pdf": "https://doi.org/10.1145/3173574.3174038", "CHI18_paper458.pdf": "https://doi.org/10.1145/3173574.3174032", "CHI18_paper114.pdf": "https://doi.org/10.1145/3173574.3173688", "CHI18_paper666.pdf": "https://doi.org/10.1145/3173574.3174240", "CHI18_paper100.pdf": "https://doi.org/10.1145/3173574.3173674", "CHI18_paper128.pdf": "https://doi.org/10.1145/3173574.3173702", "CSCW18_paper29.pdf": "https://doi.org/10.1145/3274298", "CSCW18_paper196.pdf": "https://doi.org/10.1145/3274465", "CHI18_paper289.pdf": "https://doi.org/10.1145/3173574.3173863", "CSCW18_paper182.pdf": "https://doi.org/10.1145/3274451", "CHI18_paper504.pdf": "https://doi.org/10.1145/3173574.3174078", "CSCW18_paper169.pdf": "https://doi.org/10.1145/3274438", "CHI18_paper262.pdf": "https://doi.org/10.1145/3173574.3173836", "CHI18_paper276.pdf": "https://doi.org/10.1145/3173574.3173850", "CHI18_paper28.pdf": "https://doi.org/10.1145/3173574.3173602", "UIST18_paper5.pdf": "https://doi.org/10.1145/3242587.3242628", "CHI18_paper510.pdf": "https://doi.org/10.1145/3173574.3174084", "CSCW18_paper155.pdf": "https://doi.org/10.1145/3274424 ", "CHI18_paper538.pdf": "https://doi.org/10.1145/3173574.3174112", "CSCW18_paper141.pdf": "https://doi.org/10.1145/3274410", "CHI18_paper14.pdf": "https://doi.org/10.1145/3173574.3173588", "CHI18_paper15.pdf": "https://doi.org/10.1145/3173574.3173589", "CSCW18_paper140.pdf": "https://doi.org/10.1145/3274409 ", "CHI18_paper539.pdf": "https://doi.org/10.1145/3173574.3174113", "CSCW18_paper154.pdf": "https://doi.org/10.1145/3274423", "CHI18_paper277.pdf": "https://doi.org/10.1145/3173574.3173851", "CHI18_paper511.pdf": "https://doi.org/10.1145/3173574.3174085", "CHI18_paper29.pdf": "https://doi.org/10.1145/3173574.3173603", "CSCW18_paper168.pdf": "https://doi.org/10.1145/3274437", "CHI18_paper505.pdf": "https://doi.org/10.1145/3173574.3174079", "CHI18_paper263.pdf": "https://doi.org/10.1145/3173574.3173837", "CHI18_paper288.pdf": "https://doi.org/10.1145/3173574.3173862", "CSCW18_paper197.pdf": "https://doi.org/10.1145/3274466", "CSCW18_paper28.pdf": "https://doi.org/10.1145/3274297", "UIST18_paper237.pdf": "", "CHI18_paper129.pdf": "https://doi.org/10.1145/3173574.3173703", "CHI18_paper667.pdf": "https://doi.org/10.1145/3173574.3174241", "CHI18_paper101.pdf": "https://doi.org/10.1145/3173574.3173675", "CHI18_paper115.pdf": "https://doi.org/10.1145/3173574.3173689", "CHI18_paper459.pdf": "https://doi.org/10.1145/3173574.3174033", "CHI18_paper303.pdf": "https://doi.org/10.1145/3173574.3173877", "CHI18_paper465.pdf": "https://doi.org/10.1145/3173574.3174039", "CHI18_paper471.pdf": "https://doi.org/10.1145/3173574.3174045", "CHI18_paper317.pdf": "https://doi.org/10.1145/3173574.3173891", "UIST18_paper141.pdf": "https://doi.org/10.1145/3242587.3242639", "CHI18_paper498.pdf": "https://doi.org/10.1145/3173574.3174072", "CHI18_paper467.pdf": "https://doi.org/10.1145/3173574.3174041", "CHI18_paper301.pdf": "https://doi.org/10.1145/3173574.3173875 ", "CHI18_paper315.pdf": "https://doi.org/10.1145/3173574.3173889", "CHI18_paper473.pdf": "https://doi.org/10.1145/3173574.3174047", "CHI18_paper329.pdf": "https://doi.org/10.1145/3173574.3173903", "CHI18_paper103.pdf": "https://doi.org/10.1145/3173574.3173677", "CHI18_paper665.pdf": "https://doi.org/10.1145/3173574.3174239", "CHI18_paper117.pdf": "https://doi.org/10.1145/3173574.3173691", "CHI18_paper659.pdf": "https://doi.org/10.1145/3173574.3174233 ", "CSCW18_paper16.pdf": "https://doi.org/10.1145/3274285", "UIST18_paper745.pdf": "https://doi.org/10.1145/3242587.3242664", "CSCW18_paper181.pdf": "https://doi.org/10.1145/3274450", "UIST18_paper779.pdf": "https://doi.org/10.1145/3242587.3242605", "CSCW18_paper195.pdf": "https://doi.org/10.1145/3274464", "CHI18_paper513.pdf": "https://doi.org/10.1145/3173574.3174087", "CHI18_paper275.pdf": "https://doi.org/10.1145/3173574.3173849", "CHI18_paper261.pdf": "https://doi.org/10.1145/3173574.3173835", "CHI18_paper507.pdf": "https://doi.org/10.1145/3173574.3174081", "CSCW18_paper142.pdf": "https://doi.org/10.1145/3274411", "CHI18_paper17.pdf": "https://doi.org/10.1145/3173574.3173591", "CHI18_paper249.pdf": "https://doi.org/10.1145/3173574.3173823", "CSCW18_paper156.pdf": "https://doi.org/10.1145/3274425", "UIST18_paper963.pdf": "https://doi.org/10.1145/3242587.3242661", "CSCW18_paper157.pdf": "https://doi.org/10.1145/3274426", "CHI18_paper16.pdf": "https://doi.org/10.1145/3173574.3173590", "CSCW18_paper143.pdf": "https://doi.org/10.1145/3274412", "CHI18_paper248.pdf": "https://doi.org/10.1145/3173574.3173822", "UIST18_paper977.pdf": "https://doi.org/10.1145/3242587.3242600", "CHI18_paper260.pdf": "https://doi.org/10.1145/3173574.3173834", "CHI18_paper506.pdf": "https://doi.org/10.1145/3173574.3174080", "CHI18_paper512.pdf": "https://doi.org/10.1145/3173574.3174086", "UIST18_paper793.pdf": "https://doi.org/10.1145/3242587.3242646", "CHI18_paper274.pdf": "https://doi.org/10.1145/3173574.3173848 ", "CSCW18_paper194.pdf": "https://doi.org/10.1145/3274463", "CSCW18_paper180.pdf": "https://doi.org/10.1145/3274449", "CSCW18_paper17.pdf": "https://doi.org/10.1145/3274286", "CHI18_paper658.pdf": "https://doi.org/10.1145/3173574.3174232", "CHI18_paper116.pdf": "https://doi.org/10.1145/3173574.3173690", "CHI18_paper102.pdf": "https://doi.org/10.1145/3173574.3173676", "CHI18_paper664.pdf": "https://doi.org/10.1145/3173574.3174238", "CHI18_paper328.pdf": "https://doi.org/10.1145/3173574.3173902", "CHI18_paper314.pdf": "https://doi.org/10.1145/3173574.3173888", "CHI18_paper472.pdf": "https://doi.org/10.1145/3173574.3174046", "CHI18_paper466.pdf": "https://doi.org/10.1145/3173574.3174040", "CHI18_paper300.pdf": "https://doi.org/10.1145/3173574.3173874", "CHI18_paper499.pdf": "https://doi.org/10.1145/3173574.3174073", "CHI18_paper480.pdf": "https://doi.org/10.1145/3173574.3174054", "CHI18_paper494.pdf": "https://doi.org/10.1145/3173574.3174068", "CHI18_paper319.pdf": "https://doi.org/10.1145/3173574.3173893", "CHI18_paper325.pdf": "https://doi.org/10.1145/3173574.3173899", "CHI18_paper443.pdf": "https://doi.org/10.1145/3173574.3174017 ", "CHI18_paper457.pdf": "https://doi.org/10.1145/3173574.3174031 ", "CHI18_paper331.pdf": "https://doi.org/10.1145/3173574.3173905", "UIST18_paper365.pdf": "https://doi.org/10.1145/3242587.3242645", "UIST18_paper403.pdf": "https://doi.org/10.1145/3242587.3242654", "CHI18_paper641.pdf": "https://doi.org/10.1145/3173574.3174215", "CHI18_paper127.pdf": "https://doi.org/10.1145/3173574.3173701", "CHI18_paper133.pdf": "https://doi.org/10.1145/3173574.3173707", "CHI18_paper655.pdf": "https://doi.org/10.1145/3173574.3174229", "UIST18_paper19.pdf": "https://doi.org/10.1145/3242587.3242595", "CSCW18_paper32.pdf": "https://doi.org/10.1145/3274301", "UIST18_paper31.pdf": "https://doi.org/10.1145/3242587.3242594", "UIST18_paper991.pdf": "https://doi.org/10.1145/3242587.3242660", "CHI18_paper292.pdf": "https://doi.org/10.1145/3173574.3173866", "CSCW18_paper199.pdf": "https://doi.org/10.1145/3274468", "CHI18_paper286.pdf": "https://doi.org/10.1145/3173574.3173860", "CSCW18_paper166.pdf": "https://doi.org/10.1145/3274435", "CHI18_paper33.pdf": "https://doi.org/10.1145/3173574.3173607", "CSCW18_paper172.pdf": "https://doi.org/10.1145/3274441", "CHI18_paper27.pdf": "https://doi.org/10.1145/3173574.3173601", "CHI18_paper279.pdf": "https://doi.org/10.1145/3173574.3173853", "CHI18_paper251.pdf": "https://doi.org/10.1145/3173574.3173825", "CHI18_paper537.pdf": "https://doi.org/10.1145/3173574.3174111", "CHI18_paper523.pdf": "https://doi.org/10.1145/3173574.3174097", "CHI18_paper245.pdf": "https://doi.org/10.1145/3173574.3173819", "CHI18_paper522.pdf": "https://doi.org/10.1145/3173574.3174096", "CHI18_paper244.pdf": "https://doi.org/10.1145/3173574.3173818", "CHI18_paper250.pdf": "https://doi.org/10.1145/3173574.3173824", "CHI18_paper536.pdf": "https://doi.org/10.1145/3173574.3174110", "CHI18_paper26.pdf": "https://doi.org/10.1145/3173574.3173600", "CSCW18_paper173.pdf": "https://doi.org/10.1145/3274442", "CHI18_paper278.pdf": "https://doi.org/10.1145/3173574.3173852", "CHI18_paper32.pdf": "https://doi.org/10.1145/3173574.3173606", "CSCW18_paper167.pdf": "https://doi.org/10.1145/3274436", "CHI18_paper287.pdf": "https://doi.org/10.1145/3173574.3173861", "CHI18_paper293.pdf": "https://doi.org/10.1145/3173574.3173867", "CSCW18_paper198.pdf": "https://doi.org/10.1145/3274467", "CSCW18_paper33.pdf": "https://doi.org/10.1145/3274302", "CSCW18_paper27.pdf": "https://doi.org/10.1145/3274296", "CHI18_paper132.pdf": "https://doi.org/10.1145/3173574.3173706", "CHI18_paper654.pdf": "https://doi.org/10.1145/3173574.3174228", "CHI18_paper640.pdf": "https://doi.org/10.1145/3173574.3174214", "CHI18_paper126.pdf": "https://doi.org/10.1145/3173574.3173700", "CHI18_paper456.pdf": "https://doi.org/10.1145/3173574.3174030", "CHI18_paper330.pdf": "https://doi.org/10.1145/3173574.3173904", "CHI18_paper324.pdf": "https://doi.org/10.1145/3173574.3173898", "CHI18_paper442.pdf": "https://doi.org/10.1145/3173574.3174016", "CHI18_paper318.pdf": "https://doi.org/10.1145/3173574.3173892", "CHI18_paper495.pdf": "https://doi.org/10.1145/3173574.3174069", "CHI18_paper481.pdf": "https://doi.org/10.1145/3173574.3174055 ", "CHI18_paper497.pdf": "https://doi.org/10.1145/3173574.3174071", "CHI18_paper483.pdf": "https://doi.org/10.1145/3173574.3174057", "CHI18_paper468.pdf": "https://doi.org/10.1145/3173574.3174042", "UIST18_paper825.pdf": "https://doi.org/10.1145/3242587.3242652", "CHI18_paper332.pdf": "https://doi.org/10.1145/3173574.3173906", "CHI18_paper454.pdf": "https://doi.org/10.1145/3173574.3174028", "CHI18_paper440.pdf": "https://doi.org/10.1145/3173574.3174014", "CHI18_paper326.pdf": "https://doi.org/10.1145/3173574.3173900", "CHI18_paper118.pdf": "https://doi.org/10.1145/3173574.3173692", "CHI18_paper656.pdf": "https://doi.org/10.1145/3173574.3174230", "CHI18_paper130.pdf": "https://doi.org/10.1145/3173574.3173704", "CHI18_paper124.pdf": "https://doi.org/10.1145/3173574.3173698", "CHI18_paper642.pdf": "https://doi.org/10.1145/3173574.3174216", "CSCW18_paper19.pdf": "https://doi.org/10.1145/3274288", "CSCW18_paper31.pdf": "https://doi.org/10.1145/3274300", "CHI18_paper285.pdf": "https://doi.org/10.1145/3173574.3173859", "CHI18_paper291.pdf": "https://doi.org/10.1145/3173574.3173865", "CSCW18_paper171.pdf": "https://doi.org/10.1145/32744401", "CHI18_paper24.pdf": "https://doi.org/10.1145/3173574.3173598", "CSCW18_paper165.pdf": "https://doi.org/10.1145/3274434", "CHI18_paper30.pdf": "https://doi.org/10.1145/3173574.3173604", "CHI18_paper508.pdf": "https://doi.org/10.1145/3173574.3174082", "UIST18_paper951.pdf": "https://doi.org/10.1145/3242587.3242632", "CHI18_paper246.pdf": "https://doi.org/10.1145/3173574.", "CHI18_paper18.pdf": "https://doi.org/10.1145/3173574.3173592", "CHI18_paper520.pdf": "https://doi.org/10.1145/3173574.3174094", "CHI18_paper534.pdf": "https://doi.org/10.1145/3173574.3174108", "CSCW18_paper159.pdf": "https://doi.org/10.1145/", "CHI18_paper252.pdf": "https://doi.org/10.1145/3173574.3173826", "CSCW18_paper158.pdf": "https://doi.org/10.1145/3274427", "CHI18_paper535.pdf": "https://doi.org/10.1145/3173574.3174109", "CHI18_paper253.pdf": "https://doi.org/10.1145/3173574.3173827", "CHI18_paper247.pdf": "https://doi.org/10.1145/3173574.3173821", "CHI18_paper521.pdf": "", "CHI18_paper19.pdf": "https://doi.org/10.1145/3173574.3173593", "CHI18_paper509.pdf": "https://doi.org/10.1145/3173574.3174083", "CHI18_paper31.pdf": "https://doi.org/10.1145/3173574.3173605", "CSCW18_paper164.pdf": "https://doi.org/10.1145/3274433", "CHI18_paper25.pdf": "https://doi.org/10.1145/3173574.3173599", "CSCW18_paper170.pdf": "https://doi.org/10.1145/3274439", "CHI18_paper290.pdf": "https://doi.org/10.1145/3173574.3173864", "CHI18_paper284.pdf": "https://doi.org/10.1145/3173574.3173858", "CSCW18_paper24.pdf": "https://doi.org/10.1145/3274293", "UIST18_paper213.pdf": "https://doi.org/10.1145/3242587.3242609", "CSCW18_paper30.pdf": "https://doi.org/10.1145/", "CSCW18_paper18.pdf": "https://doi.org/10.1145/3274287", "CHI18_paper125.pdf": "https://doi.org/10.1145/3173574.3173699", "CHI18_paper643.pdf": "https://doi.org/10.1145/3173574.3174217", "CHI18_paper657.pdf": "https://doi.org/10.1145/3173574.3174231", "CHI18_paper131.pdf": "https://doi.org/10.1145/3173574.3173705", "CHI18_paper119.pdf": "https://doi.org/10.1145/3173574.3173693", "CHI18_paper441.pdf": "https://doi.org/10.1145/3173574.3174015", "CHI18_paper327.pdf": "https://doi.org/10.1145/3173574.3173901", "CHI18_paper333.pdf": "https://doi.org/10.1145/3173574.3173907", "CHI18_paper455.pdf": "https://doi.org/10.1145/3173574.3174029", "CHI18_paper469.pdf": "https://doi.org/10.1145/3173574.3174043", "CHI18_paper482.pdf": "https://doi.org/10.1145/3173574.3174056", "UIST18_paper165.pdf": "https://doi.org/10.1145/3242587.3242598", "CHI18_paper496.pdf": "https://doi.org/10.1145/3173574.3174070", "CHI18_paper492.pdf": "https://doi.org/10.1145/3173574.3174066", "CHI18_paper486.pdf": "https://doi.org/10.1145/3173574.3174060", "CHI18_paper451.pdf": "https://doi.org/10.1145/3173574.3174025", "CHI18_paper337.pdf": "https://doi.org/10.1145/3173574.3173911", "CHI18_paper323.pdf": "https://doi.org/10.1145/3173574.3173897", "CHI18_paper445.pdf": "https://doi.org/10.1145/3173574.3174019", "CHI18_paper479.pdf": "https://doi.org/10.1145/3173574.3174053", "UIST18_paper411.pdf": "https://doi.org/10.1145/3242587.3242611", "CHI18_paper135.pdf": "https://doi.org/10.1145/3173574.3173709", "CHI18_paper653.pdf": "https://doi.org/10.1145/3173574.3174227", "CHI18_paper647.pdf": "https://doi.org/10.1145/3173574.3174221", "CHI18_paper121.pdf": "https://doi.org/10.1145/3173574.3173695", "CHI18_paper109.pdf": "https://doi.org/10.1145/3173574.3173683", "CSCW18_paper34.pdf": "https://doi.org/10.1145/3274303", "CSCW18_paper20.pdf": "https://doi.org/10.1145/3274289", "CHI18_paper280.pdf": "https://doi.org/10.1145/3173574.3173854", "CHI18_paper294.pdf": "https://doi.org/10.1145/3173574.3173868", "CSCW18_paper148.pdf": "https://doi.org/10.1145/3274417", "CHI18_paper525.pdf": "https://doi.org/10.1145/3173574.3174099", "CHI18_paper243.pdf": "https://doi.org/10.1145/3173574.3173817", "CHI18_paper257.pdf": "https://doi.org/10.1145/3173574.3173831", "CHI18_paper531.pdf": "https://doi.org/10.1145/3173574.3174105", "CHI18_paper519.pdf": "https://doi.org/10.1145/3173574.3174093", "CSCW18_paper174.pdf": "https://doi.org/10.1145/3274443", "CHI18_paper21.pdf": "https://doi.org/10.1145/3173574.3173595", "CSCW18_paper160.pdf": "https://doi.org/10.1145/3274470", "CHI18_paper35.pdf": "https://doi.org/10.1145/3173574.3173609", "CHI18_paper34.pdf": "https://doi.org/10.1145/3173574.3173608", "CSCW18_paper161.pdf": "https://doi.org/10.1145/3274430.", "CHI18_paper20.pdf": "https://doi.org/10.1145/3173574.3173594", "CSCW18_paper175.pdf": "https://doi.org/10.1145/3274444", "CHI18_paper518.pdf": "https://doi.org/10.1145/3173574.3174092", "CHI18_paper256.pdf": "https://doi.org/10.1145/3173574.3173830", "CHI18_paper530.pdf": "https://doi.org/10.1145/3173574.3174104", "CHI18_paper524.pdf": "https://doi.org/10.1145/3173574.3174098", "CSCW18_paper149.pdf": "https://doi.org/10.1145/3274418", "CHI18_paper242.pdf": "https://doi.org/10.1145/3173574.3173816", "CHI18_paper295.pdf": "https://doi.org/10.1145/3173574.3173869", "CHI18_paper281.pdf": "https://doi.org/10.1145/3173574.3173855", "CSCW18_paper21.pdf": "https://doi.org/10.1145/3274290", "CHI18_paper108.pdf": "https://doi.org/10.1145/3173574.3173682", "UIST18_paper389.pdf": "https://doi.org/10.1145/3242587.3242653", "CHI18_paper646.pdf": "https://doi.org/10.1145/3173574.3174220", "CHI18_paper120.pdf": "https://doi.org/10.1145/3173574.3173694", "CHI18_paper134.pdf": "https://doi.org/10.1145/3173574.3173708", "CHI18_paper652.pdf": "https://doi.org/10.1145/3173574.3174226", "CHI18_paper478.pdf": "https://doi.org/10.1145/3173574.3174052", "CHI18_paper322.pdf": "https://doi.org/10.1145/3173574.3173896", "CHI18_paper444.pdf": "https://doi.org/10.1145/3173574.3174018", "CHI18_paper450.pdf": "https://doi.org/10.1145/3173574.3174024", "UIST18_paper809.pdf": "https://doi.org/10.1145/3242587.3242644", "CHI18_paper336.pdf": "https://doi.org/10.1145/3173574.3173910 ", "CHI18_paper487.pdf": "https://doi.org/10.1145/3173574.3174061", "CHI18_paper493.pdf": "https://doi.org/10.1145/3173574.3174067", "CHI18_paper485.pdf": "https://doi.org/10.1145/3173574.3174059", "CHI18_paper491.pdf": "https://doi.org/10.1145/3173574.3174065", "CHI18_paper446.pdf": "https://doi.org/10.1145/3173574.3174020", "CHI18_paper320.pdf": "https://doi.org/10.1145/3173574.3173894", "CHI18_paper334.pdf": "https://doi.org/10.1145/3173574.3173908", "CHI18_paper452.pdf": "", "CHI18_paper308.pdf": "https://doi.org/10.1145/3173574.3173882", "UIST18_paper189.pdf": "https://doi.org/10.1145/3242587.3242666", "CHI18_paper122.pdf": "https://doi.org/10.1145/3173574.3173696", "CHI18_paper644.pdf": "https://doi.org/10.1145/3173574.3174218", "CHI18_paper650.pdf": "https://doi.org/10.1145/3173574.3174224", "CHI18_paper136.pdf": "https://doi.org/10.1145/3173574.3173710", "CSCW18_paper201.pdf": "https://doi.org/10.1145/3274771", "CSCW18_paper23.pdf": "https://doi.org/10.1145/3274292", "CSCW18_paper37.pdf": "https://doi.org/10.1145/3274306", "CHI18_paper297.pdf": "https://doi.org/10.1145/3173574.3173871", "CHI18_paper283.pdf": "https://doi.org/10.1145/3173574.3173857.", "CSCW18_paper188.pdf": "https://doi.org/10.1145/3274457", "CHI18_paper532.pdf": "https://doi.org/10.1145/3173574.3174106", "CHI18_paper254.pdf": "https://doi.org/10.1145/3173574.3173828", "CHI18_paper240.pdf": "https://doi.org/10.1145/3173574.3173814", "CHI18_paper526.pdf": "https://doi.org/10.1145/3173574.3174100", "CSCW18_paper163.pdf": "https://doi.org/10.1145/3274432", "CHI18_paper36.pdf": "https://doi.org/10.1145/3173574.3173610", "CHI18_paper268.pdf": "https://doi.org/10.1145/3173574.3173842", "CSCW18_paper177.pdf": "https://doi.org/10.1145/3274446", "CHI18_paper22.pdf": "https://doi.org/10.1145/3173574.3173596", "CHI18_paper23.pdf": "https://doi.org/10.1145/3173574.3173597", "CSCW18_paper176.pdf": "https://doi.org/10.1145/3274445", "CHI18_paper37.pdf": "https://doi.org/10.1145/3173574.3173611", "CSCW18_paper162.pdf": "https://doi.org/10.1145/3274431", "CHI18_paper269.pdf": "https://doi.org/10.1145/3173574.3173843", "CHI18_paper241.pdf": "https://doi.org/10.1145/3173574.3173815", "CHI18_paper527.pdf": "https://doi.org/10.1145/3173574.3174101", "CHI18_paper533.pdf": "https://doi.org/10.1145/3173574.3174107", "CHI18_paper255.pdf": "https://doi.org/10.1145/3173574.3173829", "CHI18_paper282.pdf": "https://doi.org/10.1145/3173574.3173856", "UIST18_paper765.pdf": "https://doi.org/10.1145/3242587.3242647", "CSCW18_paper189.pdf": "https://doi.org/10.1145/3274458", "CHI18_paper296.pdf": "https://doi.org/10.1145/373574.3173870", "UIST18_paper201.pdf": "https://doi.org/10.1145/3242587.3242633", "CSCW18_paper36.pdf": "https://doi.org/10.1145/3274305", "CSCW18_paper22.pdf": "https://doi.org/10.1145/3274291", "CSCW18_paper200.pdf": "https://doi.org/10.1145/3274469", "CHI18_paper651.pdf": "https://doi.org/10.1145/3173574.3174225", "CHI18_paper137.pdf": "https://doi.org/10.1145/3173574.3173711", "CHI18_paper123.pdf": "https://doi.org/10.1145/3173574.3173697", "CHI18_paper645.pdf": "https://doi.org/10.1145/3173574.3174219", "CHI18_paper309.pdf": "https://doi.org/10.1145/3173574.3173883", "CHI18_paper335.pdf": "https://doi.org/10.1145/3173574.3173909", "CHI18_paper453.pdf": "https://doi.org/10.1145/3173574.3174027", "CHI18_paper447.pdf": "https://doi.org/10.1145/3173574.3174021", "CHI18_paper321.pdf": "https://doi.org/10.1145/3173574.3173895", "UIST18_paper177.pdf": "https://doi.org/10.1145/3242587.3242621", "CHI18_paper490.pdf": "https://doi.org/10.1145/3173574.3174064", "CHI18_paper484.pdf": "https://doi.org/10.1145/3173574.3174058", "CHI18_paper385.pdf": "https://doi.org/10.1145/3173574.3173959", "CHI18_paper391.pdf": "https://doi.org/10.1145/3173574.3173965", "Ubicomp18_paper189.pdf": "https://doi.org/10.1145/3287067", "Ubicomp18_paper162.pdf": "https://doi.org/10.1145/3287040", "CHI18_paper408.pdf": "https://doi.org/10.1145/3173574.3173982", "Ubicomp18_paper176.pdf": "https://doi.org/10.1145/3287054", "CHI18_paper2.pdf": "https://doi.org/10.1145/3173574.3173576", "CHI18_paper420.pdf": "https://doi.org/10.1145/3173574.3173994", "CHI18_paper346.pdf": "https://doi.org/10.1145/3173574.3173920", "CHI18_paper352.pdf": "https://doi.org/10.1145/3173574.3173926", "CHI18_paper434.pdf": "https://doi.org/10.1145/3173574.3174008", "CHI18_paper187.pdf": "https://doi.org/10.1145/3173574.3173761", "CHI18_paper193.pdf": "https://doi.org/10.1145/3173574.3173767", "CHI18_paper178.pdf": "https://doi.org/10.1145/3173574.3173752", "CHI18_paper144.pdf": "https://doi.org/10.1145/3173574.3173718", "CHI18_paper622.pdf": "https://doi.org/10.1145/3173574.3174196", "CHI18_paper636.pdf": "https://doi.org/10.1145/3173574.3174210", "CHI18_paper150.pdf": "https://doi.org/10.1145/3173574.3173724", "CSCW18_paper79.pdf": "https://doi.org/10.1145/3274348", "CSCW18_paper45.pdf": "https://doi.org/10.1145/3274314", "CSCW18_paper51.pdf": "https://doi.org/10.1145/3274320", "Ubicomp18_paper200.pdf": "https://doi.org/10.1145/3287078", "UIST18_paper299.pdf": "https://doi.org/10.1145/3242587.3242624", "CSCW18_paper86.pdf": "https://doi.org/10.1145/3274355", "CSCW18_paper92.pdf": "https://doi.org/10.1145/3274361", "CHI18_paper93.pdf": "https://doi.org/10.1145/3173574.3173667", "CHI18_paper87.pdf": "https://doi.org/10.1145/3173574.3173661 ", "CHI18_paper597.pdf": "https://doi.org/10.1145/3173574.3174171", "CHI18_paper583.pdf": "https://doi.org/10.1145/3173574.3174157", "CHI18_paper568.pdf": "https://doi.org/10.1145/3173574.3174142 ", "CSCW18_paper105.pdf": "https://doi.org/10.1145/", "CHI18_paper50.pdf": "https://doi.org/10.1145/3173574.3173624", "CSCW18_paper111.pdf": "https://doi.org/10.1145/3274380", "CHI18_paper44.pdf": "https://doi.org/10.1145/3173574.3173618", "CSCW18_paper139.pdf": "https://doi.org/10.1145/3274407", "CHI18_paper554.pdf": "https://doi.org/10.1145/3173574.3174128", "CHI18_paper232.pdf": "https://doi.org/10.1145/3173574.3173806", "CHI18_paper226.pdf": "https://doi.org/10.1145/3173574.3173800", "CHI18_paper540.pdf": "https://doi.org/10.1145/3173574.3174114", "CHI18_paper78.pdf": "https://doi.org/10.1145/3173574.3173652", "CHI18_paper227.pdf": "https://doi.org/10.1145/3173574.3173801", "CHI18_paper79.pdf": "https://doi.org/10.1145/3173574.3173653", "CHI18_paper541.pdf": "https://doi.org/10.1145/3173574.3174115", "CHI18_paper555.pdf": "https://doi.org/10.1145/3173574.3174129", "CSCW18_paper138.pdf": "https://doi.org/10.1145/3274407", "CHI18_paper233.pdf": "https://doi.org/10.1145/3173574.3173807", "CHI18_paper45.pdf": "https://doi.org/10.1145/3173574.3173619", "CSCW18_paper110.pdf": "https://doi.org/10.1145/3274379", "CHI18_paper51.pdf": "https://doi.org/10.1145/3173574.3173625", "CHI18_paper569.pdf": "https://doi.org/10.1145/3173574.3174143", "CHI18_paper582.pdf": "https://doi.org/10.1145/3173574.3174156", "CHI18_paper596.pdf": "https://doi.org/10.1145/3173574.3174170", "CHI18_paper86.pdf": "https://doi.org/10.1145/3173574.3173660", "CHI18_paper92.pdf": "https://doi.org/10.1145/3173574.3173666", "Ubicomp18_paper201.pdf": "https://doi.org/10.1145/3287079", "CSCW18_paper50.pdf": "https://doi.org/10.1145/3274319", "UIST18_paper53.pdf": "https://doi.org/10.1145/3242587.3242626", "CHI18_paper637.pdf": "https://doi.org/10.1145/3173574.3174211", "CHI18_paper151.pdf": "https://doi.org/10.1145/3173574.3173725", "CHI18_paper145.pdf": "https://doi.org/10.1145/3173574.3173719", "CHI18_paper623.pdf": "https://doi.org/10.1145/3173574.3174197 ", "CHI18_paper179.pdf": "https://doi.org/10.1145/3173574.3173753", "UIST18_paper313.pdf": "https://doi.org/10.1145/3242587.3242641", "CHI18_paper192.pdf": "https://doi.org/10.1145/3173574.3173766", "CHI18_paper186.pdf": "https://doi.org/10.1145/3173574.3173760 ", "CHI18_paper353.pdf": "https://doi.org/10.1145/3173574.3173927", "CHI18_paper435.pdf": "https://doi.org/10.1145/3173574.3174009", "CHI18_paper421.pdf": "https://doi.org/10.1145/3173574.3173995", "CHI18_paper3.pdf": "https://doi.org/10.1145/3173574.3173577", "CHI18_paper347.pdf": "https://doi.org/10.1145/3173574.3173921", "Ubicomp18_paper177.pdf": "https://doi.org/10.1145/3287055", "CHI18_paper409.pdf": "https://doi.org/10.1145/3173574.3173983", "Ubicomp18_paper163.pdf": "https://doi.org/10.1145/3287041", "CHI18_paper390.pdf": "https://doi.org/10.1145/3173574.3173964", "Ubicomp18_paper188.pdf": "https://doi.org/10.1145/3287066", "CHI18_paper384.pdf": "https://doi.org/10.1145/3173574.3173958", "UIST18_paper663.pdf": "https://doi.org/10.1145/3242587.3242618", "UIST18_paper649.pdf": "https://doi.org/10.1145/3242587.3242612", "UIST18_paper113.pdf": "https://doi.org/10.1145/3242587.3242607", "CHI18_paper392.pdf": "https://doi.org/10.1145/3173574.3173966", "UIST18_paper675.pdf": "https://doi.org/10.1145/3242587.3242613", "CHI18_paper386.pdf": "https://doi.org/10.1145/3173574.3173960", "Ubicomp18_paper175.pdf": "https://doi.org/10.1145/3287053", "CHI18_paper379.pdf": "https://doi.org/10.1145/3173574.3173953", "Ubicomp18_paper161.pdf": "https://doi.org/10.1145/3287039", "CHI18_paper437.pdf": "https://doi.org/10.1145/3173574.3174011", "CHI18_paper351.pdf": "https://doi.org/10.1145/3173574.3173925", "CHI18_paper345.pdf": "https://doi.org/10.1145/3173574.3173919", "CHI18_paper1.pdf": "https://doi.org/10.1145/3173574.3173575", "CHI18_paper423.pdf": "https://doi.org/10.1145/3173574.3173997", "CHI18_paper190.pdf": "https://doi.org/10.1145/3173574.3173764", "CHI18_paper184.pdf": "https://doi.org/10.1145/3173574.3173758", "CHI18_paper609.pdf": "https://doi.org/10.1145/3173574.3174183", "CHI18_paper153.pdf": "https://doi.org/10.1145/3173574.3173727", "CHI18_paper635.pdf": "https://doi.org/10.1145/3173574.3174209", "CHI18_paper621.pdf": "https://doi.org/10.1145/3173574.3174195", "CHI18_paper147.pdf": "https://doi.org/10.1145/3173574.3173721", "CSCW18_paper52.pdf": "https://doi.org/10.1145/3274321", "CSCW18_paper46.pdf": "https://doi.org/10.1145/", "UIST18_paper45.pdf": "https://doi.org/10.1145/3242587.3242631", "CHI18_paper84.pdf": "https://doi.org/10.1145/3173574.3173658", "CHI18_paper90.pdf": "https://doi.org/10.1145/3173574.3173664", "CHI18_paper580.pdf": "https://doi.org/10.1145/3173574.3174154", "CHI18_paper594.pdf": "https://doi.org/10.1145/3173574.3174168", "CSCW18_paper112.pdf": "https://doi.org/10.1145/3274381", "CHI18_paper47.pdf": "https://doi.org/10.1145/3173574.3173621", "CHI18_paper219.pdf": "https://doi.org/10.1145/3173574.3173793", "CSCW18_paper106.pdf": "https://doi.org/10.1145/3274375", "CHI18_paper53.pdf": "https://doi.org/10.1145/3173574.3173627", "CHI18_paper543.pdf": "https://doi.org/10.1145/3173574.3174117", "CHI18_paper225.pdf": "https://doi.org/10.1145/3173574.3173799", "CHI18_paper231.pdf": "https://doi.org/10.1145/3173574.3173805", "CHI18_paper557.pdf": "https://doi.org/10.1145/3173574.3174131", "CHI18_paper230.pdf": "https://doi.org/10.1145/3173574.3173804", "CHI18_paper556.pdf": "https://doi.org/10.1145/3173574.3174130", "CHI18_paper542.pdf": "https://doi.org/10.1145/3173574.3174116.", "CHI18_paper224.pdf": "https://doi.org/10.1145/3173574.3173798", "CHI18_paper52.pdf": "https://doi.org/10.1145/3173574.3173626", "CSCW18_paper107.pdf": "https://doi.org/10.1145/3274376.", "CHI18_paper46.pdf": "https://doi.org/10.1145/3173574.3173620", "CSCW18_paper113.pdf": "https://doi.org/10.1145/3274382 ", "UIST18_paper927.pdf": "https://doi.org/10.1145/3242587.3242588", "CHI18_paper218.pdf": "https://doi.org/10.1145/3173574.3173792", "CHI18_paper595.pdf": "https://doi.org/10.1145/3025453.3025516", "CHI18_paper581.pdf": "https://doi.org/10.1145/3173574.3174155", "CHI18_paper91.pdf": "https://doi.org/10.1145/3173574.3173665", "CHI18_paper85.pdf": "https://doi.org/10.1145/3173574.3173659", "UIST18_paper87.pdf": "https://doi.org/10.1145/3242587.3242658", "Ubicomp18_paper202.pdf": "https://doi.org/10.1145/3287080", "CSCW18_paper47.pdf": "https://doi.org/10.1145/3274316", "CSCW18_paper53.pdf": "https://doi.org/10.1145/3274322", "CHI18_paper620.pdf": "https://doi.org/10.1145/3173574.3174194", "CHI18_paper146.pdf": "https://doi.org/10.1145/3173574.3173720", "CHI18_paper152.pdf": "https://doi.org/10.1145/3173574.3173726", "CHI18_paper634.pdf": "https://doi.org/10.1145/3173574.3174208", "CHI18_paper608.pdf": "https://doi.org/10.1145/3173574.3174182", "CHI18_paper185.pdf": "https://doi.org/10.1145/3173574.3173759", "CHI18_paper191.pdf": "https://doi.org/10.1145/3173574.3173765", "CHI18_paper344.pdf": "https://doi.org/10.1145/3173574.3173918", "CHI18_paper422.pdf": "https://doi.org/10.1145/3173574.3173996", "CHI18_paper436.pdf": "https://doi.org/10.1145/3173574.3174010", "CHI18_paper350.pdf": "https://doi.org/10.1145/3173574.3173924", "CHI18_paper378.pdf": "https://doi.org/10.1145/3173574.3173952", "Ubicomp18_paper160.pdf": "https://doi.org/10.1145/3287038", "Ubicomp18_paper174.pdf": "https://doi.org/10.1145/3287052", "UIST18_paper853.pdf": "https://doi.org/10.1145/3242587.3242597", "CHI18_paper387.pdf": "https://doi.org/10.1145/3173574.3173961", "CHI18_paper393.pdf": "https://doi.org/10.1145/3173574.3173967", "CHI18_paper397.pdf": "https://doi.org/10.1145/3173574.3173971", "CHI18_paper383.pdf": "https://doi.org/10.1145/3173574.3173957", "CHI18_paper354.pdf": "https://doi.org/10.1145/3173574.3173928", "CHI18_paper432.pdf": "https://doi.org/10.1145/3173574.3174006", "CHI18_paper4.pdf": "https://doi.org/10.1145/3173574.3173578", "CHI18_paper426.pdf": "https://doi.org/10.1145/3173574.3174000", "Ubicomp18_paper158.pdf": "https://doi.org/10.1145/3287036", "CHI18_paper340.pdf": "https://doi.org/10.1145/3173574.3173914", "CHI18_paper368.pdf": "https://doi.org/10.1145/3173574.3173942", "Ubicomp18_paper170.pdf": "https://doi.org/10.1145/3287048", "Ubicomp18_paper164.pdf": "https://doi.org/10.1145/", "CHI18_paper195.pdf": "https://doi.org/10.1145/3173574.3173769", "CHI18_paper181.pdf": "https://doi.org/10.1145/3173574.3173755", "CHI18_paper630.pdf": "https://doi.org/10.1145/3173574.3174204", "CHI18_paper156.pdf": "https://doi.org/10.1145/3173574.3173730", "CHI18_paper142.pdf": "https://doi.org/10.1145/3173574.3173716", "CHI18_paper624.pdf": "https://doi.org/10.1145/3173574.3174198", "UIST18_paper499.pdf": "https://doi.org/10.1145/3242587.3242630", "CHI18_paper618.pdf": "https://doi.org/10.1145/3173574.3174192", "CHI18_paper585.pdf": "https://doi.org/10.1145/3173574.3174159", "CHI18_paper591.pdf": "https://doi.org/10.1145/3173574.3174165", "CHI18_paper81.pdf": "https://doi.org/10.1145/3173574.3173655", "CHI18_paper95.pdf": "https://doi.org/10.1145/3173574.3173669", "CHI18_paper220.pdf": "https://doi.org/10.1145/3173574.3173794", "CHI18_paper546.pdf": "https://doi.org/10.1145/3173574.3174120", "CHI18_paper552.pdf": "https://doi.org/10.1145/3173574.3174126", "CHI18_paper234.pdf": "https://doi.org/10.1145/3173574.3173808", "CSCW18_paper117.pdf": "https://doi.org/10.1145/3274386", "CHI18_paper42.pdf": "https://doi.org/10.1145/3173574.3173616", "CSCW18_paper103.pdf": "https://doi.org/10.1145/3274372", "CHI18_paper56.pdf": "https://doi.org/10.1145/3173574.3173630", "UIST18_paper937.pdf": "https://doi.org/10.1145/3242587.3242649", "CHI18_paper208.pdf": "https://doi.org/10.1145/3173574.3173782", "CHI18_paper57.pdf": "https://doi.org/10.1145/3173574.3173631", "CSCW18_paper102.pdf": "https://doi.org/10.1145/3274371", "CHI18_paper209.pdf": "https://doi.org/10.1145/3173574.3173783", "CHI18_paper43.pdf": "https://doi.org/10.475/123_4", "CSCW18_paper116.pdf": "https://doi.org/10.1145/3274385", "CHI18_paper553.pdf": "https://doi.org/10.1145/3173574.3174127", "CHI18_paper235.pdf": "https://doi.org/10.1145/3173574.3173809", "CHI18_paper221.pdf": "https://doi.org/10.1145/3173574.3173795", "CHI18_paper547.pdf": "https://doi.org/10.1145/3173574.3174121", "CHI18_paper94.pdf": "https://doi.org/10.1145/3173574.3173668", "CHI18_paper80.pdf": "https://doi.org/10.1145/3173574.3173654", "CHI18_paper590.pdf": "https://doi.org/10.1145/3173574.3174164", "UIST18_paper711.pdf": "https://doi.org/10.1145/3242587.3242642", "CHI18_paper584.pdf": "https://doi.org/10.1145/3173574.3174158", "CSCW18_paper95.pdf": "https://doi.org/10.1145/3274364", "CSCW18_paper42.pdf": "https://doi.org/10.1145/3274311", "UIST18_paper275.pdf": "https://doi.org/10.1145/3242587.3242592", "UIST18_paper261.pdf": "https://doi.org/10.1145/3242587.3242623 ", "CSCW18_paper56.pdf": "https://doi.org/10.1145/3274325", "CHI18_paper619.pdf": "https://doi.org/10.1145/3173574.3174193", "CHI18_paper143.pdf": "https://doi.org/10.1145/3173574.3173717", "CHI18_paper625.pdf": "https://doi.org/10.1145/3173574.3174199", "CHI18_paper631.pdf": "https://doi.org/10.1145/3173574.3174205", "CHI18_paper157.pdf": "https://doi.org/10.1145/3173574.3173731", "CHI18_paper180.pdf": "https://doi.org/10.1145/3173574.3173754", "CHI18_paper194.pdf": "https://doi.org/10.1145/3173574.3173768", "UIST18_paper473.pdf": "https://doi.org/10.1145/3242587.3242620", "Ubicomp18_paper165.pdf": "https://doi.org/10.1145/3287043", "CHI18_paper369.pdf": "https://doi.org/10.1145/3173574.3173943", "Ubicomp18_paper171.pdf": "https://doi.org/10.1145/3287049", "Ubicomp18_paper159.pdf": "https://doi.org/10.1145/3287037", "CHI18_paper427.pdf": "https://doi.org/10.1145/3173574.3174001", "CHI18_paper5.pdf": "https://doi.org/10.1145/10.1145/3173574.3173579", "CHI18_paper341.pdf": "https://doi.org/10.1145/3173574.3173915", "CHI18_paper355.pdf": "https://doi.org/10.1145/3173574.3173929", "CHI18_paper433.pdf": "https://doi.org/10.1145/3173574.3174007", "CHI18_paper382.pdf": "https://doi.org/10.1145/3173574.3173956", "CHI18_paper396.pdf": "https://doi.org/10.1145/3173574.3173970", "CHI18_paper380.pdf": "https://doi.org/10.1145/3173574.3173954", "Ubicomp18_paper198.pdf": "https://doi.org/10.1145/3287076", "CHI18_paper394.pdf": "https://doi.org/10.1145/3173574.3173968", "CHI18_paper343.pdf": "https://doi.org/10.1145/3173574.3173917", "CHI18_paper7.pdf": "https://doi.org/10.1145/3173574.3173581", "CHI18_paper425.pdf": "https://doi.org/10.1145/3173574.3173999", "CHI18_paper431.pdf": "https://doi.org/10.1145/3173574.3174005", "CHI18_paper357.pdf": "https://doi.org/10.1145/3173574.3173931", "CHI18_paper419.pdf": "https://doi.org/10.1145/3173574.3173993", "Ubicomp18_paper167.pdf": "https://doi.org/10.1145/3287045", "Ubicomp18_paper173.pdf": "https://doi.org/10.1145/3287051", "CHI18_paper182.pdf": "https://doi.org/10.1145/3173574.3173756", "CHI18_paper196.pdf": "https://doi.org/10.1145/3173574.3173770", "CHI18_paper627.pdf": "https://doi.org/10.1145/3173574.3174201", "CHI18_paper141.pdf": "https://doi.org/10.1145/3173574.3173715", "CHI18_paper155.pdf": "https://doi.org/10.1145/3173574.3173729", "CHI18_paper633.pdf": "https://doi.org/10.1145/3173574.3174207", "CHI18_paper169.pdf": "https://doi.org/10.1145/3173574.3173743", "UIST18_paper511.pdf": "https://doi.org/10.1145/3242587.3242648", "CSCW18_paper54.pdf": "https://doi.org/10.1145/3274323", "CSCW18_paper68.pdf": "https://doi.org/10.1145/3274337", "CSCW18_paper97.pdf": "https://doi.org/10.1145/3274366", "CHI18_paper592.pdf": "https://doi.org/10.1145/3173574.3174166", "CHI18_paper586.pdf": "https://doi.org/10.1145/3173574.3174160", "CHI18_paper96.pdf": "https://doi.org/10.1145/3173574.3173670", "CHI18_paper82.pdf": "https://doi.org/10.1145/3173574.3173656", "CHI18_paper69.pdf": "https://doi.org/10.1145/3173574.3173643", "CHI18_paper551.pdf": "https://doi.org/10.1145/3173574.3174125", "CHI18_paper545.pdf": "https://doi.org/10.1145/3173574.3174119", "CSCW18_paper128.pdf": "https://doi.org/10.1145/3274397", "CHI18_paper223.pdf": "https://doi.org/10.1145/3173574.3173797", "CHI18_paper55.pdf": "https://doi.org/10.1145/3173574.3173629", "CSCW18_paper114.pdf": "https://doi.org/10.1145/3274383", "CHI18_paper41.pdf": "https://doi.org/10.1145/3173574.3173615", "CHI18_paper579.pdf": "https://doi.org/10.1145/3173574.3174153", "CHI18_paper578.pdf": "https://doi.org/10.1145/3173574.3174152 ", "CHI18_paper40.pdf": "https://doi.org/10.1145/3173574.3173614", "CSCW18_paper115.pdf": "https://doi.org/10.1145/3274384", "CHI18_paper54.pdf": "https://doi.org/10.1145/3173574.3173628", "CSCW18_paper129.pdf": "https://doi.org/10.1145/3274398", "CHI18_paper544.pdf": "https://doi.org/10.1145/3173574.3174118", "CHI18_paper222.pdf": "https://doi.org/10.1145/3173574.3173796", "CHI18_paper236.pdf": "https://doi.org/10.1145/3173574.3173810", "CHI18_paper550.pdf": "https://doi.org/10.1145/3173574.3174124", "CHI18_paper68.pdf": "https://doi.org/10.1145/3173574.3173642", "CHI18_paper83.pdf": "https://doi.org/10.1145/3173574.3173657", "CHI18_paper97.pdf": "https://doi.org/10.1145/3173574.3173671", "CHI18_paper587.pdf": "https://doi.org/10.1145/3173574.3174161", "CHI18_paper593.pdf": "https://doi.org/10.1145/3173574.3174167", "CSCW18_paper96.pdf": "https://doi.org/10.1145/3274365", "CSCW18_paper82.pdf": "https://doi.org/10.1145/3274351", "CSCW18_paper41.pdf": "https://doi.org/10.1145/3274310", "CHI18_paper168.pdf": "https://doi.org/10.1145/3173574.3173742 ", "CHI18_paper154.pdf": "https://doi.org/10.1145/3173574.3173728", "CHI18_paper632.pdf": "https://doi.org/10.1145/3173574.3174206", "CHI18_paper626.pdf": "https://doi.org/10.1145/3173574.3174200", "CHI18_paper140.pdf": "https://doi.org/10.1145/3173574.3173714 ", "CHI18_paper197.pdf": "https://doi.org/10.1145/3173574.3173771", "CHI18_paper183.pdf": "https://doi.org/10.1145/3173574.3173757", "Ubicomp18_paper172.pdf": "https://doi.org/10.1145/3287050", "Ubicomp18_paper166.pdf": "https://doi.org/10.1145/3287044", "CHI18_paper418.pdf": "https://doi.org/10.1145/3173574.3173992", "CHI18_paper430.pdf": "https://doi.org/10.1145/3173574.3174004", "CHI18_paper356.pdf": "https://doi.org/10.1145/3173574.3173930", "CHI18_paper342.pdf": "https://doi.org/10.1145/3173574.3173916", "CHI18_paper424.pdf": "https://doi.org/10.1145/3173574.3173998", "CHI18_paper6.pdf": "https://doi.org/10.1145/3173574.3173580", "CHI18_paper395.pdf": "https://doi.org/10.1145/3173574.3173969", "CHI18_paper381.pdf": "https://doi.org/10.1145/3173574.3173955", "Ubicomp18_paper199.pdf": "https://doi.org/10.1145/"}, "captions_dict": {"CHI18_paper363-Figure2-1.png": "Figure 2. Experiment setup: HTC Vive lighthouses were placed in front of the participant, while the Optitrack cameras and the LG projector where placed around and over the participant.", "CHI18_paper21-Figure1-1.png": "Figure 1. We propose to extend keyboard shortcuts with arm and wrist rotations gestures, performed while pressing down a key. The figure shows left/right wrist rolls. The user\u2019s rotation angles can be used, for example, for continuous control, such as changing the volume.", "CHI18_paper21-Figure2-1.png": "Figure 2. The six rotation directions.", "CHI18_paper334-Figure5-1.png": "Figure 5: Top-ten chosen gestures in each of the three conditions: standing, sitting and projection", "CHI18_paper508-Figure7-1.png": "Figure 7. Data collection on kinematics. High-fidelity optical motion tracking was used to track a marker on the finger nail. A custom-made single-button setup was created, using switches and key caps from commercial keyboards.", "CHI18_paper508-Figure3-1.png": "Figure 3. Perceptual control of a button: the motor system has no access to the true moment when the button is activated, but it can try to reduce error (distance) between estimated and perceived sensations. Left: perceptual control fails when error is high. Right: precise control is achieved when estimated and perceived activation co-occur in time.", "CHI18_paper99-Figure3-1.png": "Figure 3: Left: User reaching to select target cube, with inset real-world environment. Right: Alignment cube.", "CHI18_paper76-Figure3-1.png": "Figure 3. While telling researchers a story about her photos, P12 zoomed into the right photo in this collage to emphasize the butterfly on her head.", "CHI18_paper76-Figure2-1.png": "Figure 2. P10 used her smartphone\u2019s built-in Magnification accessibility feature to view this photo on Instagram. When finished, she did a three-finger tap to zoom out (left). This gesture was interpreted as a double tap, which liked the photo (middle). In frustration, she then zoomed out and un-liked the photo (right).", "CHI18_paper76-Figure1-1.png": "Figure 1. A teen with low vision (P7) holds her phone close to her face to more easily view the screen.", "CSCW18_paper185-Figure4-1.png": "Fig. 4. Emma scrolling to the next clue while Olivia moves the big picture to the new clue location. I-Spy images courtesy of Maria Neradova.", "CSCW18_paper185-Figure6-1.png": "Fig. 6. Example of Ava nudging Sophia\u2019s hand out of the way of the big picture.", "CSCW18_paper185-Figure5-1.png": "Fig. 5. Example of Ben claiming the current clue image while also interacting with the big picture. I-Spy images courtesy of Maria Neradova.", "CHI18_paper288-Figure1-1.png": "Figure 1. The experimental setup (a) and a close-up view of the grid interface (b) in the study.", "CHI18_paper477-Figure4-1.png": "Figure 4: Example of text selection using NORMAL. The user performs a", "CHI18_paper477-Figure3-1.png": "Figure 3: Example of text selection using FORCESELECT HIGH-", "CHI18_paper477-Figure1-1.png": "Figure 1: Close-up of the \u201cmode gauge\u201d. Also, notice in the background", "CHI18_paper477-Figure2-1.png": "Figure 2: Example of text selection using FORCESELECT. The user per-", "CHI18_paper162-Figure11-1.png": "Figure 11. Embedding scraps cut from a silicone oven mitt into a pair of tongs makes it possible to be used with hot objects.", "CHI18_paper162-Figure6-1.png": "Figure 6. Embedding sponge softens and tightens the grip of this printed wrench. Medley allows users to draw a custom-shape sponge (DOW=3) to replace part of the original wrench handle.", "CHI18_paper359-Figure2-1.png": "Figure 2. The VR view of the 360\u00b0 live-video feed, as it was presented to the viewer through the smartphone-VR setup.", "Ubicomp18_paper161-Figure2-1.png": "Fig. 2. Objects used in Study 1. In the basic group, we selected six everyday objects in accordance with the Schlesinger taxonomy [37]. In the size group (control weight), there are four levels in size for each shape and all objects have the same weight of 100g. In the weight group (control size), objects sharing the same shape have the same size while there are four levels in weight. The objects in the latter two groups were 3D printed. Weights were controlled by adding iron sand inside.", "Ubicomp18_paper161-Figure9-1.png": "Fig. 9. Examples of actual grips (a, c) and the corresponding simulated grips (b, d).", "CHI18_paper436-Figure2-1.png": "Figure 2. Three different trajectories chosen for evaluation: (A) Rectangular trajectory, (B) Circular trajectory, (C) Sinusoidal trajectory. The size of the moving object in the experiment was 10 pixels.", "CHI18_paper436-Figure6-1.png": "Figure 6. User working in an air control tower. Moving dots representing airplanes on the screen can cause smooth pursuit eye movements. (A) The system detects high cognitive workload from the user and dispatches some observation tasks to a colleague. (B) Alleviated cognitive workload measured after user interface adaption.", "CHI18_paper436-Figure7-1.png": "Figure 7. User playing a quiz game on a public display. (A) The system infers that the question is inducing high cognitive workload. The system is, therefore, observing if this behavior persists. (B) The system provides a hint to avoid frustration.", "Ubicomp18_paper198-Figure3-1.png": "Fig. 3. The final design of the HeadGesture set for the nine commands. The movement of head is indicated by the arrows. \"2\u00d7\" represents the repeating of the action for twice. \"1s\" is an illustration for a dwell.", "CHI18_paper123-Figure7-1.png": "Figure 7: Selecting shapes: (a) Select Tool selects entire shapes and collections of shapes. (b) Direct Select Tool selects anchor points and line segments of shapes.", "CHI18_paper123-Figure8-1.png": "Figure 8: Collections with layouts: (a) Repeat Grid. (b) Partition Stack. (c) Partition Stacks Nested in a Repeat Grid", "CHI18_paper123-Figure9-1.png": "Figure 9: Peers highlighted on selection: (a) Peer shapes. (b) Peer anchor points. (c) Peer line segments. (d) Peer shapes in free-form layout.", "CHI18_paper380-Figure2-1.png": "Figure 2: Modes of interaction with Tangible Landscape (from left to right): Sculpting topography with hands; Sculpting with tools; Placing markers to establish way-points; Drawing walking routes; Establishing viewpoints; Patch placement for planting vegetation.", "CHI18_paper380-Figure5-1.png": "Figure 5: Interaction, feedback, example solutions for tangible teaching lessons, and illustrations of prototypical task interactions.", "CHI18_paper95-Figure3-1.png": "Figure 3. Top and side view of LumiWatch, with illustration of projector\u2019s field of view. Top: tangential field of view. Bottom: axial field of view.", "CHI18_paper95-Figure4-1.png": "Figure 4. Top of screen: raw data from our ten time-offlight sensors (red dots), with estimated touch point shown in green. Bottom of screen: resulting touch paths. On arm: current path is projected for debugging.", "CHI18_paper441-Figure21-1.png": "Figure 21. Designers must consider model orientation in the printer. Structure 1 is more fragile than 2: (1) horizontally", "UIST18_paper711-Figure2-1.png": "Figure 2. EarTouch device prototype. Left: targets used in the study shown mapped on the ear. Right: tracking camera mounted to the HMD.", "CHI18_paper142-Figure1-1.png": "Figure 1. Our framework enables users to both pan & zoom the context view and to create independent focus views, either DragMags or lenses.", "CHI18_paper142-Figure2-1.png": "Figure 2. Navigation actions and associated candidate gestures.", "CHI18_paper284-Figure6-1.png": "Figure 6. Children performing \"Throw\" and \"Catch\" activities with the Nodes.", "CHI18_paper284-Figure7-1.png": "Figure 7. The Scratch-based research platform used to select an animation feedback in one of the Nodes devices.", "UIST18_paper87-Figure1-1.png": "Figure 1. GridDrones system with an array of self-levitating physical voxels represented by small quadcopters.", "UIST18_paper87-Figure11-1.png": "Figure 11. GridDrones interactive animation of the flight of a butterfly at the LEGO\u00ae World Expo 2018.", "UIST18_paper87-Figure10-1.png": "Figure 10. Child playing with Flying LEGO\u00ae using the embodied controller at the LEGO\u00ae World Expo 2018.", "UIST18_paper87-Figure8-1.png": "Figure 8. Creating a catenary archway with GridDrones. a) A flat grid of 2x7 drones forms the basis; b) The user uses a \u201cPoint\u201d gesture to ray cast and select two key stones in the center of the grid, and sets a 100% rigid topological grouping with the smartphone app. He then selects all drones, programming a catenary curvature relationship between them using the smartphone; c) The user moves the keystones upward. An inverse gravitational curve begins to develop; d) The resulting archway would be structurally sound, if physically built.", "UIST18_paper697-Figure3-1.png": "Figure 3. (a) Common places to deform the auricle; (b) Percentage breakdown of the proposed auricular gestures.", "CHI18_paper569-Figure17-1.png": "Figure 17. Finger sliding sensing demonstration", "CHI18_paper569-Figure15-1.png": "Figure 15. Capacitive sensor demonstration", "CHI18_paper569-Figure27-1.png": "Figure 27. Lampshade", "CHI18_paper210-Figure1-1.png": "Figure 1. A vision for bodily games and play.", "CHI18_paper210-Figure3-1.png": "Figure 3. Ava, the eBike.", "CHI18_paper210-Figure2-1.png": "Figure 2. Balance Ninja.", "CHI18_paper210-Figure4-1.png": "Figure 4. Life Tree including the HMD view.", "Ubicomp18_paper194-Figure1-1.png": "Fig. 1. Obstacle detection using smartphone.", "CHI18_paper178-Figure1-1.png": "Figure 1: FingerT9 uses thumb-to-finger interaction (a) on T9 keyboard layout mapped onto finger segments (b) for text entry on smartwatches.", "CHI18_paper251-Figure1-1.png": "Figure 1. Comparing a typical list of tabs (left) with Bento\u2019s search centered navigation from the same exploratory search task.", "CHI18_paper251-Figure2-1.png": "Figure 2. The different manipulations that can be applied to a search result", "CHI18_paper529-Figure1-1.png": "Figure 1. With PageFlip, a user selects a command (e.g. font size) by dragging the top-right corner, and adjusts its values (e.g., text size) by \u2018peeling\u2019 the corner of the page.", "CHI18_paper529-Figure8-1.png": "Figure 8. SwipeTap interface and workflow: (up) changing size; (below) selecting an icon.", "CHI18_paper529-Figure4-1.png": "Figure 4. Study 1 design: (a) a target is shown when a trial starts; (b) a user drags the target corner to start selection; (c) the target is successfully selected.", "CHI18_paper529-Figure3-1.png": "Figure 3. A participant in study 1.", "CHI18_paper529-Figure11-1.png": "Figure 11. A user (a) starts dragging the corner and holds; (b) continues to drag the corner after holding; (c) pushes back the corner after holding; (d) drags the corner without holding.", "CHI18_paper529-Figure12-1.png": "Figure 12. Message editing with PageFlip.", "CHI18_paper529-Figure14-1.png": "Figure 14. Create a textured letter with PageFlip.", "CHI18_paper529-Figure13-1.png": "Figure 13. App notification, preview and switch with PageFlip.", "CHI18_paper529-Figure6-1.png": "Figure 6. PageFlip interface and workflow: (up) select a letter; and (below) change color. Note that we implemented all six tasks, other pictures are omitted to save space.", "CHI18_paper529-Figure7-1.png": "Figure 7. Radial Menu interface and workflow: (up) select a number; and (below) change stroke weight.", "CHI18_paper179-Figure6-1.png": "Figure 6. Households interacting with Time-Turner", "UIST18_paper53-Figure6-1.png": "Figure 6: As a partner, MobiLimb can express behaviors and embody virtual agents. a) Cat with a tail, that reacts to users\u2019 actions. b) Hostile scorpion. c) Curious device. d) Assistive guide showing how to scroll on a page.", "UIST18_paper53-Figure7-1.png": "Figure 7: MobiLimb can serve as haptic interface and touch the user on a) the hand or c) the wrist. b) A human-like skin texture can cover the device. d) Physical text messages can be sent between users.", "UIST18_paper53-Figure2-1.png": "Figure 2: Design space of MobiLimb.", "UIST18_paper53-Figure5-1.png": "Figure 5: MobiLimb as a tool: a) Notifications display, b) 3D joint manipulation, c) Video preview, d) Improve grasping, e) Directional light, f) Self-actuated movement", "UIST18_paper511-Figure2-1.png": "Figure 2: (a) Our adaptation of the fairy tale \u2018Goldilocks and the Three Bears\u2019, in which Goldilocks maliciously enters the home of the three bears, eats their porridges, sits on their chairs and sleeps in their beds. (b) The user in our tracking space of 5m x 5m.", "UIST18_paper511-Figure12-1.png": "Figure 12: (a) Our first control condition implements a teleport functionality and displays chaperone bounds to keep users from leaving the tracking volume. (b) Our second control condition, scaled motion, changes the mapping of physical motion (solid line) to virtual motion (dashed line).", "UIST18_paper141-Figure10-1.png": "Figure 10: e-NABLE hand integrated with our 3D printed backscatter switch.", "UIST18_paper141-Figure14-1.png": "Figure 14: Application prototypes. (a) shows our pill bottle prototype and (b) shows our insulin pen case. For the latter, the user pushes the plunger to use the insulin pen, which in turn uses our ratchet mechanism to store the usage data.", "UIST18_paper45-Figure4-1.png": "Figure 4. (a) A user performs a task standing eleven meters apart from the wall. (b) A screenshot of the user\u2019s view with a wall plane, a source (red cube), and a target (green cube).", "CHI18_paper354-Figure17-1.png": "Figure 17. (a) Multi-player board games; (b) trading-desk.", "CHI18_paper354-Figure18-1.png": "Figure 18. Setup for visual analytics application.", "CHI18_paper354-Figure16-1.png": "Figure 16. Audio-track mixing setup: (a) control 7-channel level fader bank, (b) equalizer, and (c) metering.", "CHI18_paper246-Figure3-1.png": "Figure 3: Experiment setup. Left: participant\u2019s view, the yellow circle shows participants gaze. Right: nal set up with the tablet PC mounted on the right side of the participant.", "CHI18_paper378-Figure4-1.png": "Figure 4. Setup for the three input conditions: Skin, Touchpad, and Button. The input interface was always brought in front of the subject on a designated area (the black rectangle on marked on the table). A separate screen displayed the interface and the Libet Clock.", "CHI18_paper378-Figure3-1.png": "Figure 3. The protocol for the active conditions in the experiment. Baseline conditions omit either the action or the beep. The Libet Clock on the right as shown on a display in the experiment.", "CHI18_paper397-Figure5-1.png": "Figure 5. Y. has drawn a new bird-card and is trying it on the Birdhouse. She is a little disappointed that nothing happens when she waives the card in front of the sensors as she uses to do with the other ones.", "CHI18_paper544-Figure4-1.png": "Figure 4. 2D results of remapping a square to a circle and changing the orientation of a rectangle. (a) Euclidean norm of the gradient. (b) x displacement. (c) y displacement.", "CHI18_paper401-Figure7-1.png": "Figure 7. A snapshot of the VR simulated driving game", "CHI18_paper401-Figure4-1.png": "Figure 4. A snapshot of the VR shooting game", "CHI18_paper401-Figure3-1.png": "Figure 3. A snapshot of the virtual Tai Chi training studio", "CHI18_paper401-Figure8-1.png": "Figure 8. A snapshot of the VR tennis game", "CHI18_paper513-Figure9-1.png": "Figure 9: Two different ways for playing the Concentration Game", "CHI18_paper82-Figure7-1.png": "Figure 7. Sensor Sticker process: 1. The user cuts the sticker, 2. adheres it to fabric, and draws connecting traces with trace color pen, 3. captures the design and removes the sticker, 4. the system stitches the sensor.", "CHI18_paper82-Figure9-1.png": "Figure 9. Basic workflow: 1. Sketch a design on fabric, 2. adhere Circuitry Stickers, 3. draw the circuit and electrical connections, 4. capture a picture of the sketch, 5. insert the fabric into the embroidery machine for stitching, and 6. replace the stickers with hardware components.", "CHI18_paper82-Figure2-1.png": "Figure 2. Sketch&Stitch comprises a smartphone, a wireless button, a PC, and an embroidery machine. It runs two custom softwares for capturing and digitizing sketches and a proprietary embroidery tool for converting images to stitches. Users capture a design with the button, verify it on the PC screen and stitch it in the embroidery machine.", "CHI18_paper143-Figure7-1.png": "Figure 7. Users wear an Oculus DK2 head-mounted display which plays back 360-degree videos and manipulate a joystick to indicate areas with potential danger. Bottom left: users see images of the scene through a \u201cvirtual camera\u201d", "UIST18_paper839-Figure7-1.png": "Figure 7. The study environment in STUDY2.", "UIST18_paper839-Figure2-1.png": "Figure 2. The (a) 3D models and (b) the hardware prototype of RollingStone.", "UIST18_paper839-Figure3-1.png": "Figure 3. The study environment in STUDY1. (a) The participant holds RollingStone device and aligns it to (b) a semi-transparent hand to (c) experience the slip profile. In each adjustment session of the texture properties, the participant can (d) experience all slip profiles at once.", "UIST18_paper839-Figure8-1.png": "Figure 8. RollingStone Applications: (a) Decorating the Room, (b) Escaping from the Room, and (c) Ninja Survival.", "CHI18_paper339-Figure5-1.png": "Figure 5. Seven out of our 10 participants interacting with the probe. P2, PII and PIX use the slider. P3 and P5 are about the change the shape by clicking on the central button. P6 and P8 use the rotary knob.", "UIST18_paper213-Figure20-1.png": "Figure 20. Smartwatches could track additional health information, such as coughing (A), and recommend actions (B).", "CHI18_paper460-Figure1-1.png": "Figure 1. The visual effects present in both visual conditions (red hand represents the virtual one): a) Clipping: if an object cannot be moved, the virtual hand follows the tracked one and penetrates the object. b) when an object can be moved, there is no representation of physical resistance. c) using pseudo haptic feedback, the virtual hand does not penetrate the unmovable object, but the offset between tracker and virtual hand increases. d) An object communicates the resistance by no longer following the exact tracking position (the offset depends on the strength of physical resistance).", "CHI18_paper437-Figure3-1.png": "Figure 3. Digits \u20191\u2019 to \u201910\u2019 from American Sign Language", "Ubicomp18_paper176-Figure3-1.png": "Fig. 3. (a) H4.F pointing at H4.M\u2019s shower data, (b) vice versa.", "UIST18_paper321-Figure1-1.png": "Figure 1. Indutivo recognizes the tap of a conductive objects on a smartwatch, such as (a) a dime, or (b) finger. It can sense (c) the rotation of a bottle cap instrumented using copper tape, (d) hinge of a metal credit card, and (e) slide of the handle of a table knife.", "UIST18_paper321-Figure8-1.png": "Figure 8. The position of the reference footprint indicates the position of the object inside the sensor.", "UIST18_paper321-Figure2-1.png": "Figure 2. Indutivo interactions.", "UIST18_paper321-Figure12-1.png": "Figure 12. Study apparatus for hinge (left) and slide (right).", "UIST18_paper321-Figure15-1.png": "Figure 15. Indutivo demo applications: (a) video player, (b) aircraft game, (c) brick breaker game, (d) audio book app, (e) fitness app, (f) setting voice mode app.", "CHI18_paper564-Figure1-1.png": "Figure 1. (a) The system\u2019s interpretation for mapping on-skin input to off-skin display differs from (b) user\u2019s perception of it. Such inconsistent mapping between the touch location on the arm and a content point on a display hampers interaction.", "CHI18_paper564-Figure2-1.png": "Figure 2. The study setup: a) The layout of the cameras of an OptiTrack motion capture system to track the arm posture based on two green marker bodies attached to the wrist and to the elbow, and the fingertip location based on a yellow marker body attached to the nail. b) Visualizing touches on an arm model used in processing motion capture data. c) Conditions where the participant maps content from a desktop display, d) from a watch, and e) from an AR headset to the arm. The dot highlighted red is the point to be mapped in the current trial.", "CHI18_paper564-Figure6-1.png": "Figure 6. a) Scrolling the map to center New York by swiping horizontally on the arm. b) 1-to-1 mapping results to a downward drift on vertical direction on the display, and a transition too short on horizontal direction. c) using the models allows to map touch to the content on a display as users intended to.", "CHI18_paper564-Figure3-1.png": "Figure 3. Rigid bodies with markers attached to the wrist (a), elbow (b), and fingernail (c) enable tracking of both the arm and fingertip location and orientation in 3D. The arm-centric coordinate system (d) on an arm allows precise touch tracking. On the middle: The x-axis of the coordinate system follows the ulna (in red), and the y-axis travels around the arm surface. The arm model (e) is interpolated along the x-axis from a circle in the base at the elbow to an ellipse at the wrist. Touch coordinates are mapped using the distance of the fingertip to the centerline of the arm model, and by projecting its position on local ellipse coordinates (f). This allows expressing the touch coordinates on a normalized 2D surface (g).", "CHI18_paper358-Figure6-1.png": "Figure 6. Participants share captured laughter with loved ones. Top: Participants with the perfume bottle. Bottom: Participants with the jar.", "UIST18_paper675-Figure2-1.png": "Figure 2. OctoPocus with traditional video prototyping. The designers create a rough stop-motion movie with only four stages of the interface, resulting in a poor representation of the dynamic interaction.", "UIST18_paper675-Figure4-1.png": "Figure 4. The Canvas sketching interface: The final composition (left) and the interface (right) show the \u201cuser overlay\u201d. Both sides have a list of sketches and animation controls at the bottom. The in/out buttons make the selected sketches appear/disappear. The sliders control the stroke-start, now at 0%, and the stroke-end, now at 100%.", "CSCW18_paper151-Figure10-1.png": "Fig. 10. Hive proposals submitted to the Mozilla accessibility design drive. a) A browser feature that provides explanations for events without parallels to the non-digital world, e.g., loading times. b) Deaf culture uses sign language call signs to adapt for more complex tasks. Here, a person using ASL can use their friend call sign to compose emails. c) Browsers react when users are having trouble, for example, when a user tries to click on a button and misses, the button increases in size.", "CHI18_paper163-Figure1-1.png": "Figure 1. Participants setting off and exploring GeoCoin", "CHI18_paper374-Figure5-1.png": "Figure 5: Curved corrugation structure: (a) initial shape, (b) material expands as curvature increases, (c) the structure can be deformed inextensibly to a positive Gaussian curvature or (d) negative Gaussian curvature [164] (\u00a9 2009 image reproduced with permission from Elsevier).", "CHI18_paper374-Figure2-1.png": "Figure 2: (a) Anticlastic (saddle-shape) curvature (v > 0), (b) synclastic (dome-shape) curvature (v < 0) [201] (\u00a9 2014 image reproduced with permission from Elsevier).", "CHI18_paper374-Figure4-1.png": "Figure 4: Kirigami honeycomb changing shape in response to cable tension (arrows indicate pulling direction) [159] (\u00a9 2016 image licensed under CC BY 4.0).", "UIST18_paper65-Figure2-1.png": "Figure 2. MetaArms design approach: a closely situated anthropomorphic arms system driven by leg and feet motion, with haptic feedback loop.", "UIST18_paper65-Figure3-1.png": "Figure 3. Schematic design of the prototyped backpack arms system.", "CHI18_paper661-Figure1-1.png": "Figure 1. Entering a time on a smartphone. Left: Setting the hour with a picker requires significant screen space in height for dragging and spinning the wheel. Right: Selecting the hour by applying force on the hour digits of the underlying label fades in the Force Picker. Rolling the thumb to the left scrolls through the hours\u2014the harder the user presses, the faster. Rolling the thumb to the right scrolls in the opposite direction. Lifting the thumb off the screen sets the value, and the Force Picker disappears. Compared to dragging and spinning, the Force Picker is more compact, so that contextual information is never pushed off-screen.", "CSCW18_paper192-Figure8-1.png": "Figure 8: American parents sat back, away from the child, when playing the instructional game.", "CSCW18_paper192-Figure4-1.png": "Figure 4: Interrupting. The child is about to tap the screen; dad reaches out to stop the action and pulls his hand away.", "CSCW18_paper192-Figure5-1.png": "Figure 5: Leading. Parents use the child\u2019s hand as a tool to complete a task.", "CSCW18_paper192-Figure6-1.png": "Figure 6: Hijacking. Parents take over and dominate game play.", "CHI18_paper61-Figure1-1.png": "Figure 1. Left: Original Wire Costume (Oskar Schlemmer, Draht-Figur 1922). Photo \u00a9 Staatsgalerie Stuttgart. Right: Our version of the costume.", "CHI18_paper61-Figure9-1.png": "Figure 9. The final look of the costume", "CHI18_paper61-Figure10-1.png": "Figure 10. Wave effect in action", "CHI18_paper289-Figure1-1.png": "Figure 1. Dyads performed the first (A) and second (B) tasks in the faceto-face conditions. In virtual reality conditions, avatars appeared across the table from each other (C), but were actually positioned on opposite sides of the motion capture stage (D). In the embodVR condition, participants were able to see both avatars (E). In the no_embodVR condition, participants were unable to see their partner and could only see their hands in the second task, to assist with furniture manipulation (F).", "CHI18_paper266-Figure7-1.png": "Figure 7. Overview of the 2x2 factorial design for the study. Consists of 4 conditions: (A) Individual Mouse Condition. (B) Collaborative Mouse Condition. (C) Individual Tangibles Condition. (D) Collaborative Tangibles Condition.", "CHI18_paper98-Figure4-1.png": "Figure 4: 3 children using the interactive blood vessel.", "CHI18_paper98-Figure2-1.png": "Figure 2: Image of Sickle Cell Virtual Reality", "CHI18_paper20-Figure2-1.png": "Figure 2. (A) A low vision user using SteeringWheel with Surface Dial. (B) The Surface Dial and its gestures.", "CHI18_paper362-Figure4-1.png": "Figure 4. Trackers were checked strategically towards gaining insights on behaviors \u2013 such as right before starting, and after finishing walking a dog, to know how many steps were gained.", "CHI18_paper362-Figure3-1.png": "Figure 3. Users were nine times more likely to check their tracker during vigorous physical activity than when sedentary. Such engagements increased in frequency near goal completion and fueled users\u2019 motivation to meet their goals", "CHI18_paper362-Figure5-1.png": "Figure 5. Trackers were used to uncover variations in routine activities. Examples ranged from checking how many extra steps were gained after taking part in a longer-than-habitual workout (left), and during a detour to work (right).", "CHI18_paper362-Figure6-1.png": "Figure 6. Users often combined PA feedback with time checking to estimate the likelihood of meeting goals. While trackers merely provided descriptive data (i.e., how much was walked by a point in time), users desired normative data (i.e., is this good enough?).", "CHI18_paper362-Figure7-1.png": "Figure 7. Users maintained an awareness of their activity levels and formed micro-plans such as grabbing water while waiting for the printer, or walking 1000 steps in the next hour.", "CHI18_paper362-Figure1-1.png": "Figure 1. Fitbit Flex (left), Fitbit Charge HR (right)", "CHI18_paper362-Figure2-1.png": "Figure 2. Participants wore a camera during two days, providing insights into how tracker use unfolds in daily life", "UIST18_paper927-Figure5-1.png": "Figure 5. Boxing experience: a punch hit at the left and middle areas of the user face triggers the left and both motors respectively. (The mark on the cloth was masked for blind review.)", "UIST18_paper927-Figure6-1.png": "Figure 6. Diving experience: the user advances underwater with arm strokes of both hands, and turn left with right-arm strokes. (The mark on the cloth was masked for blind review.)", "UIST18_paper927-Figure7-1.png": "Figure 7. Attention Guidance: left or right-sided normal force on face guides users to search toward left or right respectively.", "CHI18_paper502-Figure7-1.png": "Figure 7. Keppi V3 worn as a necklace (left) and as a keychain (right)", "CHI18_paper502-Figure1-1.png": "Figure 1. Keppi Version 1 (left) and Version 2 (right)", "UIST18_paper347-Figure1-1.png": "Figure 1: Guided Finger-Aware Shortcuts detect when a special hand", "UIST18_paper347-Figure3-1.png": "Figure 3: FingerArc and FingerChord: (a) holding an action key with a special hand posture for a predesignated delay time reveals the shortcut interface; (b) selecting the primary command using the index finger with others tucked in (FingerArc) or the middle finger (FingerChord); (c-e) selecting other commands using the angle of the thumb (FingerArc) or pressing different key areas (FingerChord); (f) releasing the key maintaining a hand posture triggers the command (e.g. primary command); (g) revealing all the fingers while holding the key cancels the operation.", "CHI18_paper406-Figure4-1.png": "Figure 4. Declining an incoming call using SurfaceSliding. In a first step (a) the user grasps the phone. Then the moves the phone in the direction of the decline symbol in respect to the center of the phone (b). After the movement (c) the decline call action is triggered.", "CHI18_paper406-Figure3-1.png": "Figure 3. Declining an incoming call selection phase using the standard touch interface. In the first step (a) the user taps the centre icon and then (b) moves it over to the decline symbol, finally (c) the release of the finger will trigger the highlighted action.", "CHI18_paper46-Figure3-1.png": "Figure 3: a) Scaled down VR user\u2019s perspective seeing the AR user as a giant, b) VR user shrunk down interacting inside the miniature dome, c) VR user is a giant looking down at the AR reconstructed space, d) The real experimental space for the AR user, and e) its virtual reconstruction for the VR user", "CHI18_paper46-Figure4-1.png": "Figure 4. Baseline condition illustrating how the remote VR user\u2019s avatar look through a HoloLens. (a-d) Tea Party task (a) VR user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar\u2019s hand to a correct tea box with a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user looks at the VR user\u2019s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.", "CHI18_paper46-Figure2-1.png": "Figure 2: The AR user views the Mini-Me from two different perspectives showing Mini-Me consistently gazing and pointing at the same location: a) in front of the whiteboard, b) side view of the whiteboard, c) As the AR user gazes at the remote VR user\u2019s lifesize avatar, the Mini-Me moves toward this avatar and d) fuses with it and disappears, e) AR user can gaze at the Mini-Me and perform an air-tap to pin it in place or f) Tap again to unpin it from that location.", "CHI18_paper241-Figure4-1.png": "Figure 4: The focus maneuver temporarily provides the badminton app with control over most of the physical space. The other apps go into a defocus state.", "CHI18_paper241-Figure2-1.png": "Figure 2: Users are confined into tiles. The rotation maneuver allows apps to move their user to the adjacent tile. This way users do not feel confined.", "CHI18_paper241-Figure6-1.png": "Figure 6: The switch maneuver allows two apps to switch tiles.", "CHI18_paper241-Figure7-1.png": "Figure 7: (a) VirtualSpace handles configurations with fewer users. (b) If free space is available, apps can perform different maneuvers at the same time, until (c) all space is allocated, then maneuvers need syncing.", "CHI18_paper241-Figure8-1.png": "Figure 8: (a) Spatial probability distribution based on 3min of using the badminton app (axis lengths: 4m). (b) VirtualSpace places the first badminton user without rotational offset. (c) The second badminton user is placed at a 180-degree angle, minimizing the overlap in their probability distributions.", "UIST18_paper557-Figure1-1.png": "Figure 1: Picture of use cases of ULPM. The left shows the normal cases of using smartphones to take videos, write text messages and watch online video; The right side shows users do not need to turn on the screen for interaction by applying ULPM, which saves power consumption especially at critically low battery level.", "CHI18_paper11-Figure6-1.png": "Figure 6: A table (shown on screen). Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys, and (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the position of the cell and its content are read out aloud.", "CHI18_paper11-Figure7-1.png": "Figure 7: A user is searching a table (shown on screen) for the word \u2018Jill\u2019. Columns are mapped to the number row of the keyboard and rows to the leftmost column of keys. (1) By default the top left cell is selected. (2) The right hand presses the \u20182\u2019 key, selecting the second column (3) The left hand selects the next row (4) The left hand selects the third row. In each case, the number of occurrences of the search query in the respective column or row are read aloud. When the query is found, the position and content of the cell are read out aloud.", "CHI18_paper11-Figure1-1.png": "Figure 1: (left) AirBnb (https://www.airbnb.com/s/places), with the initial selection chosen by SPRITEs highlighted when the user presses the topmost key in the rightmost keyboard key. (middle) When the user presses \u2018\\\u2019 key, SPRITEs reads out \u201cmenu\u201d and double pressing \u2018\\\u2019 activates the menu on the numeric row. (right) Pressing the \u20181\u2019 key on the numeric row reads out the first element in the menu.", "CHI18_paper11-Figure5-1.png": "Figure 5: A menu bar (top of screen) is mapped onto the numeric row of keys. (left) The user selects the first menu (middle) The user selects the second menu (right) The user has opened a sub-menu, which is now mapped onto the next row.", "Ubicomp18_paper184-Figure4-1.png": "Fig. 4. Experimenter 1 (E1) driving the car, participant in the front passenger seat, and Experimenter 2 (E2) in the back seat managing the VR system.", "UIST18_paper853-Figure5-1.png": "Figure 5. Resolving conflicts with Parallel Objects. a) A user seeks to manipulate objects that are being manipulated, b) Creating Parallel Objects to enable parallel manipulation, c) Manipulating a subset of Parallel Objects, d) Unselected Parallel Objects fade away as no conflict has occurred.", "UIST18_paper853-Figure6-1.png": "Figure 6. Manipulating Parallel Containers. a) Parallel Containers, b) Switching parallel version, c) Highlighting all the versions, d) Hiding all the versions except the chosen one, e) Merging a parallel version into the chosen one.", "UIST18_paper853-Figure7-1.png": "Figure 7 Parallel Avatars a) Tapping on the shoulder, b) Pulling off and resizing a Parallel Avtar, c) Placing a Parallel Avatar, d) Teleporting to the Parallel Avatar to get the view.", "UIST18_paper853-Figure8-1.png": "Figure 8 Teleportation. a) A user teleports, b) leaving motion lines and two Parallel Avatars. c) The other user teleport to a Parallel Avatar d) to reuse the teleportation.", "UIST18_paper853-Figure9-1.png": "Figure 9 Dynamic Parallel Avatars. Avatars\u2019 poster and action are synchronized with respect to their scales.", "UIST18_paper853-Figure4-1.png": "Figure 4. Replay motion with Container. a) Selecting a moving car, b) the car moves in the scene. c) The other hand can spin the Container to go back and forth in time.", "UIST18_paper853-Figure3-1.png": "Figure 3. Interaction with Containers. a) Creating a Container, b) Selecting objects with a Container, c) Cutting / Deleting objects from the scene, d) Browsing previously created Containers e) Pasting contained objects into the scene, f) Grabbing the Container with another hand to manipulate the objects using it as proxy, g) Transforming the selected Objects, h) Viewing the surrounding space with Container, i) Including unselected objects in the surrounding space into the Container.", "CHI18_paper353-Figure4-1.png": "Figure 4. Example of a designer using ProtoAR\u2019s interactive capture tools to for physical-digital prototyping of a mobile AR furniture placement app.", "CHI18_paper579-Figure9-1.png": "Figure 9. AR cutting tool. User removes the spout by cutting all the supporting edges.", "CHI18_paper579-Figure13-1.png": "Figure 13. The sequence of interactions to create a cylindrical extrusion. a). User selects extrude from the marking menu. b). User selects circle. c). User selects \u201cdraw on plane.\u201d d) User draws a circle on the XZ-plane by specifying 2 points. e). User select curve. f). User selects \u201cdraw in 3D space.\u201d f.). Extrusion. g). User tapers terminal end using the joystick.", "CHI18_paper579-Figure4-1.png": "Figure 4. Creating a teapot with RoMA. a). User designs the teapot body. b). User creates the spout while the robotic arm prints the teapot body. c). Robotic arm retreats and digital geometry rotates as user turns platform. d). User designs the handle against the partially printed teapot body.", "CHI18_paper579-Figure8-1.png": "Figure 8. Fighter jet stand.", "CHI18_paper579-Figure7-1.png": "Figure 7. Adding a cape to a toy figure. a). User doodles to create a spline patch with the AR controller. b). Patch rendered in AR. c). User creates the cape in AR. d). Robot prints directly on the lion model. e). Printed result.", "UIST18_paper473-Figure4-1.png": "Figure 4. Information counter in a smart showroom. (a) Several RFIMatch modules are embedded in the objects and underneath the table. A visitor (b) hears a voice instruction by pointing at the info icon with the fingerstall, (c) places a cup on the info icon to see detailed information, and (d) places another cup on the checkout icon to checkout an order.", "UIST18_paper473-Figure3-1.png": "Figure 3. Display rack in a smart showroom. (a) An RFIMatch module is mounted behind each item. Several RFIMatch fingerstalls are provided with voice guidance in different languages. (b) A visitor wearing the German fingerstall hears the embedded audio message in German by touching the icon. (c) Two users wearing different fingerstalls greet each other in their own languages.", "CHI18_paper220-Figure2-1.png": "Figure 2. Interaction model for AlterWear displays. Users 1) pick a design using their Smartphone or other NFC-enabled device, 2) tag the device to their display to update it, and then 3) wear their new design without the need to recharge it, update it, or maintain it.", "CHI18_paper27-Figure2-1.png": "Figure 2. Editing operations and matching gestures. (a) Caret movement using horizontal directional gestures. (b) Text selection using caret movement and touching the screen with a second thumb. (c) After selection, users cycle through the clipboard operations using vertical directional gestures and keeping the other thumb on the screen.", "CHI18_paper298-Figure3-1.png": "Figure 3. Virtual and tangible version of the barrel. To place an attack, the barrel must be moved next to a hole in the player\u2019s own area. This is the action that the defending player needs to notice and react to, which was used to study players\u2019 awareness of each other\u2019s actions.", "CHI18_paper298-Figure1-1.png": "Figure 1. Four users playing the tangible version of our tabletop game. While playing Whac-A-Mole, each player also has to become aware of and defend attacks that other players trigger using their tangible, 3Dprinted barrels.", "UIST18_paper127-Figure16-1.png": "Figure 16. Working with a physical prototype that consists of four foamcore layers (a). The geometry of the model is continuously captured and can be rendered in Blender with the house model of part 1 (b).", "UIST18_paper127-Figure2-1.png": "Figure 2. Representative strategies of the blue and red groups: (a) Use an existing piece to design a new part or make measurements; (b) Cut a piece using a building block reference or cut with the help of a ruler; (c) Manipulate cut and assembled pieces to explore dimensions and symmetries, or perform stability tests.", "CHI18_paper89-Figure14-1.png": "Figure 14: Making the pendulum reach its two degrees of freedom requires two user interactions: (a) The first hit makes the prop swing back and forth and (b) the second hit allows it to orbit.", "CHI18_paper89-Figure13-1.png": "Figure 13: (a) Two Vive trackers allow the system to track the reconfigurable board in all its individual states. (b) The pendulum. (c) the state of the spherical pendulum is described by rotation angles on the pivot.", "CHI18_paper89-Figure3-1.png": "Figure 3: (a) The first room of our example experience requires users to (b) move a suitcase, lay it flat, and (c) step on to short circuit a cable. (d) This is supported by two physical props, i.e., a folded board and a pendulum.", "CHI18_paper89-Figure15-1.png": "Figure 15: iTurk\u2019s pendulum movement behind the scene.", "CHI18_paper89-Figure4-1.png": "Figure 4: The fuse room is designed to geometrically overlap with the first room. This allows the two physical props to be located in both rooms.", "CHI18_paper89-Figure5-1.png": "Figure 5: (a) The second room requires users to move and erect a fuse box, (b) open it, and (c) push the \u201con\u201d button. (d) This is supported by the same physical prop that served as suitcase in the previous room.", "CHI18_paper89-Figure6-1.png": "Figure 6: The reactor room also overlaps with the other rooms.", "CHI18_paper89-Figure7-1.png": "Figure 7: (a) The reactor requires users to press and hold two buttons on the railing, in order to reveal the plasma ball, which (b) users launch by hitting it towards the reactor. (c) The rector explosion causes shrapnel to fly towards the user. (d) The plasma ball and all shrapnel are rendered using the pendulum prop.", "CHI18_paper89-Figure16-1.png": "Figure 16: Participant during study", "CHI18_paper89-Figure8-1.png": "Figure 8: The remainder of the experience.", "CHI18_paper89-Figure10-1.png": "Figure 10: Our demo experience contains examples of (a) inanimate use, (b) animate prop, (c) keep prop animated, and (d) stop the prop.", "CHI18_paper89-Figure9-1.png": "Figure 9: Some of the uses of the foldable board in our demo experience.", "CHI18_paper89-Figure11-1.png": "Figure 11: A football experience by replacing the end effector of the pendulum with a punching bag,", "CHI18_paper236-Figure3-1.png": "Figure 3. (a) The first version of the probe is non interactive. Children used it to built tactile bracelets and generate novel scenarios for documenting and reflecting on field-trips (here, in a museum). (b) The second version is a functional bracelet recording and playing sound.", "CHI18_paper236-Figure2-1.png": "Figure 2. The second version of the probe is worn on the wrist. It consists of a strap covered with velcro on which an audio recorder/player.", "CHI18_paper31-Figure1-1.png": "Figure 1. Participant exploring the comfortable area of the thumb on a Nexus 6 in front of an OptiTrack motion capture system.", "UIST18_paper19-Figure10-1.png": "Figure 10. Illustration of path following task evaluation. Users follow 3 different virtual traces (a, c, d) in the AR scene (b).", "UIST18_paper19-Figure9-1.png": "Figure 9. Setup for view pointing task evaluation. User sits on a rolling chair points to different directions with visual cues.", "UIST18_paper19-Figure2-1.png": "Figure 2. Registration between two users with SynchronizAR.", "UIST18_paper19-Figure12-1.png": "Figure 12. SynchronizAR supports spontaneous collaboration, i.e., a new user (b) join an existing AR collaboration (a) instantly (c).", "UIST18_paper19-Figure15-1.png": "Figure 15. SynchronizAR being used for human-robot interactions(c). The robot mimics the user\u2019s movement (b). And they can access each other\u2019s views (a, d).", "UIST18_paper19-Figure6-1.png": "Figure 6. Technical evaluation setups.", "UIST18_paper737-Figure1-1.png": "Figure 1. Example of Magneto-Haptics. (A) Three cylindrical magnets provide a magnetic force to the cube magnet. (B) This feedback creates a sensation as if one is moving an object on a gradient during active touch.", "UIST18_paper737-Figure6-1.png": "Figure 6. LEGO blocks embedded with various shapes of magnetohaptics (top) with corresponding haptic potential curves labeled on the side (bottom). Users can explore new combinations of haptic sensation.", "UIST18_paper737-Figure8-1.png": "Figure 8. Example designs of physical interfaces embedded with magnets: (a) slider, (b) dial, and (c) push button.", "UIST18_paper737-Figure7-1.png": "Figure 7. Example application of enhancing an interactive interface by magneto-haptics: (A and C through E) the iPad touch interface is enhanced by a magnet-embedded acrylic board. (B) The simulation result from the visualization tool.", "CHI18_paper164-Figure1-1.png": "Figure 1. The shoulder surfing views evaluated in this work (cropped images).", "UIST18_paper275-Figure4-1.png": "Figure 4. The Jumping sculpture (material: marble; rendered body parts: all). (a) First and final video frames. (b) Novel-view rendering. (c, d) The motion sculpture is inserted back into the original scene and to a synthetic scene, respectively.", "CHI18_paper426-Figure4-1.png": "Figure 4. JND study setup", "CHI18_paper426-Figure9-1.png": "Figure 9. Jetto games (a) Ice hockey; (b)Tower Defense; (c) Survival shooter.", "CHI18_paper349-Figure3-1.png": "Figure 3. Experiment setup.", "CHI18_paper125-Figure1-1.png": "Figure 1: Illustration of different space heating systems: (a) coal fire, (b) simple gas fire, (c) programmable thermostat and (d) smart thermostat", "CSCW18_paper140-Figure5-1.png": "Figure 5. F5 Javier (Father) viewing the choices Ana (daughter) selects during a V2 search task.", "CSCW18_paper140-Figure3-1.png": "Figure 3. F22 Mother pointing at paper on the wall to show her daughter the spelling for siding during a V2 search task.", "CSCW18_paper140-Figure2-1.png": "Figure 2. F4 Mother (left thumb) and daughter (right thumb) thumbing the phone at the same time during a V2 search task.", "CHI18_paper173-Figure3-1.png": "Figure 3. Illustration of the experimental setup: participants stood 2 meters away from a rear-projector screen. The input device was connected via USB to the computer in order to guarantee stable data transfer for smooth control.", "CHI18_paper173-Figure1-1.png": "Figure 1. Left: Pointer manipulation in the rotation techniques, W1RR and P1RR, is governed by the orientation changes of the control device. (a) For W1RR, the user must sweep the entire forearm to cause changes orientation of the smartwatch. (b) For P1RR, the user can use the wrist sweeps as a more subtle form of manipulation; Right: W1RR can calibrate the center frame of reference from inactive state (a) to raising the arm in front (b) to switch to manipulation mode. From this mode, a 45\u00ba wrist flick outwards and back can be used to trigger a click action (c).", "Ubicomp18_paper170-Figure5-1.png": "Fig. 5. FarmChat speech-based interaction", "CHI18_paper132-Figure4-1.png": "Figure 4. The experimental setup.", "CHI18_paper132-Figure1-1.png": "Figure 1. Examples of application scenarios of a multimodal tactile display: (a) Touch-state feedback, an individual usage example, (b) Highlighting, a simultaneous usage example, and (c) Delimiter, a sequential usage example.", "CHI18_paper470-Figure3-1.png": "Figure 3. Participant testing their e-textile button.", "CHI18_paper165-Figure1-1.png": "Figure 1. VR-OOM allows participants to experience the physical sensations of the real-world with the controlled virtual environments and events. Photo by Arjan Reef.", "CHI18_paper165-Figure3-1.png": "Figure 3. View from inside vehicle with participant on left, Driving Wizard on right, and Interaction Wizard in backseat, top. Participant\u2019s view with hand in foreground, left. Bird\u2019s eye view of virtual vehicle in virtual world, right.", "CHI18_paper372-Figure1-1.png": "Figure 1. In Yamove, two pairs of players perform improvised moves in synchrony, in a three-round dance battle.", "UIST18_paper521-Figure1-1.png": "Figure 1. Left: Users select a destination to teleport to using their controller with a raycast. Right; a portal (blue circle) that shows a preview of the location to be teleported to appears either to the left, right or center depending on the position of the user in the tracking space. Users must step into the portal to activate teleportation, which unobtrusively reorients and repositions them away from the tracking space boundary in order to increase available walking space.", "CHI18_paper237-Figure1-1.png": "Figure 1. A snapshot of our work. We formally define the problem of moving target selection (a). We propose a TernaryGaussian model to interpret endpoint distribution in moving target selection (b, c, d). We demonstrate two extensions of the model: 1) predicting pointing errors (e), and 2) assisting target selection (f).", "CHI18_paper237-Figure12-1.png": "Figure 12. The working process of BayesPointer. (a) The two moving targets with different speed and size; (b) the blue one is determined as the intended one with our BayesPointer.", "UIST18_paper937-Figure1-1.png": "Figure 1. Arbility allows blind end users to interact with web page elements that are otherwise inaccessible via a screen reader. Here a blind end user would like to place a food order a day in advance, but the restaurant\u2019s calendar web page is not accessible because it requires interacting with elements that are not keyboard-focusable and the interaction is listening for a mousedown event (rather than click), which not every screen reader application fres. Arbility allows the end user to hand off this targeted visual interaction task to a sighted crowd worker via the chat panel. The crowd worker interacts with the calendar page to select the end user\u2019s desired order date, and Arbility sends the worker\u2019s proposed action to the end user, who optionally accepts or rejects it. Throughout the task, the crowd worker interacts with a mirrored version of the end user\u2019s web page.", "UIST18_paper825-Figure1-1.png": "Figure 1: Varying pen grip posture to change input modes: (a) resting side of palm writes; (b) resting heel of palm highlights; (c) contacting extended pinky erases; (d) touch index beside pen for gesture commands. Pink and blue regions show where palm and fingers contact the surface.", "UIST18_paper825-Figure2-1.png": "Figure 2: Input space examples and notation: (a) side, heel, or floating palm contact; (c) touching pen grip fingers to the surface near the pen tip; (b) extending and touching non-grip fingers outside or inside the palm; (d) examples of complete postures. The dark pink regions show where the palm contacts the surface and cyan circles show where fingers contact the surface.", "UIST18_paper825-Figure9-1.png": "Figure 9: Application highlights: (a) switching from pencil to highlighter with Side and Heel postures (document annotation); (b) choosing pen colour from radial menu with Side-RingOut-PinkyOut (document annotation); (c) object creation menu with Side-PinkyOut (vector drawing); (d) using SidePinkyIn to use handwriting recognition for creating a text object (vector drawing); (e) gesture command mode using Float-Index (vector drawing).", "UIST18_paper649-Figure5-1.png": "Figure 5: Interactive steps for separating animation objects. (a) Step 1: The user circles the objects. (b) The exemplar objects are shown for review. (c) Step 2: Similar objects are found in the image. The user decides to refine the found objects. (d) The Smart object selection brush is used to mark the light pink objects. (e) All the objects are found. (f) Step 3: The background is cleared of all objects.", "UIST18_paper649-Figure6-1.png": "Figure 6: User interaction when animating. (a) The user draws a motion path. (b) A kinetic texture is automatically generated with an emitter (blue), where the density of the texture is set to approximate the original image. (c) The objects are automatically scaled over the path.", "CHI18_paper299-Figure2-1.png": "Figure 2. Five purposes of shape changes in end-user interactive devices.", "CHI18_paper626-Figure1-1.png": "Figure 1. Example of writing \u201ci am waiting for aiden\u201d. The user typed \u201cwaiting for\u201d as a multi-word input (left). Swiping right recognized the words. Uf a finger is down, the nearest key appears in a large font (middle). After typing the remaining letters, the user obtains \u201caiden\u201d (right).", "CHI18_paper558-Figure2-1.png": "Figure 2. The 6 posture angles that require personalization in a computer workspace, based on ergonomics standards and guidelines from Europe, Canada, Australia, Hong Kong, and the United States: 1) forward head tilt, 2) vertical viewing angle, 3) upper arm to vertical, 4) lower arm to horizontal, 5) thigh to horizontal, and 6) knee angle.", "CHI18_paper558-Figure6-1.png": "Figure 6. Two custom postures for anthropometric measurement. The white line on the user\u2019s thigh indicates the thickness extracted from the user\u2019s color map from the Kinect.", "CHI18_paper558-Figure4-1.png": "Figure 4. The 7 body dimensions measured using the Kinect sensor and used to calculate the personalized workspace settings, including: (a) hip to head, (b) torso, (c) upper arm, (d) forearm, (e) lower leg, (f) thigh thickness and (g) popliteal height.", "CHI18_paper558-Figure1-1.png": "Figure 1. ActiveErgo is the first active approach to improving ergonomics by providing automatic and personalized computer workspace adjustment. Our prototype uses a Microsoft Kinect sensor for skeletal tracking and uses robotic arms and motorized desk to provide automatic workspace adjustment.", "UIST18_paper901-Figure9-1.png": "Figure 9. Examples of differences in grasps across conditions. Left: without haptic feedback, participants penetrate virtual objects. Right: with haptic feedback, fingers conform to the object\u2019s shape (green book).", "CHI18_paper201-Figure3-1.png": "Figure 3 The table-non-table in households 4 and 5", "CHI18_paper201-Figure5-1.png": "Figure 5 The first day (left) and last day (right) of study #7", "CHI18_paper128-Figure2-1.png": "Figure 2. a) When an object is grabbed it is pulled down by the weight force (F(g)). The imaginary force (F(o)) is working against the weight force and increases with the offset between visual and tracked position. b) When an object is grabbed, the visual position first remains on the tracked position. While lifting, the visual position is shifted towards the object one\u2019s. c) The faster an object is moved, the more the visual position is shifted towards the tracked one.", "CHI18_paper647-Figure2-1.png": "Figure 2. This figure illustrates the experimental setup. (1) HTC Vive optical tracker (at 2.5m) and tracking space with 4\u00d7 4m2. (2) Virtual keyboard, stimulus and text input field. (3) Participant wearing HTC Vive and tracked hand-held controllers. (4) PC for experiment control and filling out questionnaires.", "Ubicomp18_paper185-Figure3-1.png": "Fig. 3. A wristworn IMU (circled in the photos) is not ideally positioned to monitor many exercises.", "CHI18_paper610-Figure1-1.png": "Figure 1. Example use case of the proposed model: Different levels of detail are shown depending on perceivable screen resolution based on the device\u2019s position and orientation in the field of view.", "CHI18_paper539-Figure1-1.png": "Figure 1. One player while interacting with the Pac-Many game.", "CHI18_paper539-Figure9-1.png": "Figure 9. On rare locations we observed helping and blocking behavior to gain benefits for the team or over the competitor.", "CHI18_paper407-Figure1-1.png": "Figure 1. APPropriate \u2013 a small storage device that contains the owner\u2019s digital possessions, allowing them to leave their phone behind, but pick up and use any other device at will, as if it were their own. Before leaving home, the user synchronises their phone to the APPropriate (part 1). After doing so, any public or borrowed devices can be appropriated and used at any time (2a\u20132c and 3a\u20133c). For example, in part 2, the user is watching a video from their media library on a public display in an autorickshaw. In part 3, the user has borrowed a phone to take a photo \u2013 the photo is saved to their APPropriate, and does not remain on the phone. Before they are able to use other devices, the user is prompted for a secret PIN that protects their data (parts 2a and 3a). Entering the correct PIN loads the user\u2019s media from the APPropriate, and displays it in the same manner as on their own phone (i.e., in individual apps on a virtual home screen, as in 2b and 3b). Later, back at home, updated media is synchronised back to the owner\u2019s phone (part 4).", "CHI18_paper407-Figure2-1.png": "Figure 2. Two of the simulated contexts as seen in the Nairobi study, for illustration (other contexts and locations are not shown). Left: travel (in this case, a matatu2), simulating a tablet attached to the seat in front of the user. Right: public space (a coffee shop), showing also the fitness sweatband used to represent a watch-based APPropriate (highlighted in red), and one of the projectors (blue) used to help simulate each context.", "UIST18_paper595-Figure5-1.png": "Figure 5: Gesture set for the glasses (top) and watch (bottom).", "UIST18_paper595-Figure15-1.png": "Figure 15: Demo applications: (a) a user swipes finger to browse websites (b) a user plays game on smartwatch.", "CHI18_paper515-Figure8-1.png": "Figure 8. The Letter Plates Montessori exercise provides feedback as words are formed and tactile letter-shapes are traced. Paper overlays on the Mat can provide additional context to the exercise.", "CHI18_paper515-Figure7-1.png": "Figure 7. The Tower Defence game combines tangible input with augmented reality holograms (left). Segmented E-Ink displays embedded in playing cards are driven by a power-harvesting I/O Tag (right).", "CHI18_paper450-Figure10-1.png": "Figure 10. Proposed applications.", "CHI18_paper446-Figure1-1.png": "Figure 1: (a) In this Mixed Reality game that uses a physical tray as prop, our mobile system renders shifts in the tray\u2019s center of gravity as the marble moves. (b) Our system creates the necessary forces by applying electrical muscle stimulation to users\u2019 triceps muscles. (c) Our approach leaves users\u2019 hands free at all times, allowing the user to interact with the tray.", "CHI18_paper446-Figure3-1.png": "Figure 3: The previous scene through the HoloLens.", "CHI18_paper446-Figure2-1.png": "Figure 2: Using a regular cup as an impromptu tangible brightness dial. (a) When she tries to increase the brightness past the allowed maximum, her hand hits a hard stop. (b) Our system renders this constraint by applying EMS to users\u2019 wrist muscles.", "CHI18_paper446-Figure5-1.png": "Figure 5: Turning on the virtual lamp. Here our EMS system renders the forces of the button\u2019s mechanism.", "CHI18_paper446-Figure6-1.png": "Figure 6: The user configures the intensity of the desired light bulb using a cup as a stand-in for a dial (passive prop). Using EMS force feedback, our system augments the tangible with constraints and detents.", "CHI18_paper446-Figure7-1.png": "Figure 7: The user manipulates two cups to control the light temperature and intensity simultaneously.", "CHI18_paper446-Figure8-1.png": "Figure 8: Here, our system enhances a fully functional thermostat with detents.", "CHI18_paper446-Figure10-1.png": "Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult\u2019s spring.", "CHI18_paper446-Figure18-1.png": "Figure 18: Participant balancing the marble (image from the study, with consent of the participant).", "CHI18_paper446-Figure11-1.png": "Figure 11: (a) At the start of the game the marble falls from the sky. (b) As it hits the tray, the EMS pulls the user\u2019s arms down quickly so as to represent the impact.", "CHI18_paper446-Figure12-1.png": "Figure 12: (a) These gooseneck lamps are repurposed as levers, with force feedback, allowing the user to input the secret combination to (b) unlock the door.", "CHI18_paper368-Figure7-1.png": "Figure 7. The relation of the \"Gaze Leading Time\", \"Gaze Leading Reaction Time\", and \"Target Identification Time\".", "UIST18_paper757-Figure1-1.png": "Figure 1. Schematic of power grip task and force exerted on a palm and fingers (\ud835\udc6d\ud835\udc91\ud835\udc82\ud835\udc8d\ud835\udc8e and \ud835\udc6d\ud835\udc87\ud835\udc8a\ud835\udc8f\ud835\udc88\ud835\udc86\ud835\udc93\ud835\udc94 indicate normal force on a palm and fingers).", "CHI18_paper411-Figure10-1.png": "Figure 10. Kinetic signage built by user study participants. The red line is the desired movement and the blue line is the movement that was built with Mechanism Perfboard.", "CHI18_paper411-Figure3-1.png": "Figure 3. Drawing the object on the AR marker board: (a) cat and (b) toy. The red line is the desired movement.", "CHI18_paper411-Figure4-1.png": "Figure 4. Generating a linkage mechanism through (a) placing the AR marker board on the test board and pressing the record button, (b) moving the AR marker board, and (c) pressing the record button again. Then, the system generates the linkage mechanism.", "CHI18_paper411-Figure5-1.png": "Figure 5. Modifying the generated mechanism by (a) moving the point and (b) changing the scale", "CHI18_paper411-Figure6-1.png": "Figure 6. Augmented guide for fabrication: (a) assembling linkage, inserting (b) gear and (c) shaft.", "CHI18_paper310-Figure1-1.png": "Figure 1. Five input modalities used in the study.", "Ubicomp18_paper201-Figure9-1.png": "Fig. 9. a) Pairing a stylus with a tablet; b) Pairing a Bluetooth Speaker with a TV; c) Pairing a printer with a smartphone (breathing light zoomed in on the top right corner); d) Pairing a smart ring with a remote display", "Ubicomp18_paper201-Figure1-1.png": "Fig. 1. Initiate pairing from a wireless mouse with a laptop using Tap-to-Pair. Top row: example scanners, and a pattern displayed on a laptop and themapped behaviors;Middle row: example advertisers, and user taps on amouse in synchronization with the pattern; Bottom row: The mouse\u2019s advertised wireless signal strength received by the laptop.", "Ubicomp18_paper201-Figure3-1.png": "Fig. 3. a) Placement of development board and LED light. b) Marked tapping area on the mouse and the keyboard (flipped). c) A participant taps on the mouse. d) A participant taps on the keyboard.", "CHI18_paper255-Figure5-1.png": "Figure 5. Screenshot (here: for portrait use) of the experience sampling method (ESM) overlay from our study. It asked participants to indicate their current posture by touching the corresponding pictogram, which also made the overlay disappear, revealing the keyboard underneath.", "CHI18_paper202-Figure1-1.png": "Figure 1. Four handgrips: a) Symmetric bimanual (B); b) Asymmetric bimanual thumb (AT); c) Single-handed (S); d)", "CHI18_paper202-Figure4-1.png": "Figure 4. Three different body postures in which different factors affect interaction such as a) Sitting at a table resting arms; b) Standing; c) Lying down with the back to the floor. (Arrows represent gravity and the circles are restrictions)", "CHI18_paper202-Figure5-1.png": "Figure 5. Example of video taken for all three body postures", "CHI18_paper644-Figure7-1.png": "Figure 7. Objects explored in the user evaluation. Left to right, top to bottom: RIFLE, BOW, TROMBONE, PISTOLS. Overlay added in post-production for visualization.", "CHI18_paper644-Figure9-1.png": "Figure 9. Summoning a vehicle using gestural input (left), then driving with a rigid steering wheel (right).", "CHI18_paper644-Figure11-1.png": "Figure 11. Using directionally-selective braking to grasp objects with two hands. As the controllers meet the object, inward motion is braked. Overlay created using post-processing.", "CHI18_paper428-Figure1-1.png": "Figure 1. Top-left: Vybe haptic gaming pad. Six numbered circles represent the positions of ERM motors. Bottom-left: Coordinate frame of a motion chair. Top-middle: Derivative-based substitution algorithm. When the angular velocity of roll is positive, three ERM motors on the right side are turned on (highlighted in red) to represent angular velocity occurring in rotational motion. The other figures can be interpreted similarly. Bottom-middle: Position-based substitution algorithm. This algorithm represents the amounts of rotations and translation by location-based vibration strengths. Right: Participant sitting on the Vybe haptic pad while wearing an HMD.", "CHI18_paper613-Figure1-1.png": "Figure 1. BioFidget is a biofeedback system that integrates physiological sensing and an information display into a smart fidget spinner for respiration training. The user (a) activates it with finger flicking, (b) reads his or her stress-related heart rate information from the display, (c) repositions it and switches it to training mode and moves it to his or her mouth, and then (d) blows on it for breathing training by using the adaptive visual feedback tool.", "CHI18_paper613-Figure8-1.png": "Figure 8. Smartphone implementation. (a) The progress of inhalation. (b) The progress and the quality of exhalation. (c) Visualization of HRV (red) and respiration (blue) information. (d) Results of an adequate respiration training. (e) Results without respiration training.", "CHI18_paper613-Figure7-1.png": "Figure 7. Visual augmentation for respiration training. (a) The purewhite ring indicates inhalation. (b) The colorful ring indicates exhalation. (c) The color ring rotates with the fidget spinner. (d) The hue and its range change according to the revolution speed.", "CHI18_paper613-Figure3-1.png": "Figure 3. Alternative BioFidget designs. (a) Basic design. (b) BioFidget with an additional clip for PPG sensing stabilization. (c) Fan-shaped wing to react to respiration. (d) BioFidget with a handheld display for rich visual biofeedback.", "CHI18_paper613-Figure10-1.png": "Figure 10. Two modes of respiration training and the experimental apparatus. A secondary PPG sensor is attached to the user for validating the PPG data on the BioFidget prototype. (a) Flick: the user flicks the fidget while exhaling. (b) Blow: the user blows on the fidget by exhaling.", "CHI18_paper185-Figure11-1.png": "Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The corresponding strokes in 3D.", "CHI18_paper185-Figure9-1.png": "Figure 9. Creating a curved drawing canvas. (a) The user draws a few strokes using the 2D and/or 3D interface. (b) A surface patch is fit to these strokes.", "CHI18_paper185-Figure12-1.png": "Figure 12. Workspace scaling tool for drawing a car. The user uses a large workspace to draw the shape of the car in a small, comfortable, scale (left). She then defines a small workspace centered on a headlight (right). This \u201czoom in\u201d effect allows her to capture the details of the headlight\u2019s shape, define a new canvas on it, and draw highlights.", "CHI18_paper185-Figure6-1.png": "Figure 6. Setup: the user puts on the HoloLens and draws with a motiontracked stylus, on a tablet (left), or mid-air (right) using a mouse affixed to the back of the tablet.", "CHI18_paper185-Figure7-1.png": "Figure 7. Strokes drawn using the 2D tablet are projected onto drawing canvases. (a) A planar drawing canvas is a rectangle with the same aspect ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users can draw closed curves on canvases to define (d) solid surfaces which also lend occlusion, lighting, and shadows to the design.", "CHI18_paper238-Figure3-1.png": "Figure 3. Unconstrained hand motion in free space leaves (a) a trail of polymer particles in the air that (b) decays over time. (c) When the hand repeatedly passes over a region, the quantity of polymer particles exceeds a certain threshold and is thus solidified (blue). (d) As time passes, the solid portion stays constant, whereas the remaining particles continue to decay. When the hand passes near a previously solidified polymer, some parts of the solidified polymer are melted back into polymer particles for resolidifying. (e) After an extended period of time, all of the polymer particles decay away, leaving only the solidified polymers.", "CHI18_paper238-Figure7-1.png": "Figure 7. When the user\u2019s hand revisits the workplace, (a) concentric grey disks that indicate the center of the hand are displayed, and (b) when the hand approaches the 3D shape and makes contact, green responsive spangles are displayed at the contact points. Further penetration is visualized in yellow to red. (c) The user can utilize this sense of spatial relationship to iteratively add new scaffolds. (The user\u2019s hand\u2014marked as a red dotted outline above\u2014is not visualized in the system.)", "CHI18_paper541-Figure1-1.png": "Figure 1. We present alternative timing methods through audio (left) and haptic (right) cues to measure SoA.", "CHI18_paper541-Figure3-1.png": "Figure 3. (A) The Intentional Binding (IB) effect. (B) IB conditions and measurement blocks. In baseline conditions, only one vent occurs either action or outcome. In active conditions both action and outcome occur. (C) Formulas to calculate IB relative to single-event judgment errors.", "CHI18_paper541-Figure8-1.png": "Figure 8. IB task procedure of Study 2 (*not done in baseline outcome blocks, ** not done in baseline action blocks).", "CHI18_paper541-Figure9-1.png": "Figure 9. Experimental tasks for the two timing methods: Libet Clock (left) and Haptic Clock (right).", "CHI18_paper541-Figure5-1.png": "Figure 5. Experimental tasks for the three timing methods: Libet clock (left), Visual Alphabet (middle) and Audio Alphabet (right).", "CHI18_paper516-Figure1-1.png": "Figure 1. Study participant P8 \u2014 who is congenitally blind \u2014 playing our racing game prototype using the racing auditory display (RAD). The RAD outputs spatialized sound and works with a standard pair of headphones. Using the RAD, players who are blind can play the same types of racing games that sighted players can play with an efficiency and sense of control that are similar to what sighted players have. Our supplemental video shows P8 using the RAD with the RAD\u2019s audio included.", "CHI18_paper87-Figure1-1.png": "Figure 1: The recall-based GUA scheme used in our study resembled Windows 8TM Picture Gesture Authentication. The users could create their password by making three of the following types of gestures: taps, lines, and circles.", "CHI18_paper629-Figure1-1.png": "Figure 1. A visually impaired student exploring the multisensory map. The system augments a tactile map with projection and audio output.", "CHI18_paper629-Figure6-1.png": "Figure 6. Student using our prototype during exploration mode: tactile map combined with projection and audio output.", "CHI18_paper629-Figure7-1.png": "Figure 7. Construction Mode: building a map using Wikki Stix and magnets combined with projection and audio feedback.", "CHI18_paper445-Figure4-1.png": "Figure 4: Example of StEM use: (a) the user drags and drops actions onto the timeline to construct the sequence of interaction of the current application screen; (b) the corresponding sequence of operators is automatically updated; (c) the user specifies the parameters of an action by manipulating its canvas representation (eg. position, size, ...); (d) after querying the database, the prediction times are displayed as stacked bars (each colour represents an operator).", "CHI18_paper150-Figure8-1.png": "Figure 8. (a) Line scaled up by a factor of 1.6x. (b) Line at 1x scale with no illusion. (c) The visual feedback in both scenarios.", "CHI18_paper150-Figure6-1.png": "Figure 6. (a) Redirection of the 45\u25e6 line onto a horizontal line. (b) Line placed at 45\u25e6 with no illusion. (c) The visual feedback in both scenarios.", "CHI18_paper150-Figure9-1.png": "Figure 9. Pin moving up and the corresponding virtual scene.", "CHI18_paper150-Figure23-1.png": "Figure 23. The virtual bouncing ball moves 3x faster than the maximum speed of the display. Vertical redirection is used to increase the perceived speed of the virtual hand.", "CHI18_paper150-Figure21-1.png": "Figure 21. Vertical and horizontal paths are rendered on the shape display in real-time as the user\u2019s hand traces the maze. Redirection is used to create the illusion that the hand is tracing paths at different angles.", "CHI18_paper150-Figure22-1.png": "Figure 22. The surfaces of the sculpted relief is scaled up onto the shape display by a factor of 1.8x to create a smoother and higher resolution representation. The original rendering of the piece is shown on the left.", "CHI18_paper150-Figure16-1.png": "Figure 16. The pin at maximum height with no illusion and all perceived speed increases evaluated at 1.5x, 2x, 2.5x, and 3x.", "CHI18_paper150-Figure13-1.png": "Figure 13. (a) Redirection of the 20\u25e6 line onto a horizontal line. (b) Line placed at 20\u25e6 with no illusion. (c) The visual feedback in both scenarios.", "CHI18_paper150-Figure15-1.png": "Figure 15. (a) Hemisphere scaled up by 1.8x. (b) Hemisphere at 1x scale with no illusion. (c) The visual feedback in both scenarios.", "CHI18_paper150-Figure4-1.png": "Figure 4. (a) User\u2019s hand moving from left to right on the shape display. (b) Virtual hand displacement scaled down on the shape display to increase display size.", "CHI18_paper150-Figure3-1.png": "Figure 3. (a) User\u2019s hand moving from left to right on the shape display. (b) Virtual hand displacement scaled up on the shape display to improve resolution.", "CHI18_paper150-Figure2-1.png": "Figure 2. (a) User\u2019s hand moving from left to right on the shape display. (b) Virtual hand redirected at an angle.", "CHI18_paper150-Figure20-1.png": "Figure 20. Pentagon maze rendered on the shape display.", "CHI18_paper150-Figure5-1.png": "Figure 5. The experimental setup consisting of the VR head-mounted display, motion capture system, and retro-reflective finger markers.", "CHI18_paper107-Figure2-1.png": "Figure 2. Output of our gesture recording software: 33% Complete, 67% Complete, Complete (f.l.t.r., animated GIF omitted); plus an example from the additional Video template set.", "CHI18_paper107-Figure5-1.png": "Figure 5. Gestures recorded using GestureWiz for (a) a maps app using multitouch zoom in/out gestures (based on [32, 22]); (b) a video player using Kinect hand gestures (based on [31, 20, 25]); and (c) cross-device gestures from Duet [5].", "CHI18_paper465-Figure7-1.png": "Figure 7. Instructor\u2019s 3D view (black star) and user\u2019s AR view (white star) of 5 modules. (a) Antitremor module, 6 blue virtual balls are placed in the pupil area, and turn to red when touched. Arrow: insertion point of the instrument. (b) Anterior chamber navigation module. Arrow: starting point in the pupil center. (c) Circular tracing module, green line represents the curve drawn by the user along the white reference circle. (d) Forceps training module: six blue balls are dragged to the small white point in the pupil center. Two white balls beside the blue ball represent the forceps tips. Arrow: wound touch warning indicators, the lower one is touched and turned to red. (e) Capsulorhexis module: white thin box represents the proximal end of the capsule flap. The box is dragged along the blue curve to the white point. (f) Capsulorhexis in the human eye.", "CHI18_paper465-Figure5-1.png": "Figure 5. (a) Spatula and (b) Capsule forceps used in cataract surgery. Arrow: reflective tracker. (c) Standard hand posture on the face model while holding forceps. (d) Dimensions of eye model, the soft part is made using EVA material. (e) The soft part allows the forceps to be tilted in the small artificial wound. (f) The elasticity will constrain the opening distance of forceps while tilting laterally.", "UIST18_paper261-Figure4-1.png": "Figure 4: Prototypes for automating stop-bars. Left: turning off a stop-bar by tapping its representation. Center: representing the ATCO to ensure a confirmation. Right: dropping an area to join two aircraft.", "CHI18_paper131-Figure1-1.png": "Figure 1. Handheld Pentelligence prototype while writing. The housing was removed for this picture to show the position of the inner hardware.", "CHI18_paper218-Figure3-1.png": "Figure 3. (a) The user makes a grab gesture on a window to (b) projectively bring it to the grabbed point.", "CHI18_paper218-Figure2-1.png": "Figure 2. (a) The user creates a big area cursor, (b) specifies a window in a cluttered situation by closing the fingers and making the cursor smaller, and (c) selects it by pinching.", "CHI18_paper218-Figure14-1.png": "Figure 14. User scenarios of Projective Windows in a (a) design studio, (b) study, (c) living room, and (d) VR scene.", "CHI18_paper218-Figure4-1.png": "Figure 4. (a) The user makes the window appear bigger by bringing it closer to the face, and (b) smaller by putting it away. The user can project a window (a, b) onto a vertical surface, or (c) make it stand on a horizontal surface.", "CHI18_paper218-Figure5-1.png": "Figure 5. (a) When the user grabs a window from a wall, moves it relative to the face, and releases it to another wall, (b) the zoom (ratio of the final to initial widths) is inversely proportional to d.", "CHI18_paper218-Figure6-1.png": "Figure 6. (a) Implementation hardware. (b) The hands, real and virtual objects in the user\u2019s view.", "CHI18_paper218-Figure7-1.png": "Figure 7. With Projective Windows (PW) using a hardware controller (PWC), (a-b) a finger pinch is substituted with (c-d) the pull of the trigger (highlighted green) on the hardware controller. Note the similarity between the bare hand gesture and the controller-based gesture.", "CHI18_paper218-Figure8-1.png": "Figure 8. The experiment was conducted with PW using a controller (PWC) and ray-casting (RC). Using PWC, the participant (a) first searched for the target (white), (b) grabbed the window (blue), (c) moved the window while adjusting its apparent size, and (d) released it onto the target when the apparent sizes matched. Using RC, the participant (e) first searched for the target (white), (f) dragged the window (blue) to the target, (g) placed it in the target, and (h) scaled it so that the absolute sizes matched.", "UIST18_paper365-Figure5-1.png": "Figure 5. Interactive example applications. a, b) Tactile augmented reality; c, d) Dynamic tactile landmarks for on-skin interactions; e) Tactile output in virtual reality; f) Private tactile notifications", "CHI18_paper189-Figure1-1.png": "Figure 1. RFIBricks is a reliable and easy-to-maintain interactive building block system based on UHF RFID sensing. (a) Interactive physical modeling. (b) Tabletop tangible gaming. (c) Tangible programming. (d) Modular input device.", "CHI18_paper189-Figure11-1.png": "Figure 11. Modular input device. (a) A user combines the desired input modality by stacking the widget block together. Subsequently, the actions of (b) pressing a push button, (c) turning a switch on or off, and (d) tilting the entire stack are detected correctly.", "CHI18_paper189-Figure10-1.png": "Figure 10. Tangible programming. A user (a) selects a desired functional block, rotates it to the orientation of the proper parameter, attaches the block, and (b) triggers the action by pressing the button.", "CHI18_paper189-Figure9-1.png": "Figure 9. Tower Defense. (a) Transparent tiles were fixed at the desired location on the tabletop. Weapon blocks were distributed to the players. A player (b) sets a weapon by placing a block, (c) upgrades a weapon by stacking, and (d) ignites a bomb by pressing the button.", "CHI18_paper189-Figure8-1.png": "Figure 8. Tangible Minecraft. A user (a) stages the characters by placing them on the plate, (b) builds the environment by stacking the blocks, and (c) places a character on top of the built mountain. (d) Results.", "CHI18_paper33-Figure5-1.png": "Figure 5. Detecting multi-touch input: a) minimum distance between two fingers which results in two distinct blobs, and the corresponding interpolated capacitive image and the extracted blobs; b) full finger placed on the sensor; c) the wearable hardware setup includes a Raspberry Pi Zero, the touch controller board and the Multi-Touch Skin Sensor.", "CHI18_paper234-Figure1-1.png": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "UIST18_paper963-Figure4-1.png": "Figure 4. Scraping data. Boxes on the left show snippets of the Rousillon control pane at various points during demonstration. Boxes on the right show snippets of the webpage with which the user is interacting, at the same points in time. a) The user has already added a movie title, PG rating, and star rating to the first row of data, shown in the first row preview at left. In the webpage, we see an actor\u2019s picture and link, but the user is not hovering over them. b) An in-site output preview. When the user hovers over an element, an inline preview shows a cell the user could add to the first row. Here the user hovers over the actor name. c) The user holds the ALT key and clicks on the actor name. The text from the preview cell is added to the first row of data (see preview at left).", "CHI18_paper25-Figure2-1.png": "Figure 2. Transient pan/zoom design supports interleaving atomic and transient actions. From a starting screen (a), a user performs a twofinger pan to center content (b); she then holds down third finger to enable transient mode, indicated by a shaded region under the pinning finger, and performs a transient zoom (c) before drawing (d). Lifting both fingers cancels transient mode and returns to the bookmarked resolution (e). She can then continue with other standard operations like panning (f).", "UIST18_paper867-Figure2-1.png": "Figure 2. The direct interaction technique enables the radiologists to interact with the environment by performing the selected gestures at the working zone.", "UIST18_paper867-Figure4-1.png": "Figure 4. Represents the visualization of the different indirect gestures for scrolling and windowing tasks.", "UIST18_paper867-Figure3-1.png": "Figure 3. The semi-structured interaction approach facilitates the windowing and/or scrolling action by entering the bar area with the digital representation of the user\u2019s hands and changing the shown slice by performing a movement (up or down / left or right)", "UIST18_paper867-Figure5-1.png": "Figure 5. Shows the representation of the hands in virtual reality. Also the described pinch gesture can be seen which enables the user to perform indirect windowing and scrolling.", "CHI18_paper367-Figure1-1.png": "Figure 1. Physical manipulations that can be applied to the Roly-Poly Mouse independently or in a combined way.", "CHI18_paper367-Figure7-1.png": "Figure 7. Overview of the RPM input apparatus (left): rolling RPM (centre) and tapping on the tablet to validate (right).", "UIST18_paper99-Figure16-1.png": "Figure 16. Direct interactive fabrication.", "UIST18_paper99-Figure15-1.png": "Figure 15. Airplane model rendered with dynamic physicalizable textbook.", "CHI18_paper223-Figure7-1.png": "Figure 7 Pen and Touch Interactions in DataInk", "CHI18_paper223-Figure2-1.png": "Figure 2 A storyboard illustrating data-oriented drawing with direct input following the principles of direct manipulation. a) Sketching visual designs using digital ink on a canvas enables one to experiment with various shapes. b) Selecting one of the shapes to specify a data-visual mapping automatically populates the canvas with all relevant data points. c) Sketching compound glyphs to represent additional data dimensions. d) Drawing a layout based on a data dimension to structure the data spatially. e) Redrawing the layout to map the data to a different data dimension. f) Redrawing a visual mark for a different data dimension.", "CHI18_paper223-Figure3-1.png": "Figure 3 The design decisions of DataInk. 1) Freeform sketching (1.a), direct manipulation (1.b, 1.c) to enable flexible authoring of visual designs, and quick access to both visual properties of glyphs and data attributes. 2) Composing glyphs specifying visual properties to data dimensions. 3) creating layouts and editing them as objects. 4) specifying visual-data mapping from visual variables or data dimensions. 5) supporting multiple workflows from the legend or the canvas.", "CHI18_paper223-Figure5-1.png": "Figure 5 Object-Oriented Drawing: sketching or reusing visuals and directly editing visual attributes on the canvas.", "CHI18_paper73-Figure5-1.png": "Figure 5. Groups 11 (top) and 10 (bottom) forming tableaux", "CHI18_paper73-Figure4-1.png": "Figure 4. (top) In TILT, SS points at a location where he needs LS to move his ROI, (bottom) In TOUCH, SS uses \u201cflexible ownership\u201d feature to drive LS\u2019s ROI to desired location.", "CHI18_paper73-Figure2-1.png": "Figure 2. (a) Using a direct TOUCH gesture on the table to control the ROI movement, (b) Using a TILT gesture on the tablet to control the ROI movement on the table.", "CHI18_paper65-Figure4-1.png": "Figure 4 \u2013 Possible ways of using textures in gestural interfaces, see also Video Figure 2.", "CHI18_paper65-Figure2-1.png": "Figure 2: a) Translation condition (vibration is mapped to displacement of the object), b) Rotation condition (vibration is mapped to change in orientation of the object) and c) Projection condition (vibration is mapped to change in position of a virtual point moving over a surface). Textures are green, motion is blue and pointer extensions for clarification are red.", "CHI18_paper409-Figure6-1.png": "Figure 6. This diagram demonstrates the two roles an ARcadia user might perform: A. designer and B. user. The designer is the one who constructs the physical prototype and programs the different mappings, the user is the one who ultimately interacts with the prototype after it is built.", "CHI18_paper188-Figure6-1.png": "Figure 6. Producing a circuit layer: (a) calibrating the laser cutter for cutting vinyl mask, (b) engraving and removing all circuit traces, (c) coat mask with Galinstan, (d) remove circuit mask, (e) blade coating of PDMS.", "CHI18_paper188-Figure8-1.png": "Figure 8. Stretchable input sensors compatible with our workflow: (a) a touch sensor, (b) a proximity sensor, (c) a slider, (d) a strain sensor, (e) a capacitive pressure sensor", "CHI18_paper188-Figure2-1.png": "Figure 2. Prototyping a basic uniform thin-film layer of silicone: (a) casting PDMS in a mold prototyped with silicone sealant, (b) blade coating both silicones at the desired thickness, (c) the final uniform thin-film sheet of silicone.", "CHI18_paper188-Figure4-1.png": "Figure 4. Fabricating the component layer: (a) engraving the positions of components, (b) populating the component layer and spray coating with silicone primer, (c) prototyping a mold with silicone sealant, (d) casting PDMS, (e) blade coating at the desired thickness.", "CHI18_paper188-Figure5-1.png": "Figure 5. Transferring the component layer to the second buildplate: (a) coating the component layer with an extra film of PDMS, (b) aligning the second buildplate and apply pressure to let all air escape, (c) after curing, release first buildplate.", "CHI18_paper219-Figure8-1.png": "Figure 8. Task evaluation setup with 8 IoT devices in an office.", "CHI18_paper219-Figure14-1.png": "Figure 14. Monitoring the IoT assets (a, b) and navigating the user towards the assets by visualizing the direction on the screen(c, d).", "CHI18_paper219-Figure2-1.png": "Figure 2. Scenariot localization principle.", "CHI18_paper219-Figure12-1.png": "Figure 12. Discoverable World. The digital representations of the discovered IoT devices are visualized within the AR scene with spatial PiPs.", "CHI18_paper219-Figure13-1.png": "Figure 13. Proximity based Control. While users move closer to the machine (a, b, c), the level of engagement is adjusted accordingly.", "Ubicomp18_paper164-Figure8-1.png": "Fig. 8. Demo applications. Left shows a user pointing to reply in a static angle menu in a message app using a flat touch. Center-left shows rotations of a side touch adjusting zoom in a map. Center-right shows selection of a brightness filter from a dynamic angle menu in photo app and right shows adjusting the brightness filter with a subsequent flat touch.", "Ubicomp18_paper164-Figure1-1.png": "Fig. 1. Sony Smartwatch 3 capturing raw touch input; data shown in callout at top-left of screen. The three images show the finger regions used in this work. Left image shows a touch with the flat finger region, center with the side and right with pair.", "Ubicomp18_paper164-Figure2-1.png": "Fig. 2. Left image shows eight angular targets and smartwatch used in the static study. Targets are subsequently referred to by the degree angles 30\u25e6 through 240\u25e6. Right image shows the smartwatch with the study interface worn by a user about to select the 120\u25e6 target with the side of their finger.", "CHI18_paper425-Figure1-1.png": "Figure 1. Measuring blood pressure using Seismo, a smartphone application that uses the built-in accelerometer and camera to calculate pulse transit time.", "CHI18_paper433-Figure1-1.png": "Figure 1. We conducted a user study comparing (a) projected in-situ navigation instructions to (b) traditional navigation instructions presented on a smartphone.", "CHI18_paper599-Figure8-1.png": "Figure 8. Application Examples: (a)Virtual Fireman Training System, (b) Virtual Tennis Simulation System.", "CHI18_paper258-Figure1-1.png": "Figure 1. Various vibrotactile phantom sensations.", "CHI18_paper258-Figure2-1.png": "Figure 2. Two configurations for 2D stationary sensations: PHONE (left) and RING (right). Vibration modules are marked by letters A\u2013D. In user studies, participants used a tablet placed on the table to enter commands and responses (bottom figures).", "CHI18_paper385-Figure1-1.png": "Figure 1. The BIGFile interface as the user navigates to \u201cDog\u201d in a file retrieval task. (a) and (c) show the adaptive part with two shortcuts, (b) and (d) the static part. In Step 1, the shortcuts do not help and the user selects \u201cAnimals\u201d in the static part, leading to Step 2 where the user directly selects \u201cDog\u201d in the first shortcut, saving one step.", "CHI18_paper90-Figure6-1.png": "Figure 6. A phase of loosely-coupled collaboration during the groupbased expert walkthrough. Domain experts navigated egocentrically to select their individual points of view.", "CHI18_paper90-Figure7-1.png": "Figure 7. A phase of tightly-coupled collaboration during the groupbased expert walkthrough. The domain experts selected a similar point of view and discussed the next analysis steps. Deictic gestures were used frequently during discussions.", "CHI18_paper151-Figure4-1.png": "Figure 4. The Light Writer", "CHI18_paper628-Figure6-1.png": "Figure 6. Highlighting interactions: Type 1 (left) & Type 2 (right)", "UIST18_paper581-Figure10-1.png": "Figure 10. Study 3 was conducted on a subway", "CHI18_paper297-Figure2-1.png": "Figure 2. The family toolkit. Clockwise from bottom-left: text module, video module, rotating knob, push button (in hand), parent dashboard.", "CHI18_paper297-Figure3-1.png": "Figure 3. The physical data loggers", "CHI18_paper69-Figure4-1.png": "Figure 4. The first method to access different block types: embedded typed blocks, accessed from a menu embedded within each block (e.g. \"Repeat 2/3 times\")", "CHI18_paper69-Figure5-1.png": "Figure 5. The second method to access different block types: audio-cue typed blocks, when a typed block in the toolbox and the blocks in the workspace that accept it play the same distinct audio cues.", "CHI18_paper69-Figure3-1.png": "Figure 3. Two methods to indicate the spatial structure of the code: (a) a spatial representation with nested statements placed vertically above inner blocks of enclosing statements, and (b) an audio representation with nesting communicated aurally with spearcons (shortened audio representations of words).", "CHI18_paper69-Figure2-1.png": "Figure 2. Two methods to move blocks: (a) audio-guided drag and drop, which speaks aloud the location of the block as it is dragged across the screen (gray box indicates audio output of program) and (b) location-first select, select, drop, where a location is selected via gray \u201cconnection blocks\u201d, then the toolbox of blocks that can be placed there appears.", "CHI18_paper86-Figure8-1.png": "Figure 8. (top) As the fnger approaches an obstacle (indicated here by the black region on the wheel), the dip strategy causes the wheel to lose contact with the fnger while an undesired feature remains under the fnger. (bottom) In the reversal strategy, the wheel begins to rotate in the opposite direction when an undesired feature is encountered.", "CHI18_paper86-Figure9-1.png": "Figure 9. (left) In the frst study, users slid their fnger horizontally across a surface. (right) In the second study, users traced a path on a surface. (right, inset) The six paths used in the second study", "CHI18_paper86-Figure12-1.png": "Figure 12. (left) A card table demo that highlights our ability to render different textures. The wheel used in this demo consists of two regions of soft felt, a hard plastic ridge, and a small section of paper. When the user touches an object in the scene, the appropriate texture is placed underneath the fngertip. (center) A painting and sculpting demo that highlights the ability to render shapes and sense the force applied to the wheel. The wheel used in this demo consists of a raised nub and a ridge to simulate holding tools. The user presses on the wheel to activate the tool. The model can be explored by touch. (right) A keyboard demo that highlights our ability to render edges and shapes. The wheel used for this demo consists of nine raised plastic regions with grooves in between. When a user approaches the edge of a key, the edge of a groove is placed under the fnger.", "CHI18_paper86-Figure13-1.png": "Figure 13. A demo with a DJ mixer board that highlights our ability to put interactive elements on the wheel. The wheel in this demo consists of several physical UI elements wired up to the device. When a user touches a virtual UI element, not only do they feel the shape of a similar physical element, but they can physical interact with the widget.", "CHI18_paper86-Figure6-1.png": "Figure 6. (left) When a user hovers over the blue surface, the rendering engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "CHI18_paper281-Figure1-1.png": "Figure 1. Remote Manipulator (ReMa) has two parts: it detects manipulations on an object (Left-yellow) using a set of sensors (Left-red), and then reproduces these with a proxy object (Right-yellow) using a Baxter robot arm (Right-red). ReMa allows shows the Manipulator Site collaborator (Right) the object with the same orientation as at the Tracking Site (Left). Collaborators can also use video chat (blue).", "CHI18_paper281-Figure8-1.png": "Figure 8: VC+ReMa \u2013 Group 1. Clara (TS) uses spatial hand gesture to describe the movement Lina (MS) should execute (annotated for clarity).", "UIST18_paper313-Figure4-1.png": "Figure 4. (a) The user puts index finger on the ring. (b) The user slides the ring and finger together.", "UIST18_paper485-Figure7-1.png": "Figure 7. I/O Braid USB-C headphones with embedded gesture recognition and audio feedback for music control, enabled by Nanoboard. The user starts playback by pinching, then rolls the I/O Braid to increase the volume.", "UIST18_paper485-Figure9-1.png": "Figure 9. I/O Braid\u2019s capability to sense touch and rotation input along the length of a headphone cord allows less precise input when on the go, for example, when jogging. The integrated visual feedback can be used to communicate phone connection or music status. This functionality could help signal social cues such as interruptibility, to onlookers. The photos show how flowing light can be used for directional feedback.", "UIST18_paper485-Figure3-1.png": "Figure 3. Rotating the I/O Braid: Relative capacitive signal strengths are shown in graph at bottom. As each pair of columns approaches a finger, its signal strength increases.", "UIST18_paper485-Figure5-1.png": "Figure 5. I/O Braid enables interaction techniques based on the capacitive sensing of proximity, contact area, contact time, roll, and pressure. For each event shown at the top, the bottom graph shows rotation through a center line plot, and nearing proximity by narrowing the height of the area plot. Additionally, a 3D representation of the cord is rendered at each event, with increasing line thickness for proximity, and varying thickness based on contact area.", "UIST18_paper485-Figure6-1.png": "Figure 6. Integrated visual feedback through braided fiber optic strands. The I/O Braid changes color on proximity and touch.", "CSCW18_paper159-Figure6-1.png": "Fig. 6. The at-home setup with a TV set facilitates the edutainment value of the task.", "CHI18_paper45-Figure1-1.png": "Figure 1. One participant is reading with RSVP while walking during the study. In the study, we investigated the effect of different text positions and presentation types on binocular see-through smart glasses.", "CHI18_paper45-Figure3-1.png": "Figure 3. Participants are reading text on a HoloLens while walking (left) or sitting (right).", "CHI18_paper184-Figure10-1.png": "Figure 10. Conditions of study (left: pen&paper, right: AdaM).", "CHI18_paper429-Figure1-1.png": "Figure 1: Potential interface for communicating awareness and intent to pedestrians (interface elements in green).", "UIST18_paper153-Figure3-1.png": "Figure 3. Drag-and-drop operation as a way to create a label from a recommendation. A\u00a9 Two recommendations provided by the system, B\u00a9 the card representing the newly created label using only the selected terms (blue).", "UIST18_paper779-Figure7-1.png": "Figure 7. Screenshots of our sample applications implemented on the InfiniTouch. Figure (a) showcases how a down-swipe with both index and middle finger selects all files in a file manager, Figure (b) demonstrates how the position of the middle finger can be used to switch between a pen and an eraser, and Figure (c) demonstrates an exemplary one-handed pinch gesture.", "UIST18_paper779-Figure3-1.png": "Figure 3. Five HAND GRIPs used in the study and adopted from previous work [47].", "CSCW18_paper118-Figure6-1.png": "Figure 6. Vignette 3- Instrumentation of the instrument system based on previous experiences, social", "CHI18_paper653-Figure5-1.png": "Figure 5. The RW and VR scenes used in our evaluation study while the green cursor is visible.", "CHI18_paper653-Figure3-1.png": "Figure 3. The VR scene we used in our study.", "CHI18_paper203-Figure3-1.png": "Figure 3. Touching of bronze heads & small statue replica.", "Ubicomp18_paper200-Figure6-1.png": "Fig. 6. From left: (a) counting playing cards (b) counting sand in hour glass matched by timer on the smartphone (c) identify Domino pieces (d) identify touch location on a numeric keypad sheet as input for calculator (e) tracking rotation of tagged game piece to control the clock hand and (f) tracking rotation of untagged smartphone using regression.", "Ubicomp18_paper200-Figure7-1.png": "Fig. 7. From left: (a) inside-out movement tracking on a desk made from compressed wood (b) ordering Lego blocks (c) distance tracking of three mugs made from different material (d) music player by slotting chips at different height (e) casino application with multiple players and (f) mathematics learning applications.", "UIST18_paper725-Figure1-1.png": "Figure 1. User traces the finger on a motion code while wearing an IMU ring to get encoded data. Figure shows our two proposed schemes: (left) Asterisk: User starts at center and traces outward & inward in the numbered order. (right) Obelisk: User traces the path. (Basis Angle: 45\u00b0)", "UIST18_paper725-Figure13-1.png": "Figure 13. Application Scenarios: (left) Using with gloves. (mid) Use on portable objects. (right) 3D Printed grooved Obelisk", "CHI18_paper199-Figure7-1.png": "Figure 7. Reactile uses a field of electro-magnetic coils fabricated with a standard PCB manufacturing. Each board has 16 x 40 coils and the final prototype uses five boards to cover 80 cm x 40 cm area with 3,200 coils. This board can actuate passive magnetic markers shown as red objects with 10 mm diameter.", "CHI18_paper199-Figure11-1.png": "Figure 11. We use computer vision to detect a rectangle workspace and positions of the markers (A). The system uses detected position information within 80 x 40 grid for path planning and controlling marker movements (B).", "CHI18_paper199-Figure12-1.png": "Figure 12. Reactile allows a user to draw a basic shape with a laser pointer. The system can converts freehand stroke into a beautified shape, and then determines the target positions.", "CHI18_paper199-Figure13-1.png": "Figure 13. Reactile lets a user to abstract attributes as variables through demonstration with blue constraint markers. When the system detects the demonstration, it updates the left panel to show a list of variables and current states.", "CHI18_paper199-Figure16-1.png": "Figure 16. The user can also create a mapping function between attributes and time-dependent variable for continuous animation.", "CHI18_paper199-Figure17-1.png": "Figure 17. An application example in data physicalization. A user defines x and y attributes, then binds them respectively to month and temperature data (A-B). The system propagates the value to each object (C).", "CHI18_paper199-Figure14-1.png": "Figure 14. Defining x, angle, and y variable by changing the location of a single point marker.", "CHI18_paper199-Figure15-1.png": "Figure 15. The user can create a mapping function with orange selection markers (e.g., rect.width = point.x - 5). Once the mapping function is created, the system can automatically propagate changes whenever the variable value is changed.", "CHI18_paper199-Figure3-1.png": "Figure 3. Create elements by drawing and construction. A programmer can create elements by arranging markers or drawing the desired shape.", "CHI18_paper199-Figure4-1.png": "Figure 4. Abstracting attributes by demonstration. An arrow\u2019s \u201cangle\u201d attribute is abstracted as a variable using two constraint markers.", "CHI18_paper199-Figure5-1.png": "Figure 5. Using constraint markers to specify different shape attributes: diagonal length (A), position (B), and angle (C).", "CHI18_paper199-Figure6-1.png": "Figure 6. Specifying behaviors by creating bindings between variables. Once a programmer connects two attributes by placing selection markers, then the system automatically binds them and propagates the change.", "UIST18_paper637-Figure6-1.png": "Figure 6. When the threshold for unlocking a tool is met, a notification is displayed in the corner of the screen (a). When the user unlocks the tool, a modal dialog with a short video demonstrating the tool is played (b).", "UIST18_paper637-Figure4-1.png": "Figure 4. The six stages of functionality in the Blocks-to-CAD system. (a) Minecraft-style block tools; (b) Tree-stamp tool; (c) 3D navigation widgets; (d) Shapes tool; (e) shape-resizing handles; (f) Workplane tool.", "CHI18_paper567-Figure1-1.png": "Figure 1. The experimental setup of Study 1.", "CHI18_paper19-Figure8-1.png": "Figure 8. The smartwatch allows (a) applying filters to data item sets; (b) deleting sets by wiping; and (c) displaying additional details-on-demand.", "CHI18_paper19-Figure7-1.png": "Figure 7. Previewing stored sets results in (a+b) inserting or highlighting the containing data points in the visualization, or (c) adapting the visualization to the respective configuration item (here: axis dimension).", "CHI18_paper19-Figure10-1.png": "Figure 10. (a) Changing the axis dimensions, and (b) remote control from a distance to set the focus onto a specific visualization view.", "CHI18_paper19-Figure9-1.png": "Figure 9. (a) Pulling, (b) previewing, and (c) pushing of sets.", "CHI18_paper19-Figure3-1.png": "Figure 3. (a) Cross-device interaction can happen with direct touch, in close proximity, or from intermediate or far distance; (b) the scope of user interactions is limited to the views in focus.", "CHI18_paper19-Figure2-1.png": "Figure 2. Primary smartwatch interactions: (a) swiping horizontally, i.e., along the arm axis for transferring content; (b) swiping vertically or (c) rotating a physical control for scrolling through stored content; and (d) moving the arm for pointing interaction.", "CHI18_paper530-Figure1-1.png": "Figure 1. Inpher makes it possible to define physical properties like bounciness of virtual objects through mimicking their physical behavior. (a) The user is equipped with virtual reality goggles and controllers for 3D input. (b) Users can grasp virtual objects and describe a physical motion to define the object\u2019s physical behavior (top). The free-flight physical motion of the object resembles the user\u2019s input curve (bottom).", "CHI18_paper160-Figure3-1.png": "Figure 3. Intermodulation II, 11/10/2016", "CHI18_paper160-Figure2-1.png": "Figure 2. Intermodulation I, 12/18/2015", "CHI18_paper160-Figure1-1.png": "Figure 1. Pablo Picasso\u2019s process in \u2018The Mystery of Picasso\u2019", "CHI18_paper160-Figure4-1.png": "Figure 4. The Electric Golem interacting with the artwork", "CHI18_paper160-Figure5-1.png": "Figure 5. The subsequent performance of Intermodulation I", "CHI18_paper160-Figure6-1.png": "Figure 6. A subsequent performance of Intermodulation II", "CHI18_paper249-Figure4-1.png": "Figure 4: Menu techniques: (a) M3 Gesture Menu, (b) Multi-Stroke Marking Menu, and (c) Linear Menu. Screenshots were taken when selecting terminal targets on the second-level menus with visual feedback.", "CHI18_paper249-Figure2-1.png": "Figure 2: Guided execution: (a) a start button appears at the bottom of the screen; (b) after pressing the button, a gesture instruction, 5\u2192 3\u2192 35, is displayed, and the activation button of the menu 5 is shown; (c) pressing the activation button 5 expands the menu, then sliding the finger to 3 displays its submenu; (d) further sliding to 35, then lifting the finger activates the item; (e) feedback is provided after the trial.", "UIST18_paper877-Figure3-1.png": "Figure 3: (a) The me handle allows users to move and (b) rotate their avatar. (c) The device renders walls and (d) impact using force feedback.", "UIST18_paper877-Figure8-1.png": "Figure 8: Walkthrough of our soccer game.", "UIST18_paper877-Figure9-1.png": "Figure 9: (a) After experimenting with various 3D printed shapes and sizes, (b) the \u201cflattened teardrop\u201d design performed best, because its orientation can be felt any time.", "UIST18_paper31-Figure3-1.png": "Figure 3. (A) Two Leap Motion sensors were attached to the HMD to support hand tracking, both in the line of sight and in front of the user\u2019s body. (B) The resulting tracking volumes are indicated with lines in light green (front-facing sensor) and blue (downward-facing sensor). (C) Haptic feedback was provided through a small vibration motor that was taped to the index finger. (D) A classic pursuit tracking task was used to evaluate task performance; participants followed a target (light circle) of 1 cm radius, which moved across a 30x30 cm panel with quasi-random motion.", "UIST18_paper31-Figure4-1.png": "Figure 4. The five conditions varied in panel position, and the type of VHS shift. From left to right: Ownershift condition (O) with gradually applied shift and panel located at the top; Instant shift condition (I) with instantly applied shift and top panel; Top condition (T) with collocated hands and panel at the top; Bottom condition (B) with collocated hands and panel at the bottom; Control condition (C) with quasi-random shifts and bottom panel. The virtual hand is overlaid and highlighted with a yellow outline.", "CHI18_paper635-Figure4-1.png": "Figure 4: Backward projection enables users to: (a) select any node corresponding to the two-dimensional projection of a data point x, (b) move the node arbitrarily in the plane, and (c) observe the chage \u2206x in the point\u2019s high-dimensional feature values.", "CHI18_paper635-Figure5-1.png": "Figure 5: Feasibility map. The feasibility map is constructed by sampling the projection plane through constrained backward projection and then verifying the existence of each solution (left). The darker area of the map, computed through interpolation, corresponds to the positions of the plane that would break the constraints for the specified data point (right).", "CHI18_paper320-Figure1-1.png": "Figure 1. Force Jacket - A: Appearance of Force Jacket; B: Individual airbag with force sensitive resistor; C: User study set-up.", "CHI18_paper320-Figure11-1.png": "Figure 11. Prototype VR Applications of the Force Jacket - A: Interactive snowball fight; B: Friendly snake crawling on user; C: Transformation into a muscular hero.", "UIST18_paper5-Figure9-1.png": "Figure 9. Natural grasp & Magic grasp.", "UIST18_paper5-Figure2-1.png": "Figure 2. PuPoP design overview (gray arrows indicate the palm direction).", "UIST18_paper5-Figure11-1.png": "Figure 11. Magic Brush Painting. a) Magic grasp by aiming at the tools. b) Paint with the magic brush. c) Erase with the magic eraser. d) Flattened PuPoP for an empty hand. e) Cylinder prop for the magic brush. f) Extended box prop for the magic eraser.", "UIST18_paper5-Figure10-1.png": "Figure 10. Quidditch Sports Training. a) Pick up a ball. b) Grasp a ball to throw. c) Catch the Golden Snitch. d) Stacked and flattened PuPoP. e) Large sphere for the balls. f) Small sphere for the Golden Snitch.", "UIST18_paper5-Figure8-1.png": "Figure 8. Prop extension. Parallel extension: a) Extension flattened state. b) Extension pops up. c) Holding in precision grasp. Tilt extension: d) Extension flattened state. e) Extension pops up. f) Holding in pen-like tripod grasp.", "UIST18_paper5-Figure7-1.png": "Figure 7. Prop stacking. Shape stacking of a cylinder and a sphere prop: a) Flattened state. b) Cylinder pops up. c) Sphere pops up. Size stacking of two boxes: d) Flattened state. e) Small box pops up. f) Large box pops up.", "CHI18_paper336-Figure13-1.png": "Figure 13: (a) Adding horizontal offsets to the hinge creates diagonal folds. (b) The connector width to the lower cell structure needs to be decreased gradually to connect to the cell top. (c) Composing multiple diagonal fold cells creates zigzag patterns that (d) can be varied in magnitude.", "CHI18_paper336-Figure4-1.png": "Figure 4: Users transition through the door handle\u2019s embedded textures by turning the knob. That winds up the strings on the inside, which compresses all cells and forms the textures.", "CHI18_paper336-Figure5-1.png": "Figure 5: (a) This shoe sole is flat by default. (b) The user transforms it into a treaded sole it by pulling a string, e.g., when it starts snowing. (c) Note that the sole is functional and robust enough to walk on.", "CHI18_paper336-Figure14-1.png": "Figure 14: (a) Leaving gaps between the members of the cell top (b) allows for creating spikey textures.", "CHI18_paper336-Figure15-1.png": "Figure 15: (a-b) Offsetting the hinge vertically and horizontally creates (c) diamond shaped textures, which flatten in the middle and thus create a different tactile feel.", "CHI18_paper336-Figure6-1.png": "Figure 6: (a) Designers fabricate one single bicycle grip that they (b) actuate continuously to different levels, (c) to feel the tactile qualities during rapid prototyping. (d) By inserting spacers after fabrication that (e) deactivates selected rows allows them to further investigate the grip\u2019s ergonomics.", "CHI18_paper224-Figure4-1.png": "Figure 4. Posting a digital sticky note to the whiteboard is a two-step process. First, the user touches the NoteCanvas at the location where the sticky note should appear, which enables the post button on the NoteCreator. Second, the user taps the post button on the NoteCreator, which will post the sticky note to the whiteboard and remove it from the NoteCreator (indicated with a semi-transparent sticky note).", "CHI18_paper603-Figure1-1.png": "Figure 1. (a) Lassoing icons. Icons whose entire area is inside of the loop are selected (highlighted in blue). The start and end points are automatically closed. The work presented here focuses on the straight stroking segments. (b-f) Steering through obstacles with various conditions.", "CHI18_paper603-Figure17-1.png": "Figure 17. This proposed illustration software automatically shows visual guidelines to fill up empty spaces during lassoing operations, which we predict to lead to better performance.", "CHI18_paper603-Figure6-1.png": "Figure 6. (a) Experimental setup and (b) visual stimuli. Our work focuses on the part of the stroke highlighted in purpose, which has length AN. This corresponds to a cyclic movement criterion, see the text.", "CHI18_paper5-Figure7-1.png": "Figure 7. Computationally corresponding action with electroluminescent light intensity.", "CHI18_paper5-Figure9-1.png": "Figure 9. Paper Torch by Nendo (http://www.nendo.jp/en/works/papertorch/).", "CHI18_paper5-Figure1-1.png": "Figure 1. Two different layering sequences were used for bottomemitting electroluminescent sample (left) and top-emitting electroluminescent sample (right).", "CHI18_paper5-Figure3-1.png": "Figure 3. Using different conductive materials in between the separated structure.", "CHI18_paper5-Figure2-1.png": "Figure 2. Using water as a replacement for the printed conductor in the bottom-emitting (left) and top-emitting (right) samples.", "CHI18_paper54-Figure6-1.png": "Figure 6. The SpaceFace application and its outside view (a), inside view (b) interaction and visualization concept (c) and physical interaction scenario (d).", "CHI18_paper54-Figure7-1.png": "Figure 7. The Conductor application showing its outside view (a), inside view (b), hand tracking region (c) and interaction scenario (d).", "CHI18_paper54-Figure10-1.png": "Figure 10. A variety of physical interaction poses participants used during the study emphasizing the vast possibilities of physical interaction arising from SpaceFace: (a) The Kraken: The Non-HMD User abused his power and wraps around the HMD User to restrict his motions. (b) The Leg-press: the HMD User utilizes his legs to either find or push the Non-HMD User away. (d) The Hedgehog: the HMD User rolls in like a hedgehog to hide from the attacks.", "CHI18_paper54-Figure4-1.png": "Figure 4. Interaction Gradient for FaceDisplay. Starting from the most engaged a: touch to b:gesture, c: external device and d: observing.", "CHI18_paper54-Figure5-1.png": "Figure 5. The FruitSlicer application with its outside view (a), inside view (b), interaction concepts (c) and visualization metaphor (d).", "CHI18_paper654-Figure5-1.png": "Figure 5. The avatar hand in Unity\u2019s VR scene when (a) grabbing, (b) touching, and (c) triggering. Note: the green rays emanating from the fingertips are hidden from the VR user and only visualize directions for determining collisions in VR.", "CHI18_paper654-Figure4-1.png": "Figure 4. CLAW operates in three haptic modes: (Top-left) When the user tucks away the thumb, off the thumb rest, the controller is in its default \u2018Touch\u2019 mode. (Top-right) The proximity sensor (red) detects when thumb and index finger align, and switches the controller into \u2018Grab\u2019 mode. (Middle) When the user has \u2018grabbed\u2019 a gun, the rotating arm (green) locks in place and mimics a gun, and the finger module (pink) acts as a trigger. (Bottom) The haptic mode is decided based on the user\u2019s thumb position and VR scenario.", "CHI18_paper300-Figure5-1.png": "Figure 5. Left: P8 (WO2) points toward overview device, other members shifted their attention to it. In NO groups pointing rarely led to shared attention (right): P29 (INT1) points toward her device; other members\u2019 focus stays on own devices.", "CHI18_paper300-Figure4-1.png": "Figure 4. Shared attention on the overview device (SSV; left) often led to active discussion (AD; right) as the device gave the group a common focus and starting point for a diccussion.", "CHI18_paper245-Figure6-1.png": "Figure 6. Left: Matching a rectangle target with orientation constraint (shown by vertical line). The target shape turns green to show that the matching requirements are met. Right: Finger-moving trial for Short Corner V in which the thumb is instructed to move right and outside the circle while the other fingers remain stable (purple dots indicate original positions)", "CHI18_paper245-Figure3-1.png": "Figure 3. Apparatus: (a) marker placement on the hand; (b) locations of participant, registration tripod, and large display (cameras and projector not represented).", "CHI18_paper245-Figure2-1.png": "Figure 2. Different fingertip raycast projections: (left) from DIP/IP joint to tip; (right) from MCP joint to tip.", "CHI18_paper245-Figure12-1.png": "Figure 12. Sample interactions demonstrating applications of multirays", "CHI18_paper496-Figure5-1.png": "Figure 5. Interaction to specify a load.", "CHI18_paper42-Figure1-1.png": "Figure 1. An illustrative scenario of eyes-free target acquisition in virtual reality. A user is doing design and he fetches tools in the interaction space around the body in an eyes-free manner. The FOV (field of view) size of his HMD is visualized (110 - 120 degrees).", "CHI18_paper42-Figure2-1.png": "Figure 2. The setting of the experiment and the task interface in Study1. A user moved the controller to acquire the virtual sphere in the 5 \u00d7 5 grid. Then he turned head to see which sphere he had acquired (the red one).", "CHI18_paper42-Figure9-1.png": "Figure 9. The concept of the experiment setting. Targets were located on both sides of users\u2019 body. A shortcut of the target layout was visualized in the front, as well as the character for the second task.", "CHI18_paper42-Figure5-1.png": "Figure 5. The concept of the experiment settings. The furniture indicates the virtual surroundings of the participants. The red spheres indicate the twelve directions that the participants rotate to, and the green sphere indicates the positioned target.", "CHI18_paper117-Figure13-1.png": "Figure 13. We created several illustrative applications.", "CHI18_paper547-Figure1-1.png": "Figure 1. Various representations of the \u2018single finger swipe\u2019 gesture in different academic papers [27,36,10,14,35,5,13,2].", "CHI18_paper547-Figure5-1.png": "Figure 5. Design themes extracted from user-elicited gesture representations.", "CHI18_paper547-Figure10-1.png": "Figure 10. A gesture representation from Microsoft\u2019s HoloLens \u201chow-to\u201d online gesture guide3 (left) and the principle-guided redesign (right).", "CHI18_paper547-Figure9-1.png": "Figure 9. A gesture representation from Apple\u2019s developer guidelines2 (left) and the principle-guided redesign (right).", "CHI18_paper547-Figure7-1.png": "Figure 7. Examples of mental model observations while participants created and enacted gesture representations.", "CHI18_paper547-Figure2-1.png": "Figure 2. Examples of taxonomy elements used in gesture representations. From top left to bottom right: (A) [5] Color, Multi-frame, 1st Person, Ghost, and Touchpoints. (B) [13] Upper-body, 3rd Person, Single Frame, 1-Sided Arrows, Dotted Lines, Finger Trail, Label and Physical Object. (C) [37] Upper-Body, 3rd Person, Single Frame, Other Motion Lines, Numbers, and Physical Object. (D) [30] Side Angle, 2-Sided Arrows, dotted Lines, Ghost, Axis, Virtual Object.", "CHI18_paper547-Figure8-1.png": "Figure 8. A gesture representation from an academic paper [36] (left) and the principle-guided redesign (right).", "CHI18_paper547-Figure3-1.png": "Figure 3. Our methodological process for eliciting gesture representations from users.", "CHI18_paper81-Figure1-1.png": "Figure 1. Pinpointing explores multimodal head and eye gaze selection for wearable AR a) Study layout of target markers, with feedback cues and HoloLens viewing field shown. b) Pinpointing techniques consist of a primary pointing motion plus secondary refinement. c) Refinement techniques: air-tap gesture, HoloLens clicker device, and head motion.", "CHI18_paper81-Figure10-1.png": "Figure 10. Compact fractal radial menus with deep structure in GazeBrowser. a) Example application controlling humidity and temperature sensors. b) HoloLens screenshot of menu used with device refinement. c) A close-up showing selection of very small radial menu items using Pinpointing. The selected item is the small yellow dot marked by the crosshair.", "CHI18_paper78-Figure2-1.png": "Figure 2: The menu interface in \"Destinations\" [2], where users first select \"Furniture\" in the \"Things\" menu (top left) and select a mug in the sub-menu (top right); the scene in \"RecRoom\" [6], where users access the position of the prop to pick up it (bottom).", "CHI18_paper78-Figure6-1.png": "Figure 6: The sensors that participants put on to record the hand gesture data, on finger joints, hand backs, arms and the head. They put on the sensors before the experiment with the help an experimenter.", "CHI18_paper443-Figure2-1.png": "Figure 2. Participant Playing the Game in Lab Setting with the Experimenter", "UIST18_paper247-Figure10-1.png": "Figure 10. We explore applications controlling three types of interactive features: (a) Haptic feedback behind an obstacle; (b) Levitation around the obstacle; (c) manipulation of non-solid objects, changing the fire\u2019s angle.", "CHI18_paper551-Figure3-1.png": "Figure 3: Children interacting with the interactive surface.", "CHI18_paper291-Figure4-1.png": "Figure 4. Bimanual scenarios for a mobile tabletop shape display.", "CHI18_paper291-Figure5-1.png": "Figure 5. Shape display self-actuation can be used to display spatial movement of objects. Here we illustrate the concept with a bouncing ball. Actuation of the pins renders the ball\u2019s vertical movement while actuation of the platform shows its projectile motion in space.", "CHI18_paper291-Figure3-1.png": "Figure 3. Example lateral UI elements for a passively mobile shape display: (a) slider, (b) joystick, and (c) knob", "CHI18_paper291-Figure2-1.png": "Figure 2. A mobile shape display can be moved over content using (a) the viewport itself or (b) objects or handles fixed relative to the viewport.", "CHI18_paper291-Figure8-1.png": "Figure 8. In a passive mobile display, rendered objects can be freely grasped, translated and rotated (left) spatial data such as terrain maps can be mapped to real-world spaces for physical exploration (right).", "CHI18_paper291-Figure6-1.png": "Figure 6. An active mobile shape display can be used to provide haptic feedback. Actuation of the pins provides vertical kinesthetic feedback (red arrows) while self-actuation of the platform provides horizontal kinesthetic feedback (blue arrows).", "CHI18_paper291-Figure7-1.png": "Figure 7. Multiple active tabletop displays can be used synchronously to simulate an infinite surface.", "CHI18_paper291-Figure9-1.png": "Figure 9. Virtual content such as a terrain map can be modified and physically explored real-time using physical proxies such as a wand (left). Two displays are used to physically render two different virtual houses (right).", "CHI18_paper638-Figure1-1.png": "Figure 1: In-house Macro Recorder.", "CHI18_paper638-Figure3-1.png": "Figure 3: In-house Proxy Mark Widget.", "CHI18_paper638-Figure2-1.png": "Figure 2: In-house Smart Assistant.", "CHI18_paper638-Figure5-1.png": "Figure 5: Interactions with data table (shown with cursor annotation).", "CHI18_paper287-Figure2-1.png": "Figure 2. The prestudy participants stood at a distance of 3.6 meters away from the projection. They were then shown one of the four representation types at a time, and asked to identify themselves as quickly as possible. Afterwards the participants were interviewed to uncover the strategies they employed to perform the task.", "UIST18_paper745-Figure8-1.png": "Figure 8: The resistive yarn can be processed using additive techniques, such as sewing (a), or constructive techniques, such as weaving (b).", "UIST18_paper745-Figure9-1.png": "Figure 9: A resistive yarn with removed coating (a), and a woven textile prototype connected to a flexible PCB (b).", "UIST18_paper745-Figure4-1.png": "Figure 4: First prototype with a basic yarn crossing connected to a microcontroller measuring the resistance.", "UIST18_paper745-Figure2-1.png": "Figure 2: The cross-section of two resistive-coated yarns, which form a resistor (a) that can measure applied force (b).", "UIST18_paper745-Figure11-1.png": "Figure 11: The machine-sewn sensor matrix acts as an interactive gesture pad. We used it to control the Spotify app on a smartphone.", "UIST18_paper745-Figure10-1.png": "Figure 10: Three hand-sewn pressure sensors on a couch are used to control different RGB lights.", "UIST18_paper745-Figure13-1.png": "Figure 13: Different embroidery stitches were augmented them with pressure-sensitive yarn to create control elements consisting of different shapes, sizes and patterns.", "CHI18_paper497-Figure6-1.png": "Figure 6. PolarTrack\u2019s tracking quality was evaluated with 1-finger, 1- hand, and 2-hands occlusion.", "CHI18_paper43-Figure3-1.png": "Figure 3. Participant shown with OptiTrack marker setup and engaging with the map application used in Experiment 1 and 2 (inset).", "CHI18_paper43-Figure1-1.png": "Figure 1. An illustrative example of a user interacting with a proximityaware mobile navigation application. Moving the phone closer changes the view within the application. At a far distance a simple arrow directs the user. As the user brings the phone closer, extra details are added.", "Ubicomp18_paper181-Figure1-1.png": "Fig. 1. FarSight system overview.", "UIST18_paper913-Figure5-1.png": "Figure 5: Apparatus for evaluating HydroRing. The user\u2019s view of the device is occluded, and they wear headphones to mask the noise of the pumps and valves.", "UIST18_paper913-Figure8-1.png": "Figure 8: Apparatus for testing tactile perception capability; a) the four devices used to produce the sensations of pressure, vibration, texture, and temperature; b) the participant assessing the vibration stimulus while wearing the HydroRing.", "UIST18_paper913-Figure11-1.png": "Figure 11: Example scenarios using mixed-reality haptics to add feedback to existing objects and devices. a) a user moves their finger over a wall, and feels vibration feedback as her hand passes over wiring hidden behind the wall. b) a user touches an interactive children\u2019s book, which provides thermal feedback corresponding to the on-screen content.", "UIST18_paper913-Figure12-1.png": "Figure 12: Example scenario of adding tactile feedback to an augmented-reality instruction manual. As the user selects items on the display and navigates the content, vibration and pressure provide real-time feedback on their actions (screens are simulated).", "UIST18_paper913-Figure10-1.png": "Figure 10: Sample scenarios where mixed-reality haptics could be used; a) while operating existing technology b) performing activities in wet or dirty environments c) performing activities that require manual dexterity.", "CHI18_paper593-Figure5-1.png": "Figure 5 Two museum visitors interact with the prototype", "CHI18_paper593-Figure4-1.png": "Figure 4. The last element of the Priming Phase of Framed Guessability is the Embodied Priming. In this picture, a participant re-enacts what she had listed on the worksheet for GYM in front of the screen.", "CHI18_paper593-Figure2-1.png": "Figure 2. Experimental setup for the Priming Phase of Framed", "CHI18_paper439-Figure6-1.png": "Figure 6. Experiment 2 setup. A user held the device in one hand, and typed on the invisible keyboard with her preferred posture. The spatially-adapted and the unadapted keyboards shared the same appearance.", "CHI18_paper340-Figure3-1.png": "Figure 3: Breathing actions and corresponding effects in the FPS game.", "CHI18_paper655-Figure2-1.png": "Figure 2. The prototype as it was used in the second study, with two brushless DC motors. Due to the changes the power supply needed to be increased up to 38.5 watt (22V). Video: https://vimeo.com/222267082", "CHI18_paper205-Figure14-1.png": "Figure 14: Data collection interface. Users are presented with text to write (A) in the scrollable collection canvas (B). A progress bar (C) informs the user on the status. A writer can save, reject or correct samples using the toolbox buttons (D).", "CHI18_paper4-Figure3-1.png": "Figure 3: Smart screens panels providing access to connected devices (H6).", "UIST18_paper499-Figure1-1.png": "Figure 1. ShareSpace toolkit presents a set of physical shield tools that allow users to add \u201cshields\u201d in the virtual environment. They help avoiding physical conflict and promote the shared use of the same physical space for heterogeneous activities of both HMD and external users.", "UIST18_paper499-Figure11-1.png": "Figure 11. (a) When a large region in the VR zone is used e.g., by an external table discussion activity, (b) the HMD user activates the AR function to understand the modification made by external activities and (c) redeem the space face-to-face. Note that the sub-figure (a)(c) are the AR view for explanation.", "UIST18_paper499-Figure10-1.png": "Figure 10. Safe zone is defined as a personal region located at the center of the maximum inscribed circle found in the available free space", "UIST18_paper499-Figure12-1.png": "Figure 12. Four study tasks in the Activity phase. Note that the experimenter is filled in gray in the figure.", "UIST18_paper499-Figure5-1.png": "Figure 5. (a) Connecting two edge shields with a proximity gesture forms (b)(c) a wall shield in the VR zone", "UIST18_paper499-Figure15-1.png": "Figure 15. Virtual wall shields made by edge shields allow to divide spaces for multiple HMD users gameplay.", "UIST18_paper499-Figure6-1.png": "Figure 6. Two setup examples of edge shield. (a) Three edge shields are arranged to model the space taken by a foldable table. (b) Two edge shields attach to the door edge and door frame can capture the required space for the opened and closed door.", "UIST18_paper499-Figure7-1.png": "Figure 7. Usage of virtual shields: (a) A normal-sized circle shield protects a single user, (b) while a double-sized one protects a user group. (c) Wall shields formed by two edge shields define large space for a user group or (d) table discussion activity. (e)(f) Aggressive shields inform immediate needs of space such as acquiring space for furniture usage.", "UIST18_paper499-Figure9-1.png": "Figure 9. (a) Once the HMD user approaches the aggressive shield installed at the door, (b) the system pauses the game with a grayed out viewport, and (c) lead the user to the safe zone indicated by a green halo. The game resumes with a button on the controller.", "CHI18_paper360-Figure1-1.png": "Figure 1. Using the palm as an additional input modality on smartphones. Figure (a) shows a palm touch when holding the device one-handed, Figure (b) and (c) show palm touches for two-handed interaction.", "CHI18_paper360-Figure5-1.png": "Figure 5. Screenshots of (a) the notification drawer in Part 3; (b) Fitts\u2019 Law task as a distraction task in Part 3; and (c) a prompt to place the palm on the touchscreen until the progressbar on top is full (Part 4).", "CHI18_paper360-Figure3-1.png": "Figure 3. All six tasks performed by participants in the data collection study.", "CHI18_paper419-Figure1-1.png": "Figure 1. Consistent, user-defined gesture sets for controlling reading flow via RSVP on three device types: phone, watch and glasses.", "CHI18_paper622-Figure4-1.png": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "CHI18_paper634-Figure8-1.png": "Figure 8 Design space for smartwatch interactions organized according to on-touch and pre-touch efforts dimensions. The design space reveals the area occupied by current standard techniques (A), and unexplored areas (B and C).", "CHI18_paper634-Figure10-1.png": "Figure 10: (a) PanPress. (b1) Initial screen. User performs a first pan. (b2) User holds contact for continuously panning. (b3) User adjusts the panning direction with subtle finger motion.", "CHI18_paper634-Figure9-1.png": "Figure 9: (a) SwipeZoom. (b1) initial screen. (b2) User presses in the bottom-left corner. (b3) User performs a diagonal swipe for continuously zooming in.", "CHI18_paper634-Figure11-1.png": "Figure 11: (a) TapZoom. (b1) Initial screen. (b2) User doubletaps on the left side of the screen. (b3) User holds contact for continuously zooming out.", "CHI18_paper634-Figure12-1.png": "Figure 12: (a) FlickPan. (b1) Initial screen. User flicks. (b2) Thumbnails of next screens (in the flick direction) appear. (b3) Selected thumbnail is now the current screen.", "CHI18_paper634-Figure3-1.png": "Figure 3: Experimental tasks. (a) Selection; (b) Flicking; (c1) pinching for zooming-out; (c2) zooming-in; and, (d1) Panning \u2013 initial screen - (d2) Panning \u2013 object close to the target.", "CHI18_paper634-Figure2-1.png": "Figure 2 : Participant interacting with a smartwatch while walking with a bag in his non-dominant hand.", "CHI18_paper63-Figure1-1.png": "Figure 1: Our system grafter allows users to remix mechanical machines. Here we use it to create a centrifuge for test tubes. (a) We start by identifying three parent models from a hobbyist repository. (b) If we used the workflow commonly practiced by makers today, we would extract relevant parts and recombine them into the desired configuration. Making an axle spin correctly in a bearing from a different parent model, however, requires time-consuming tweaks & test-prints. (c) Grafter, in contrast, extracts groups of elements that already work well together (aka mechanisms), which it fuses with mechanisms from other parents. This achieves the same (d) final 3D printed object, but without the tweaking.", "CSCW18_paper128-Figure2-1.png": "Fig. 2. A visual illustration of an example scenario of hybrid collaboration, including synchronous team meetings, involving remote team members, and asynchronous individual work.", "CHI18_paper248-Figure5-1.png": "Figure 5. Connect the dots using LinkTool. (a) How long does it take to walk from my hotel to Royal Albert Hall? (b) Tap highlighted location to enable LinkTool. (c) Drag LinkTool to Royal Albert Hall, and press with force to confirm connection between both locations. (d) Route and information view appear. (e) How about walking to British Museum? Drag LinkTool again to British Museum SpaceToken. (f) Route and information view appear after LinkTool touches SpaceToken. (Tapping SpaceToken again would cause display to zoom out so British Museum is visible.)", "CHI18_paper248-Figure6-1.png": "Figure 6. CollectionTool. (a) Using one finger to drag out of CollectionTool enables picker (blue line), which allows users to pick any on-screen location or SpaceToken. (b) Composite SpaceToken at top of CollectionTool represents CollectionTool content (three locations here). Tapping composite SpaceToken shows all three locations on display.", "CHI18_paper248-Figure7-1.png": "Figure 7. (a) SpaceBar is a scrollbar for a route. (b) User can change size and position of elevator indicator to make corresponding portion of route visible. (c) Conversely, elevator indicator is also updated as user interacts with route. (d) User can jump to any portion of route quickly by tapping SpaceBar, or (e) traverse route by scrubbing SpaceBar.", "CHI18_paper248-Figure9-1.png": "Figure 9. Drawing interface to mark custom spatial entities. (a) Holding drawing button at bottom left corner of display, (b) activates drawing mode, dimming screen, allowing user to sketch line to specify path or (c\u2013d) sketch (nearly) closed outline and scribble inside it to specify area. (e) Tapping \u201c+\u201d button creates SpaceToken for the marked area. (f) An area SpaceToken is added above the \u201c+\u201d button.", "CHI18_paper248-Figure3-1.png": "Figure 3. Creating SpaceToken to represent location. (a) After highlighting location, user taps \u201c+\u201d button to create SpaceToken for location. (b) New SpaceToken is added. (c) User can reorder SpaceTokens by long-pressing one and moving it to desired location. (d) SpaceToken can be removed by swiping it off display edge.", "CHI18_paper248-Figure4-1.png": "Figure 4. Using SpaceTokens to specify visibility and position constraints. (a) Where is British Museum? (b) Tap British Museum SpaceToken to see museum (specifying visibility constraint). SpaceToken turns red, screen zooms and pans as appropriate, and blue leader line indicates corresponding location. (c\u2013d) Alternatively, drag British Museum SpaceToken to specify where to display museum (specifying position constraint). LinkTool appears above SpaceToken when dragged. Multiple SpaceTokens can be used to mix and match visibility/position constraints of different locations.", "CHI18_paper531-Figure4-1.png": "Figure 4: Our Making Core Memory workshops: (left to right) Mountain View, CA, Seattle, WA, and Los Angeles, CA", "CHI18_paper531-Figure2-1.png": "Figure 2: Weaving patches for our Quilt.", "UIST18_paper335-Figure1-1.png": "Figure 1. (Center) When one primary finger is engaged in touch interaction, the rest of the secondary \u201cidle\u201d fingers (blue) are still but are able to move. We utilize these idle fingers to perform various hand poses before a touch or in-air gestures during a touch to add modality and expressiveness to the primary touch event: (a) opening a file by tapping with a \u201cBasic\u201d hand pose, (b) opening a context menu of the file by tapping with a \u201cSpread All\u201d hand pose, (c) deleting the touched object with the index finger by flicking with the thumb and (d) drawing a line with the index finger while controlling the width of the brush stroke by swiping up/down with the thumb.", "UIST18_paper335-Figure8-1.png": "Figure 8. Users can control the touched object by performing in-air gestures with the secondary finger, in this case the thumb, such as (a) tap for copy and paste, (b) flick for delete, and (c) swipe up for enlarge.", "UIST18_paper335-Figure6-1.png": "Figure 6. With the index finger as a primary finger, users can draw (a) a free-form line with a \u201cBasic\u201d hand pose, (b) a straight line with a \u201cStick\u201d hand pose, or (c) a curved line with a \u201cSpread All\u201d hand pose.", "UIST18_paper335-Figure7-1.png": "Figure 7. In a single pinch gesture with two primary fingers, users have different scale values by having different hand poses with the secondary finger: (a) small scale with a \u201cBend+2\u201d hand pose, (b) medium scale with a \u201cBasic+2\u201d hand pose, and (c) large scale with a \u201cSpread+2\u201d hand pose.", "UIST18_paper335-Figure2-1.png": "Figure 2. A 2\u00d72 design space was constructed based on conventional touch gestures (single- and multi-touch) and interaction periods (before and during touch). Based on the design space, we present the Touch+Finger gesture set and a summary of user ratings on the ease of performance of each gesture on a 5-point Likert scale (S: static touch gesture, D: dynamic touch gesture, SD in parentheses; 1 = most difficult, 5 = easiest, and *: uncomfortable gestures that received user rating under 3).", "UIST18_paper335-Figure10-1.png": "Figure 10. A user can adjust an image to exact size at the desired location on the screen at once in the following way: (a) determine the diagonal of the image as the distance between two finger points at the desired location, and then (b) insert it by bending the middle+ fingers (\u201cBending+2\u201d).", "UIST18_paper335-Figure9-1.png": "Figure 9. A user can continuously control the width of a brush stroke with secondary in-air gestures \u201cSwipe Up/Down\u201d while drawing a line with the index finger.", "UIST18_paper335-Figure3-1.png": "Figure 3. Our Touch+Finger prototype consisted of two ring-like devices with a IMU sensor attached, a touch-based device, and a PC for data processing", "UIST18_paper765-Figure1-1.png": "Figure 1. We explore skin irritations, such as itching, by means of electrical stimulation for use as a feedback modality. The sensations generated with this method range from gentle and soothing ones all the way to stinging and irritating ones. Compared to vibrotactile feedback, itching grabs attention more and increases the urge to react.", "CHI18_paper589-Figure5-1.png": "Figure 5: Definition used for the pitch and roll angles.", "CHI18_paper589-Figure1-1.png": "Figure 1: Angles (yaw, pitch and roll) describing a finger orientation.", "CHI18_paper423-Figure10-1.png": "Figure 10. Richard with the SelfReflector in his shop \u00a9Jayne Wallace", "CHI18_paper423-Figure8-1.png": "Figure 8. SelfReflector in use \u00a9Jon Rogers.", "CHI18_paper18-Figure2-1.png": "Figure 2. Users target an object by pointing at it. The line shows the ray extending in the same direction as the finger, towards the target.", "CHI18_paper18-Figure4-1.png": "Figure 4. Occluded objects can be difficult to select with ray-casting, because the ray might also intersect objects in front of them.", "CHI18_paper18-Figure5-1.png": "Figure 5. Fore/aft finger movement controls the \u2018depth cursor\u2019 used to select from the targets intersected by the ray. This shows how the finger position (\u2018X\u2019) is used to select an occluded object.", "CHI18_paper18-Figure6-1.png": "Figure 6. Selection can be followed by other actions, like repositioning objects by mapping object position to finger position.", "Ubicomp18_paper162-Figure8-1.png": "Fig. 8. Input actions with various objects.", "Ubicomp18_paper162-Figure9-1.png": "Fig. 9. Input actions with a gift wrapping bag Fig. 10. Input actions to capture printed patterns", "Ubicomp18_paper162-Figure1-1.png": "Fig. 1. Using UnicrePaint and some generated pictures", "CHI18_paper209-Figure1-1.png": "Figure 1. Main radial menu of a recorder application, which allows to add various types of annotations to a particular point in physical environment.", "CHI18_paper209-Figure2-1.png": "Figure 2. Learner is \u2018wearing\u2019 an expert\u2019s experience, performing an ultrasound diagnosis with the player application", "Ubicomp18_paper174-Figure1-1.png": "Fig. 1. We report on our implementations of cue-based authentication on situated displays using different input modalities. We evaluate and compare the use of touch (A, D), mid-air gestures (B, E), and gaze (C, F) to respond to on-screen cues. To enter \u201c0\u201d via touch, the user observes that the cue overlaid at digit \u201c0\u201d in Figure D is pointing upwards, hence the user provides an upward touch gesture in the yellow box. In Figure E, a user performs a mid-air gesture to the left with his left hand to input \u201c7\u201d. In Figure F, each digit moves in distinct trajectory, and a user selects \u201c3\u201d by following its circular movement. The size of the interface was adapted depending on the modality to account for the different interaction distances.", "CHI18_paper177-Figure2-1.png": "Figure 2. Our implementation uses a 3D printed PLA dish with an acrylic base plate. 2.4 mm diameter steel electrodes were embedded at 11 mm intervals and driven by a switching circuit.", "CHI18_paper198-Figure9-1.png": "Figure 9. Interactive Campus Map: Model III"}}